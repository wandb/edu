{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/decision-opt-course/2_bimbo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{decisionopt-nb2b} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Applying decision optimization for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb -Uqqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from pathlib import Path\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_QUIET\"] = \"true\"  # Keep notebook output clean\n",
    "wandb_project = \"decision_opt_bimbo\"\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "# Let's load the data from a W&B artifact\n",
    "with wandb.init(project=wandb_project) as run:\n",
    "    artifact = run.use_artifact(\n",
    "        \"wandb_course/decision_opt/grupo-bimbo-inventory-demand:latest\"\n",
    "    )\n",
    "    data_dir = Path(artifact.download())\n",
    "\n",
    "data = pd.read_csv(data_dir / \"train.csv\")\n",
    "clientes = pd.read_csv(data_dir / \"cliente_tabla.csv\")\n",
    "productos = pd.read_csv(data_dir / \"producto_tabla.csv\")\n",
    "town_state = pd.read_csv(data_dir / \"town_state.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "data = data.merge(clientes, on=\"Cliente_ID\", how=\"left\")\n",
    "data = data.merge(productos, on=\"Producto_ID\", how=\"left\")\n",
    "data = data.merge(town_state, on=\"Agencia_ID\", how=\"left\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns\n",
    "categorical_cols = [\"Agencia_ID\", \"Canal_ID\", \"Ruta_SAK\", \"Cliente_ID\", \"Producto_ID\"]\n",
    "\n",
    "# Define the label encoder\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data[col])\n",
    "    data[col] = le.transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "num_unique_vals = {col: data[col].nunique() for col in categorical_cols}\n",
    "embedding_sizes = {col: min(50, num_unique_vals[col] // 2) for col in categorical_cols}\n",
    "\n",
    "# Split into features and target\n",
    "X = data[categorical_cols].values\n",
    "y = data[\"Demanda_uni_equil\"].values\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Define the Dataset class\n",
    "class BimboDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = [torch.tensor(X[:, i], dtype=torch.long) for i in range(X.shape[1])]\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [x[idx] for x in self.X], self.y[idx]\n",
    "\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = BimboDataset(X_train, y_train)\n",
    "val_dataset = BimboDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, hidden_size=128):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(num_unique_vals[col], embedding_sizes[col])\n",
    "                for col in categorical_cols\n",
    "            ])\n",
    "        self.fc1 = nn.Linear(sum(embedding_sizes.values()), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = [embedding(x_i) for x_i, embedding in zip(x, self.embeddings)]\n",
    "        x = torch.cat(x, dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x).squeeze(-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(loss_fn, num_epochs=5):\n",
    "    model = SimpleModel(embedding_sizes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.tolist())\n",
    "                val_targets.extend(targets.tolist())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        r2 = r2_score(val_targets, val_preds)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"r_squared\": r2,\n",
    "            }\n",
    ")\n",
    "    return model, np.array(val_preds), np.array(val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_business_metrics(stocking_decisions, actual_demand, name, tags):\n",
    "    with wandb.init(\n",
    "        project=wandb_project,\n",
    "        name=name,\n",
    "        tags=tags,\n",
    "        job_type=\"decision\",\n",
    "    ):\n",
    "        frac_understocks = (stocking_decisions < actual_demand).mean()\n",
    "        total_understocked_amt = (actual_demand - stocking_decisions).clip(0).sum()\n",
    "        frac_overstocks = (stocking_decisions > actual_demand).mean()\n",
    "        total_overstocked_amt = (stocking_decisions - actual_demand).clip(0).sum()\n",
    "        utility = -3 * total_understocked_amt - total_overstocked_amt\n",
    "        mae = mean_absolute_error(actual_demand, stocking_decisions)\n",
    "        mse = mean_squared_error(actual_demand, stocking_decisions)\n",
    "        r2_score(actual_demand, stocking_decisions),\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"frac_understocks\": frac_understocks,\n",
    "                \"total_understocked_amt\": total_understocked_amt,\n",
    "                \"frac_overstocks\": frac_overstocks,\n",
    "                \"total_overstocked_amt\": total_overstocked_amt,\n",
    "                \"utility\": utility,\n",
    "                \"mae\": mae,\n",
    "                \"mse\": mse,\n",
    "                \"r2_score\": r2_score,\n",
    "            }\n",
    "        )\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with wandb.init(\n",
    "    project=wandb_project, name=\"mse_optimized\", tags=[\"mse_loss\"]\n",
    "):\n",
    "    loss = nn.MSELoss()\n",
    "    mse_model, mse_val_preds, mse_val_targets = train_model(loss, num_epochs=5)\n",
    "    # save mse_model as artifact\n",
    "    torch.save(mse_model.state_dict(), \"mse_model.pt\")\n",
    "    wandb.save(\"mse_model.pt\")\n",
    "\n",
    "mse_val_stock = np.ceil(mse_val_preds)\n",
    "log_business_metrics(mse_val_stock, mse_val_targets, \"mse_loss_predictions\", tags=[\"mse_loss\", \"stock_predicted_sales\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_stocking_rule = np.ceil(1.5 * mse_val_preds)\n",
    "log_business_metrics(alternative_stocking_rule,\n",
    "                     mse_val_targets,\n",
    "                     \"50_pct_above_mse_loss_predictions\",\n",
    "                     tags=[\"mse_loss\", \"stock_50_pct_above_predicted_sales\"]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(\n",
    "    project=wandb_project, name=\"mae_optimized\", tags=[\"mae_loss\"]\n",
    "):\n",
    "    loss = nn.L1Loss()\n",
    "    mae_model, mae_val_preds, mae_val_targets = train_model(loss, num_epochs=5)\n",
    "\n",
    "mae_val_stock = np.ceil(mae_val_preds)\n",
    "log_business_metrics(mae_val_stock,\n",
    "                    mae_val_targets,\n",
    "                    'mae_loss_predictions',\n",
    "                    tags=[\"mae_loss\", \"stock_predicted_sales\"]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_mae_stocking_rule = np.ceil(1.5 * mae_val_preds)\n",
    "log_business_metrics(above_mae_stocking_rule,\n",
    "                     mse_val_targets,\n",
    "                     \"50_pct_above_mae_loss_predictions\",\n",
    "                     tags=[\"mse_loss\", \"stock_50_pct_above_predicted_sales\"]\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to example of understock costing \\\\$3 per unit and overstock costing \\\\$1 per unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, actual):\n",
    "        diff = outputs - actual\n",
    "        loss = torch.where(outputs > actual, diff, -3 * diff)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "with wandb.init(\n",
    "    project=\"regression_decision_opt\", name=\"our_utility_loss\", tags=[\"custom_loss\"]\n",
    "):\n",
    "    custom_model, custom_val_preds, custom_val_targets = train_model(\n",
    "        CustomLoss(), num_epochs=5\n",
    "    )\n",
    "\n",
    "custom_val_stock = np.ceil(custom_val_preds)\n",
    "log_business_metrics(custom_val_stock, custom_val_targets, 'utility_fn_loss_predictions', tags=['stock_predicted_sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Full Distributions\n",
    "\n",
    "Using QuantileRegressionForest: https://scikit-garden.github.io/examples/QuantileRegressionForests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install quantile-forest -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "qrf = RandomForestQuantileRegressor(\n",
    "    n_estimators=100, min_samples_leaf=50, random_state=0\n",
    ")\n",
    "qrf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [i / 100 for i in range(5, 100, 5)]\n",
    "sample_preds = qrf.predict(X_val, quantiles=quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_demand_prediction = sample_preds[5]\n",
    "one_demand_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def rarely_run_out_rule(prediction):\n",
    "    outlier_bound = 3 * np.mean(prediction)\n",
    "    to_stock = math.ceil(min(prediction[-2], outlier_bound))\n",
    "    return to_stock\n",
    "\n",
    "rarely_run_out_rule(one_demand_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stocking_decisions = np.apply_along_axis(rarely_run_out_rule, 1, sample_preds)\n",
    "(all_stocking_decisions < sample_preds[:, -2]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_business_metrics(all_stocking_decisions, y_val, 'capped_90th_percentile', tags=['probabilistic_forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(all_stocking_decisions < sample_preds[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"stocked: {all_stocking_decisions[126]} when demand was {sample_preds[126, :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in [0, 126, 295, 298, 557, 620, 882]:\n",
    "    print(f\"stocked: {all_stocking_decisions[row]}. Input {sample_preds[row, :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
