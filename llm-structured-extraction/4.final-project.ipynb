{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/llm-structured-extraction/4.final-project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{llmeng-1-final} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Colab\n",
    "\n",
    "Run this code if you're using Google Colab, you can skip if you're running locally. You may need to restart Colab after installing requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Download files on colab\n",
    "if not Path(\"requirements.txt\").exists():\n",
    "    !wget https://raw.githubusercontent.com/wandb/edu/main/llm-structured-extraction/{requirements.txt,helpers.py}\n",
    "    !pip install -r requirements.txt -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import openai\n",
    "\n",
    "# Setup your Openai API key\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "  openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Weights & Biases to track experiments\n",
    "\n",
    "Experimenting with prompts, function calling and response model schema is critical to get good results. As LLM Engineers, we will be methodical and use Weights & Biases to track our experiments.\n",
    "\n",
    "Here are a few things you should consider logging: \n",
    "\n",
    "1. Save input and output pairs for later analysis\n",
    "2. Save the JSON schema for the response_model\n",
    "3. Having snapshots of the model and data allow us to compare results over time, and as we make changes to the model we can see how the results change.\n",
    "\n",
    "This is particularly useful when we might want to blend a mix of synthetic and real data to evaluate our model. We will use the `wandb` library to track our experiments and save the results to a dashboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import instructor\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from helpers import dicts_to_df\n",
    "from datetime import date\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class DateRange(BaseModel):\n",
    "    chain_of_thought: str = Field(\n",
    "        description=\"Think step by step to plan what is the best time range to search in\"\n",
    "    )\n",
    "    start: date\n",
    "    end: date\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    rewritten_query: str = Field(\n",
    "        description=\"Rewrite the query to make it more specific\"\n",
    "    )\n",
    "    published_daterange: DateRange = Field(\n",
    "        description=\"Effective date range to search in\"\n",
    "    )\n",
    "\n",
    "    def report(self):\n",
    "        dct = self.model_dump()\n",
    "        dct[\"usage\"] = self._raw_response.usage.model_dump()\n",
    "        return dct\n",
    "\n",
    "\n",
    "\n",
    "# We'll use a different client for async calls\n",
    "# To highlight the difference and how we can use both\n",
    "aclient = instructor.patch(AsyncOpenAI())\n",
    "\n",
    "\n",
    "async def expand_query(\n",
    "    q, *, model: str = \"gpt-3.5-turbo\", temp: float = 0\n",
    ") -> Query:\n",
    "    return await aclient.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temp,\n",
    "        response_model=Query,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "temp = 0\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"query\",\n",
    "    config={\"model\": model, \"temp\": temp},\n",
    ")\n",
    "\n",
    "test_queries = [\n",
    "    \"latest developments in artificial intelligence last 3 weeks\",\n",
    "    \"renewable energy trends past month\",\n",
    "    \"quantum computing advancements last 2 months\",\n",
    "    \"biotechnology updates last 10 days\",\n",
    "]\n",
    "start = time.perf_counter()\n",
    "queries = await asyncio.gather(\n",
    "    *[expand_query(q, model=model, temp=temp) for q in test_queries]\n",
    ")\n",
    "duration = time.perf_counter() - start\n",
    "\n",
    "with open(\"schema.json\", \"w+\") as f:\n",
    "    schema = Query.model_json_schema()\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "with open(\"results.jsonlines\", \"w+\") as f:\n",
    "    for query in queries:\n",
    "        f.write(query.model_dump_json() + \"\\n\")\n",
    "\n",
    "df = dicts_to_df([q.report() for q in queries])\n",
    "df[\"input\"] = test_queries\n",
    "df.to_csv(\"results.csv\")\n",
    "\n",
    "\n",
    "run.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))})\n",
    "\n",
    "run.log(\n",
    "    {\n",
    "        \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),\n",
    "        \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),\n",
    "        \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),\n",
    "        \"duration (s)\": duration,\n",
    "        \"average duration (s)\": duration / len(queries),\n",
    "        \"n_queries\": len(queries),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "run.log(\n",
    "    {\n",
    "        \"results\": wandb.Table(dataframe=df),\n",
    "    }\n",
    ")\n",
    "\n",
    "files = wandb.Artifact(\"data\", type=\"dataset\")\n",
    "\n",
    "files.add_file(\"schema.json\")\n",
    "files.add_file(\"results.jsonlines\")\n",
    "files.add_file(\"results.csv\")\n",
    "\n",
    "\n",
    "run.log_artifact(files)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a W&B Report\n",
    "\n",
    "After logging your experiments, create a [W&B Report](https://docs.wandb.ai/guides/reports/create-a-report) and document your findings. Copy the link to your report into a text file and submit it as the final project assignment in our course platform. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
