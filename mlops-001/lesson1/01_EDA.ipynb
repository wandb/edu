{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/mlops-001/lesson1/01_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{course-lesson1} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa157c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!wget https://raw.githubusercontent.com/wandb/edu/main/mlops-001/lesson1/requirements.txt\n",
    "!wget https://raw.githubusercontent.com/wandb/edu/main/mlops-001/lesson1/params.py\n",
    "!wget https://raw.githubusercontent.com/wandb/edu/main/mlops-001/lesson1/utils.py\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a212966e-8f38-4b4a-8e00-2985120439ca",
   "metadata": {},
   "source": [
    "# EDA \n",
    "<!--- @wandbcode{course-lesson1} -->\n",
    "\n",
    "In this notebook, we will download a sample of the [BDD100K](https://www.bdd100k.com/) semantic segmentation dataset and use W&B Artifacts and Tables to version and analyze our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17490b58-a8f2-489e-9c71-ca0e078591dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False # set this flag to True to use a small subset of data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee88efc-3257-4ed7-bb8c-1695f4a8c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import params\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055764da-7252-4796-86af-585007dea288",
   "metadata": {},
   "source": [
    "We have defined some global configuration parameters in the `params.py` file. `ENTITY` should correspond to your W&B Team name if you work in a team, replace it with `None` if you work individually. \n",
    "\n",
    "In the section below, we will use `untar_data` function from `fastai` to download and unzip our datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae3750-a653-4265-9c52-c1f9af9b8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/wandb_course/bdd_simple_1k.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da50d19-860a-4b39-8320-a376c483da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(untar_data(URL, force_download=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5b830-e685-4a0d-81e8-6dda10697dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6708ae-abaa-432f-83b0-028a99222096",
   "metadata": {},
   "source": [
    "Here we define several functions to help us process the data and upload it as a `Table` to W&B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779f980-4d9b-4645-9726-642a25334e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(fname):\n",
    "    return (fname.parent.parent/\"labels\")/f\"{fname.stem}_mask.png\"\n",
    "\n",
    "def get_classes_per_image(mask_data, class_labels):\n",
    "    unique = list(np.unique(mask_data))\n",
    "    result_dict = {}\n",
    "    for _class in class_labels.keys():\n",
    "        result_dict[class_labels[_class]] = int(_class in unique)\n",
    "    return result_dict\n",
    "\n",
    "def _create_table(image_files, class_labels):\n",
    "    \"Create a table with the dataset\"\n",
    "    labels = [str(class_labels[_lab]) for _lab in list(class_labels)]\n",
    "    table = wandb.Table(columns=[\"File_Name\", \"Images\", \"Split\"] + labels)\n",
    "    \n",
    "    for i, image_file in progress_bar(enumerate(image_files), total=len(image_files)):\n",
    "        image = Image.open(image_file)\n",
    "        mask_data = np.array(Image.open(label_func(image_file)))\n",
    "        class_in_image = get_classes_per_image(mask_data, class_labels)\n",
    "        table.add_data(\n",
    "            str(image_file.name),\n",
    "            wandb.Image(\n",
    "                    image,\n",
    "                    masks={\n",
    "                        \"predictions\": {\n",
    "                            \"mask_data\": mask_data,\n",
    "                            \"class_labels\": class_labels,\n",
    "                        }\n",
    "                    }\n",
    "            ),\n",
    "            \"None\", # we don't have a dataset split yet\n",
    "            *[class_in_image[_lab] for _lab in labels]\n",
    "        )\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac085cd-2409-4baf-a887-108e3dca65b9",
   "metadata": {},
   "source": [
    "We will start a new W&B `run` and put everything into a raw Artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87141b3-b9b7-4a40-839c-64c835484f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=params.WANDB_PROJECT, entity=params.ENTITY, job_type=\"upload\")\n",
    "raw_data_at = wandb.Artifact(params.RAW_DATA_AT, type=\"raw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de6968-b465-4e2e-aaf1-787a48c7981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_at.add_file(path/'LICENSE.txt', name='LICENSE.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10addc71-201d-4c29-af23-9a5d9a5cbddf",
   "metadata": {},
   "source": [
    "Let's add the images and label masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e29a83-ec60-4518-8b73-078f1719bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_at.add_dir(path/'images', name='images')\n",
    "raw_data_at.add_dir(path/'labels', name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b989eb-36da-4194-9a34-88126749f210",
   "metadata": {},
   "source": [
    "Let's get the file names of images in our dataset and use the function we defined above to create a W&B Table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa2dae-1b2b-4c70-8dcf-b079f4f05b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = get_image_files(path/\"images\", recurse=False)\n",
    "\n",
    "# sample a subset if DEBUG\n",
    "if DEBUG: image_files = image_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ff088-4cb3-4739-b4e8-308d03645ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = _create_table(image_files, params.BDD_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73cd37-6a72-44f7-a5cf-6abe7d327092",
   "metadata": {},
   "source": [
    "Finally, we will add the Table to our Artifact, log it to W&B and finish our `run`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4183e9c-75e4-48a8-a2e1-0f4492c0b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_at.add(table, \"eda_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ed9c8-6905-4841-a9f7-e3f3b9b6d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_artifact(raw_data_at)\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
