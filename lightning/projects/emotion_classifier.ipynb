{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotion-classifier.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS5-7kZVPYn6"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# Emotion Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwl91FcAPxKC"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning==1.3.8 torchviz wandb\n",
        "!git clone https://github.com/wandb/lit_utils\n",
        "!cd \"/content/lit_utils\" && git pull\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "import lit_utils as lu\n",
        "\n",
        "lu.utils.filter_warnings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiykCpNiW_RT"
      },
      "source": [
        "## Defining the `Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZIcFSsZQ3rz"
      },
      "source": [
        "class LitEmotionClassifier(lu.nn.modules.LoggedImageClassifierModule):\n",
        "\n",
        "  def __init__(self, config, max_images_to_display=32):\n",
        "    super().__init__()\n",
        "\n",
        "    self.labels = [\"Angry\", \"Disgusted\", \"Afraid\", \"Happy\",\n",
        "                \"Sad\", \"Surprised\", \"Neutral\"]\n",
        "\n",
        "    # first, convolutional component\n",
        "    self.conv_layers = torch.nn.Sequential(*[                    \n",
        "    ])\n",
        "\n",
        "    # need a fixed-size input for fully-connected component,\n",
        "    #  so apply a \"re-sizing\" layer, to size set in config\n",
        "    self.resize_layer = torch.nn.AdaptiveAvgPool2d(\n",
        "      (config[\"final_height\"], config[\"final_width\"]))\n",
        "    \n",
        "    # now, we can apply our fully-connected component\n",
        "    final_size = config[\"final_height\"] * config[\"final_width\"] * config[\"conv.channels\"][-1]\n",
        "    self.fc_layers = torch.nn.Sequential(*[\n",
        "        lu.nn.fc.FullyConnected(\n",
        "            in_features=final_size,\n",
        "            out_features=config[\"output.size\"]),                                                           \n",
        "    ])\n",
        "\n",
        "    self.optimizer = config[\"optimizer\"]\n",
        "    self.optimizer_params = config[\"optimizer.params\"]\n",
        "    self.loss = config[\"loss_fn\"]\n",
        "\n",
        "  def forward(self, x):\n",
        "    # first apply convolutional layers\n",
        "    for layer in self.conv_layers: \n",
        "      x = layer(x)\n",
        "\n",
        "    # then convert to a fixed-size vector\n",
        "    x = self.resize_layer(x)\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "    # then apply the fully-connected layers\n",
        "    for layer in self.fc_layers: # snap together the LEGOs\n",
        "      x = layer(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zr6c9o7X9C6"
      },
      "source": [
        "## Building the `Model` and Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8rAPoX-YHBu"
      },
      "source": [
        "config = {\n",
        "  \"batch_size\": 256,\n",
        "  \"max_epochs\": 10,\n",
        "  \"conv.channels\": [1],\n",
        "  \"final_height\": 24,\n",
        "  \"final_width\": 24,\n",
        "  \"fc.size\": [],\n",
        "  \"activation\": torch.nn.ReLU(),\n",
        "  \"loss_fn\": torch.nn.CrossEntropyLoss(),  # cross-entropy loss\n",
        "  \"optimizer\": torch.optim.Adam,\n",
        "  \"optimizer.params\": {\"lr\": 0.001},\n",
        "}\n",
        "\n",
        "\n",
        "dmodule = lu.datamodules.FERDataModule(batch_size=config[\"batch_size\"])\n",
        "config[\"output.size\"] = len(dmodule.classes)\n",
        "lec = LitEmotionClassifier(config)\n",
        "dmodule.setup()\n",
        "dmodule.prepare_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQMzEoEyU_4t"
      },
      "source": [
        "### Debugging Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "067CRKkuVBxh"
      },
      "source": [
        "# for debugging purposes (checking shapes, etc.), make these available\n",
        "dloader = dmodule.train_dataloader()  # set up the Loader\n",
        "\n",
        "example_batch = next(iter(dloader))  # grab a batch from the Loader\n",
        "example_x, example_y = example_batch[0].to(\"cuda\"), example_batch[1].to(\"cuda\")\n",
        "\n",
        "print(f\"Input Shape: {example_x.shape}\")\n",
        "print(f\"Target Shape: {example_y.shape}\")\n",
        "\n",
        "lec.to(\"cuda\")\n",
        "outputs = lec.forward(example_x)\n",
        "print(f\"Output Shape: {outputs.shape}\")\n",
        "print(f\"Loss: {lec.loss(outputs, example_y)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eV4GL75Z-TH"
      },
      "source": [
        "### Running `.fit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1rA-q1FaAam"
      },
      "source": [
        "with wandb.init(project=\"lit-fer\", entity=\"wandb\", config=config):\n",
        "  # ü™µ configure logging\n",
        "  cbs=[lu.callbacks.WandbCallback(),  # callbacks add extra features, like better logging\n",
        "       lu.callbacks.FilterLogCallback(image_size=(config[\"final_height\"], config[\"final_width\"]), log_input=True),  # this one logs the weights as images\n",
        "       lu.callbacks.ImagePredLogCallback(labels=dmodule.classes, on_train=True)  # and this one logs the inputs and outputs\n",
        "       ]\n",
        "  wandblogger = pl.loggers.WandbLogger(save_code=True)\n",
        "\n",
        "  # üëü configure Trainer \n",
        "  trainer = pl.Trainer(gpus=1,  # use the GPU for .forward\n",
        "                       logger=wandblogger,  # log to Weights & Biases\n",
        "                       callbacks=cbs,  # use callbacks to log lots of run data\n",
        "                       max_epochs=config[\"max_epochs\"], log_every_n_steps=1,\n",
        "                       progress_bar_refresh_rate=50)\n",
        "\n",
        "  # üèÉ‚Äç‚ôÄÔ∏è run the Trainer on the model\n",
        "  trainer.fit(lec, datamodule=dmodule)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}