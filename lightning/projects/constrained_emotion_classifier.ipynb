{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "constrained_emotion_classifier.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvfHmiGkiK5wPpBldiig2+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/lightning/projects/constrained_emotion_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS5-7kZVPYn6"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# Designing a Memory-Constrained Emotion Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwl91FcAPxKC"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch_lightning torchviz wandb\n",
        "\n",
        "repo_url = \"https://raw.githubusercontent.com/wandb/edu/main/\"\n",
        "utils_path = \"lightning/utils.py\"\n",
        "# Download a util file of helper methods for this notebook\n",
        "!curl {repo_url + utils_path} --output utils.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bWIJyxrPRku"
      },
      "source": [
        "from pathlib import Path\n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets\n",
        "import wandb\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelPruning, QuantizationAwareTraining\n",
        "\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K45APMHVDCv"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDvJlwRlUg9c"
      },
      "source": [
        "## Facial Expression `DataModule` and `DataLoaders`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMDEO9ekRiAR"
      },
      "source": [
        "class FERDataModule(pl.LightningDataModule):\n",
        "  \"\"\"DataModule for downloading and preparing the FER2013 dataset.\n",
        "  \"\"\"\n",
        "  tar_url = \"https://www.dropbox.com/s/opuvvdv3uligypx/fer2013.tar\"\n",
        "  local_path = Path(\"fer2013\")\n",
        "\n",
        "  def __init__(self, batch_size=64):\n",
        "    super().__init__()  # ‚ö°: we inherit from LightningDataModule\n",
        "    self.batch_size = batch_size\n",
        "    self.val_batch_size = 10 * self.batch_size\n",
        "\n",
        "  def prepare_data(self, validation_size=0.2, force_reload=False):\n",
        "    # ‚ö°: how do we set up the data?\n",
        "    if hasattr(self, \"training_data\") and not force_reload:\n",
        "      return  # only re-run if we haven't been run before\n",
        "\n",
        "    # download the data from the internet\n",
        "    self.download_data()\n",
        "\n",
        "    # read it from a .csv file\n",
        "    faces, emotions = self.read_data()\n",
        "\n",
        "    # normalize it\n",
        "    faces = torch.divide(faces, 255.)\n",
        "\n",
        "    # split it into training and validation\n",
        "    validation_size = int(len(faces) * 0.8)\n",
        "\n",
        "    self.training_data = torch.utils.data.TensorDataset(\n",
        "      faces[:-validation_size], emotions[:-validation_size])\n",
        "    self.validation_data = torch.utils.data.TensorDataset(\n",
        "      faces[-validation_size:], emotions[-validation_size:])\n",
        "    \n",
        "    # record metadata\n",
        "    self.num_total, self.num_classes = emotions.shape[0], torch.max(emotions)\n",
        "    self.num_train = self.num_total - validation_size\n",
        "    self.num_validation = validation_size\n",
        "\n",
        "  def train_dataloader(self):  # ‚ö°: how do we go from dataset to dataloader?\n",
        "    \"\"\"The DataLoaders returned by a DataModule produce data for a model.\n",
        "    \n",
        "    This DataLoader is used during training.\"\"\"\n",
        "    return DataLoader(self.training_data, batch_size=self.batch_size,\n",
        "                      num_workers=1, pin_memory=True)\n",
        "\n",
        "  def val_dataloader(self):  # ‚ö°: what about during validation?\n",
        "    \"\"\"The DataLoaders returned by a DataModule produce data for a model.\n",
        "    \n",
        "    This DataLoader is used during validation, at the end of each epoch.\"\"\"\n",
        "    return DataLoader(self.validation_data, batch_size=self.val_batch_size,\n",
        "                      num_workers=1, pin_memory=True)\n",
        "\n",
        "  def download_data(self):\n",
        "    if not os.path.exists(self.local_path):\n",
        "      print(\"Downloading the face emotion dataset...\")\n",
        "      subprocess.check_output(\n",
        "          f\"curl -SL {self.tar_url} | tar xz\", shell=True)\n",
        "      print(\"...done\")\n",
        "      \n",
        "  def read_data(self):\n",
        "    \"\"\"Read the data from a .csv into torch Tensors\"\"\"\n",
        "    data = pd.read_csv(self.local_path / \"fer2013.csv\")\n",
        "    pixels = data[\"pixels\"].tolist()\n",
        "    width, height = 48, 48\n",
        "    faces = []\n",
        "    for pixel_sequence in pixels:\n",
        "        face = np.asarray(pixel_sequence.split(\n",
        "            ' '), dtype=np.uint8).reshape(1, width, height,)\n",
        "        faces.append(face.astype(\"float32\"))\n",
        "\n",
        "    faces = np.asarray(faces)\n",
        "    emotions = data[\"emotion\"].to_numpy()\n",
        "\n",
        "    return torch.tensor(faces), torch.tensor(emotions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8IAP0JGXPiG"
      },
      "source": [
        "# Utility Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuxlkfFuYVZZ"
      },
      "source": [
        "These cells provide extra functionality related to logging and optimizing\n",
        "performance metrics:\n",
        "static quantization, model size and (nonzero) parameter counting,\n",
        "and weight pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx1GqTU682Tq"
      },
      "source": [
        "### Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE_TbTHKXRlP"
      },
      "source": [
        "def run_static_quantization(network, xs, qconfig=\"fbgemm\"):\n",
        "  \"\"\"Return a quantized version of supplied network.\n",
        "\n",
        "  Runs forward pass of network with xs, so make sure they're on\n",
        "  the same device. Returns a copy of the network, so watch memory consumption.\n",
        "\n",
        "  Note that this uses torch.quantization, rather than PyTorchLightning.\n",
        "\n",
        "  network: torch.Module, network to be quantized.\n",
        "  xs: torch.Tensor, valid inputs for network.forward.\n",
        "  qconfig: string, \"fbgemm\" to quantize for server/x86, \"qnnpack\" for mobile/ARM\n",
        "  \"\"\"\n",
        "  # set up quantization\n",
        "  network.qconfig = torch.quantization.get_default_qconfig(qconfig)\n",
        "  network.eval()\n",
        "\n",
        "  # attach methods for collecting activation statistics to set quantization bounds\n",
        "  qnetwork = torch.quantization.prepare(network)\n",
        "  \n",
        "  # run inputs through network, collect stats\n",
        "  qnetwork.forward(xs)\n",
        "  \n",
        "  # convert network to uint8 using quantization statistics\n",
        "  qnetwork = torch.quantization.convert(qnetwork)\n",
        "\n",
        "  return qnetwork"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPiZVrNu85KR"
      },
      "source": [
        "### Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icA7WI2gZSlY"
      },
      "source": [
        "# Helper functions for building ModelPruning Callbacks\n",
        "\n",
        "def make_pruner(prune_config, network=None, n_epochs=None):\n",
        "  \"\"\"Builds a ModelPruning PyTorchLightning Callback from a dictionary.\n",
        "\n",
        "  Aside from the keyword arguments to pl.Callbacks.ModelPruning, this dictionary\n",
        "  may contain the keys \"target_sparsity\"\n",
        "  \n",
        "  target_sparsity is combined with n_epochs to determine the value of the\n",
        "  \"amount\" keyword argument to ModelPruning, which specifies how much pruning to\n",
        "  do on each epoch.\n",
        "\n",
        "  parameters can be None, \"conv\", or \"linear\". It is used to fetch the\n",
        "  parameters which are to be pruned from the provided network. See\n",
        "  get_parameters_to_prune for details. Note that None corresponds to pruning\n",
        "  all parameters.\n",
        "  \"\"\"\n",
        "  if \"target_sparsity\" in prune_config.keys():\n",
        "    target = prune_config.pop(\"target_sparsity\")\n",
        "    assert n_epochs is not None, \"when specifying target sparsity, must provide number of epochs\"\n",
        "    prune_config[\"amount\"] = compute_iterative_prune(target, n_epochs)\n",
        "\n",
        "  assert \"amount\" in prune_config.keys(), \"must specify stepwise pruning amount or target\"\n",
        "\n",
        "  if \"parameters\" in prune_config.keys():\n",
        "    parameters = prune_config.pop(\"parameters\")\n",
        "    if parameters is not None:\n",
        "      assert network is not None, \"when specifying parameters, must provide network\"\n",
        "    prune_config[\"parameters_to_prune\"] = get_parameters_to_prune(parameters, network)\n",
        "\n",
        "  assert \"parameters_to_prune\" in prune_config.keys(), \"must specify which parameters to prune, or None\"\n",
        "\n",
        "  return ModelPruning(**prune_config)\n",
        "\n",
        "\n",
        "def get_parameters_to_prune(parameters, network):\n",
        "  \"\"\"Return the weights of network matching the parameters value.\n",
        "\n",
        "  Parameters must be one of \"conv\" or \"linear\", or None,\n",
        "  in which case None is also returned.\n",
        "  \"\"\"\n",
        "  if parameters == \"conv\":\n",
        "    return [(layer, \"weight\") for layer in network.modules()\n",
        "            if isinstance(layer, torch.nn.Conv2d)]\n",
        "  elif parameters == \"linear\":\n",
        "    return [(layer, \"weight\") for layer in network.modules()\n",
        "            if isinstance(layer, torch.nn.Linear)]\n",
        "  elif parameters is None:\n",
        "    return\n",
        "  else:\n",
        "    raise ValueError(f\"could not understand parameters value: {parameters}\")\n",
        "\n",
        "\n",
        "def compute_iterative_prune(target_sparsity, n_epochs):\n",
        "  return 1 - math.pow(1 - target_sparsity, 1 / n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9db0dmIE89Gu"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFyyi4O1YSPv"
      },
      "source": [
        "# Metric calculation for model file size (quantization)\n",
        "#  and total non-zero parameters (pruning)\n",
        "\n",
        "def print_model_size(network):\n",
        "  \"\"\"Save model to disk and print filesize\"\"\"\n",
        "  torch.save(network.state_dict(), \"tmp.pt\")\n",
        "  size_mb = os.path.getsize(\"tmp.pt\") / 1e6\n",
        "  print(f\"{round(size_mb, 2)} MB\")\n",
        "  os.remove('tmp.pt')\n",
        "  return size_mb\n",
        "\n",
        "\n",
        "def count_nonzero(module):\n",
        "  \"\"\"Counts the total number of non-zero parameters in a module.\n",
        "  \n",
        "  For compatibility with networks with active torch.nn.utils.prune methods,\n",
        "  checks for _mask tensors, which are applied during forward passes and so\n",
        "  represent the actual sparsity of the networks.\"\"\"\n",
        "  if module.named_buffers():\n",
        "    masks = {name[:-5]: mask_tensor for name, mask_tensor in module.named_buffers()\n",
        "             if name.endswith(\"_mask\")}\n",
        "  else:\n",
        "    masks = {}\n",
        "\n",
        "  nparams = 0\n",
        "  with torch.no_grad():\n",
        "    for name, tensor in module.named_parameters():\n",
        "      if name[:-5] in masks.keys():\n",
        "        tensor = masks[name[:-5]]\n",
        "      nparams += int(torch.sum(tensor != 0))\n",
        "\n",
        "  return nparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiykCpNiW_RT"
      },
      "source": [
        "## Defining the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evXY1Yyy9Hkt"
      },
      "source": [
        "### Classes for `FullyConnected` and `Convolution`al Blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJl2nLM1hij-"
      },
      "source": [
        "class FullyConnected(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, in_features, out_features,\n",
        "               activation=None, batchnorm=False, dropout=0.):\n",
        "    super().__init__()\n",
        "    self.linear = torch.nn.Linear(in_features, out_features)\n",
        "\n",
        "    if activation is None:  # defaults to passing inputs unchanged\n",
        "      activation = torch.nn.Identity\n",
        "    self.activation = activation()\n",
        "\n",
        "    post_act = []\n",
        "    if batchnorm:\n",
        "      post_act.append(torch.nn.BatchNorm1d(out_features))\n",
        "    if dropout:\n",
        "      post_act.append(torch.nn.Dropout(dropout))\n",
        "\n",
        "    self.post_act = torch.nn.Sequential(*post_act)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.post_act(self.activation(self.linear(x)))\n",
        "\n",
        "\n",
        "class Convolution(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size,\n",
        "               activation=None, batchnorm=False, dropout=0.):\n",
        "    super().__init__()\n",
        "    self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size)\n",
        "\n",
        "    if activation is None:\n",
        "      activation = torch.nn.Identity  # defaults to passing inputs unchanged\n",
        "    self.activation = activation()\n",
        "\n",
        "    post_act = []\n",
        "    if batchnorm:\n",
        "      post_act.append(torch.nn.BatchNorm2d(out_channels))\n",
        "    if dropout:\n",
        "      post_act.append(torch.nn.Dropout2d(dropout))\n",
        "\n",
        "    self.post_act = torch.nn.Sequential(*post_act)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.post_act(self.activation(self.conv2d(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91GAb0Z09PaD"
      },
      "source": [
        "### Model Class: `LitEmotionClassifier`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXLhu2qg9NjX"
      },
      "source": [
        "class LitEmotionClassifier(utils.LoggedImageClassifierModule):\n",
        "\n",
        "  def __init__(self, config, max_images_to_display=32):\n",
        "    super().__init__(max_images_to_display=max_images_to_display)\n",
        "\n",
        "    self.labels = [\"Angry\", \"Disgusted\", \"Afraid\", \"Happy\",\n",
        "                   \"Sad\", \"Surprised\", \"Neutral\"]\n",
        "\n",
        "    # define layers here; apply them in forward\n",
        "    #  for compatibility, use the FullyConnected and Convolution blocks above\n",
        "    self.linear = FullyConnected(in_features=1 * 48 * 48,\n",
        "                                 out_features=len(self.labels))\n",
        "\n",
        "    # note: applied in training_step, not forward\n",
        "    self.output_layer = torch.nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    # for quantization; if no quantization applied, these do nothing\n",
        "    self.quant = torch.quantization.QuantStub()  # quantize inputs\n",
        "    self.dequant = torch.quantization.DeQuantStub()   # dequantize outputs \n",
        "\n",
        "    self.optimizer = config[\"optimizer\"]\n",
        "    self.optimizer_params = config[\"optimizer.params\"]\n",
        "    self.loss = config[\"loss\"]\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.quant(x)  # apply quantization, if applicable\n",
        "\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "    x = self.linear(x)\n",
        "    \n",
        "    x = self.dequant(x)  # apply dequantization, if applicable\n",
        "    # Note: LogSoftmax is applied outside of forward, for compatibility with quantization\n",
        "    return x\n",
        "\n",
        "  def training_step(self, batch, idx):\n",
        "    xs, ys = batch\n",
        "    y_hats = self.output_layer(self.forward(xs))\n",
        "    loss = self.loss(y_hats, ys)\n",
        "\n",
        "    logging_scalars = {\"loss\": loss}\n",
        "    for metric in self.training_metrics:\n",
        "        self.add_metric(metric, logging_scalars, y_hats, ys)\n",
        "\n",
        "    self.do_logging(xs, ys, idx, y_hats, logging_scalars)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, idx):\n",
        "    xs, ys = batch\n",
        "    y_hats = self.output_layer(self.forward(xs))\n",
        "    loss = self.loss(y_hats, ys)\n",
        "\n",
        "    logging_scalars = {\"loss\": loss}\n",
        "    for metric in self.validation_metrics:\n",
        "        self.add_metric(metric, logging_scalars, y_hats, ys)\n",
        "\n",
        "    self.do_logging(xs, ys, idx, y_hats, logging_scalars, step=\"val\")\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return self.optimizer(self.parameters(), **self.optimizer_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eV4GL75Z-TH"
      },
      "source": [
        "## Defining the Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii6FWAePXGdH"
      },
      "source": [
        "def train(network, dmodule, config):\n",
        "  with wandb.init(config=config, project=\"lit-fer-constrained\", entity=\"wandb\") as run:\n",
        "\n",
        "    callbacks = []\n",
        "  \n",
        "    # QAT:\n",
        "    #  if doing quant-aware training, add to callbacks\n",
        "    if config[\"quantization\"] == \"qat\":\n",
        "      callbacks.append(QuantizationAwareTraining(input_compatible=False))\n",
        "  \n",
        "    # PRUNING:\n",
        "    #  if doing pruning, add to callbacks\n",
        "    if config[\"pruning\"]:\n",
        "      assert config[\"quantization\"] == \"none\", \"cannot combine pruning and quantization\"\n",
        "  \n",
        "      for prune_config in config[\"pruning\"].values():\n",
        "        callbacks.append(make_pruner(prune_config, network, n_epochs=config[\"max_epochs\"]))\n",
        "\n",
        "    # üëü configure Trainer \n",
        "    trainer = pl.Trainer(\n",
        "      gpus=1, max_epochs=config[\"max_epochs\"], log_every_n_steps=1,\n",
        "      logger=pl.loggers.WandbLogger(log_model=True, save_code=True),\n",
        "      callbacks=callbacks,\n",
        "      progress_bar_refresh_rate=50)\n",
        "                        \n",
        "    # üèÉ‚Äç‚ôÄÔ∏è run the Trainer on the model\n",
        "    trainer.fit(network, dmodule)\n",
        "\n",
        "    wandb.summary[\"params\"] = network.count_params()\n",
        "    wandb.summary[\"nonzero_params\"] = count_nonzero(network)\n",
        "\n",
        "    # STATIC:\n",
        "    #  if doing static post-training quantization, apply it now\n",
        "    if config[\"quantization\"] == \"post\":\n",
        "      xs, _ = next(iter(dmodule.train_dataloader()))\n",
        "      network = run_static_quantization(network, xs)  # see below for implementation\n",
        "\n",
        "    # report metrics to wandb\n",
        "    wandb.summary[\"size_mb\"] = print_model_size(network)\n",
        "\n",
        "  return network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zr6c9o7X9C6"
      },
      "source": [
        "## Loading the Data, Building the Model, and Running Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8rAPoX-YHBu"
      },
      "source": [
        "config = {\n",
        "  \"batch_size\": 256,\n",
        "  \"max_epochs\": 10,\n",
        "  \"activation\": torch.nn.ReLU,\n",
        "  \"loss\": torch.nn.NLLLoss(),\n",
        "  \"optimizer\": torch.optim.Adam,\n",
        "  \"optimizer.params\": {\"lr\": 0.001},\n",
        "  \"quantization\": \"none\",  # \"none\" or \"qat\" or \"post\"\n",
        "  # pruning is configured at the end of this cell\n",
        "}\n",
        "\n",
        "## Loading the data\n",
        "\n",
        "dmodule = FERDataModule(batch_size=config[\"batch_size\"])\n",
        "dmodule.prepare_data()\n",
        "\n",
        "# for debugging purposes (checking shapes, etc.), make these available\n",
        "dloader = dmodule.train_dataloader()  # set up the Loader\n",
        "\n",
        "example_batch = next(iter(dloader))  # grab a batch from the Loader\n",
        "example_x, example_y = example_batch[0].to(\"cuda\"), example_batch[1].to(\"cuda\")\n",
        "\n",
        "print(f\"Input Shape: {example_x.shape}\")\n",
        "print(f\"Target Shape: {example_y.shape}\")\n",
        "\n",
        "## Building the model\n",
        "\n",
        "lec = LitEmotionClassifier(config)\n",
        "\n",
        "lec.to(\"cuda\")\n",
        "outputs = lec.forward(example_x)\n",
        "print(f\"Output Shape: {outputs.shape}\")\n",
        "\n",
        "## Pruning\n",
        "\n",
        "config[\"pruning\"] = {}  # see wandb.me/lit-prune-colab for more examples\n",
        "global_prune_config = {  # config for applying pruning to the entire network\n",
        "  \"parameters\": None,\n",
        "  \"pruning_fn\": \"l1_unstructured\",\n",
        "  \"target_sparsity\": 0.9,  # target sparsity level for this pruner\n",
        "  \"use_global_unstructured\": True,\n",
        "  \"pruning_dim\": None,\n",
        "  \"pruning_norm\": None,\n",
        "}\n",
        "# config[\"pruning\"][\"global\"] = global_prune_config  # comment to remove global pruning\n",
        "\n",
        "## Training\n",
        "lec = train(lec, dmodule, config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}