{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perceptron-fives.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c216tB5gbTG8"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# A Perceptron for Detecting Fives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN1M5zGebb9J"
      },
      "source": [
        "## Installing and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu5Gebw6uvDF"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning==1.3.8 torchviz wandb\n",
        "!git clone https://github.com/wandb/lit_utils\n",
        "!cd \"/content/lit_utils\" && git pull\n",
        "\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets\n",
        "import wandb\n",
        "\n",
        "import lit_utils as lu\n",
        "\n",
        "# remove slow mirror from list of MNIST mirrors\n",
        "torchvision.datasets.MNIST.mirrors = lu.datamodules.ClassificationMNIST.mirrors\n",
        "\n",
        "lu.utils.filter_warnings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lop1xIYiyYHC"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM1iZhbQbfyy"
      },
      "source": [
        "## Defining the `Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElHWRKqcu8oL"
      },
      "source": [
        "class LitPerceptronModel(lu.nn.modules.LoggedImageClassifierModule):\n",
        "  \"\"\"A simple Perceptron Model, with under-the-hood wandb\n",
        "  and pytorch-lightning features (logging, metrics, etc.).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):  # make the model\n",
        "    super().__init__()\n",
        "    self.perceptron = torch.nn.Linear(in_features=28 * 28, out_features=1)\n",
        "    self.loss = torch.nn.MSELoss()\n",
        "\n",
        "  def forward(self, x):  # produce outputs\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "    x = self.perceptron(x)  # apply model\n",
        "    return torch.squeeze(x)\n",
        "\n",
        "  def configure_optimizers(self):  # âš¡: setup for .fit\n",
        "    return torch.optim.Adam(self.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0WwC57bigK"
      },
      "source": [
        "## Defining a `DataLoader`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7InOksIvsTK"
      },
      "source": [
        "class IsFiveDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, batch_size=64):\n",
        "    super().__init__()  # âš¡: we inherit from LightningDataModule\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def prepare_data(self): # âš¡: how do we set up the data?\n",
        "    # download the data from the internet\n",
        "    mnist = torchvision.datasets.MNIST(\"./data\", train=True, download=True)\n",
        "\n",
        "    # set up shapes and types\n",
        "    self.digits, self.is_5 = mnist.data.float(), (mnist.targets == 5).float()\n",
        "    self.digits = 255 - self.digits  # invert colors\n",
        "    self.dataset = torch.utils.data.TensorDataset(self.digits, self.is_5)\n",
        "\n",
        "  def train_dataloader(self):  # âš¡: how do we go from dataset to dataloader?\n",
        "    \"\"\"The DataLoaders returned by a DataModule produce data for a model.\n",
        "    \n",
        "    This DataLoader is used during training.\"\"\"\n",
        "    return DataLoader(self.dataset, batch_size=self.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq_qDw3Ubv8d"
      },
      "source": [
        "## Building and Training the `Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaYToi8Z5WlQ"
      },
      "source": [
        "dmodule  = IsFiveDataModule(batch_size=256)\n",
        "lp = LitPerceptronModel()\n",
        "\n",
        "dmodule.prepare_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8E52qNDr8O0"
      },
      "source": [
        "### Debugging Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu5EIhmrrxxO"
      },
      "source": [
        "# for debugging purposes (checking shapes, etc.), make these available\n",
        "dloader = dmodule.train_dataloader()\n",
        "\n",
        "example_batch = next(iter(dloader))\n",
        "example_x, example_y = example_batch[0].to(\"cuda\"), example_batch[1].to(\"cuda\")\n",
        "\n",
        "print(f\"Input Shape: {example_x.shape}\")\n",
        "print(f\"Target Shape: {example_y.shape}\")\n",
        "\n",
        "lp.to(\"cuda\")\n",
        "outputs = lp.forward(example_x)\n",
        "print(f\"Output Shape: {outputs.shape}\")\n",
        "print(f\"Loss : {lp.loss(outputs, example_y)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm_uxxIyr_j9"
      },
      "source": [
        "### Running `.fit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0RIrZI-LNeN"
      },
      "source": [
        "with wandb.init(project=\"lit-perceptron\", entity=\"wandb\"):\n",
        "\n",
        "  # ðŸªµ configure logging\n",
        "  cbs = [lu.callbacks.WandbCallback(),  # callbacks add extra features, like better logging\n",
        "         lu.callbacks.ImagePredLogCallback(labels=[\"Not 5\", \"Is 5\"], on_train=True)]\n",
        "\n",
        "  wandblogger = pl.loggers.WandbLogger(save_code=True)\n",
        "  if hasattr(lp, \"_wandb_watch_called\") and lp._wandb_watch_called:\n",
        "    wandblogger.watch(lp)  # track gradients\n",
        "\n",
        "  # ðŸ‘Ÿ configure Trainer \n",
        "  trainer = pl.Trainer(gpus=1,  # use the GPU for .forward\n",
        "                      logger=wandblogger,\n",
        "                      callbacks=cbs,  # use callbacks to log lots of run data\n",
        "                      max_epochs=10, log_every_n_steps=1,\n",
        "                      progress_bar_refresh_rate=50)\n",
        "\n",
        "  # ðŸƒâ€â™€ï¸ run the Trainer on the model\n",
        "  trainer.fit(lp, dmodule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGUlLaPQJENc"
      },
      "source": [
        "## Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuSI5XqE-wwf"
      },
      "source": [
        "For the exercises below, you'll want to review the results\n",
        "of your training runs on [Weights & Biases](https://wandb.ai/site).\n",
        "The link to each run's dashboard will be printed in the cell output above,\n",
        "with the name \"Run Page\".\n",
        "\n",
        "You should be able to find your run,\n",
        "along with other runs created using this notebook,\n",
        "in [this Weights & Biases dashboard](http://wandb.me/lit-perceptron-workspace),\n",
        "which shows results across many runs.\n",
        "\n",
        "You can see an example run [here](https://wandb.ai/wandb/lit-perceptron/runs/3orr7yha).\n",
        "\n",
        " > _Tip_: to launch new training runs,\n",
        " restart the Colab notebook and run all the cells at once\n",
        " (\"Runtime > Restart and run all\").\n",
        " That way, you can always be sure what code\n",
        " was run, in case you hit an error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqPCaU0HKFJO"
      },
      "source": [
        "\n",
        "### Set A.\n",
        "\n",
        "#### **Exercise**: Deep learning models are built by snapping \"LEGOs\" together: modular, combinable pieces. What are the LEGOs of this model?\n",
        "\n",
        "#### **Exercise**: Why do we need `flatten`?\n",
        "\n",
        "#### **Exercise**: With the default settings, does the `Model` do well? How can you tell? Look at both the accuracy and the predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekxvRP6tKGGE"
      },
      "source": [
        "\n",
        "### Set B.\n",
        "\n",
        "#### **Exercise**: The outputs of the model aren't in the right range. Add `torch.sigmoid` to `forward` to squish them down before they get compared to the targets.\n",
        "\n",
        "#### **Exercise**: The inputs to the model aren't in the right range either. Add `self.digits = torch.divide(self.digits, 255.)` to standardize them (after the colors are inverted!).\n",
        "\n",
        "#### **Exercise**: After you've made these two improvements, re-run the model. How is it doing now? Try also running a model with only one of the two improvements. Is either sufficient on its own for the model to train well?"
      ]
    }
  ]
}