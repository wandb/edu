{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quantization-cnn.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMb/XsPo/vmMvV7TKHETdr2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/lightning/performance/quantization_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc465k10-Fjv"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nil6iK_nPkx"
      },
      "source": [
        "# Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp7552BjkTjy"
      },
      "source": [
        "_Note_: This notebook trains and profiles multiple models\n",
        "and can take several minutes to run.\n",
        "\n",
        "We recommend that you run all of the cells first (Runtime > Run All)\n",
        "and then read through the code and exercises while the notebook executes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohn3Svjfsrkp"
      },
      "source": [
        "You can also check out the results in the public\n",
        "[W&B Workspace for this notebook](https://wandb.ai/wandb/quantize/workspace)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVT4qCkhC6gU"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install wandb pytorch_lightning==1.3.2 torchviz\n",
        "\n",
        "repo_url = \"https://raw.githubusercontent.com/wandb/edu/main/\"\n",
        "utils_path = \"lightning/utils.py\"\n",
        "# Download a util file of helper methods for this notebook\n",
        "!curl {repo_url + utils_path} > utils.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew38zmQXEZic"
      },
      "source": [
        "# standard libraries, for profiling\n",
        "import os\n",
        "import time\n",
        "\n",
        "# usual DL imports\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "\n",
        "# special import for quantization with Lightning\n",
        "from pytorch_lightning.callbacks import QuantizationAwareTraining\n",
        "\n",
        "# utilities for PyTorch Lightning and wandb\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxElwa3jAuVN"
      },
      "source": [
        "# filter out deprecation warnings from torch.quantization\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action=\"ignore\", category=DeprecationWarning, module=r\"torch.quantization\")\n",
        "warnings.filterwarnings(action=\"ignore\", category=UserWarning, module=r\"torch.quantization\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ-_hZznjUdm"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf3MD74xl_q9"
      },
      "source": [
        "# Setup Code: Model, Data, and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mu8ANOIO7y6"
      },
      "source": [
        "def print_model_size(network):\n",
        "  \"\"\"Save model to disk and print filesize\"\"\"\n",
        "  torch.save(network.state_dict(), \"tmp.pt\")\n",
        "  size_mb = os.path.getsize(\"tmp.pt\") / 1e6\n",
        "  print(f\"{round(size_mb, 2)} MB\")\n",
        "  os.remove('tmp.pt')\n",
        "  return size_mb\n",
        "\n",
        "\n",
        "class FullyConnected(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, in_features, out_features,\n",
        "               activation=None, batchnorm=False):\n",
        "    super().__init__()\n",
        "    self.linear = torch.nn.Linear(in_features, out_features)\n",
        "\n",
        "    if activation is None:  # defaults to passing inputs unchanged\n",
        "      activation = torch.nn.Identity()\n",
        "    self.activation = activation\n",
        "\n",
        "    if batchnorm:\n",
        "      self.post_act = torch.nn.BatchNorm1d(out_features)\n",
        "    else:\n",
        "      self.post_act = torch.nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.post_act(self.activation(self.linear(x)))\n",
        "\n",
        "\n",
        "class Convolution(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size,\n",
        "               activation=None, batchnorm=False):\n",
        "    super().__init__()\n",
        "    self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size)\n",
        "\n",
        "    if activation is None:\n",
        "      activation = torch.nn.Identity()  # defaults to passing inputs unchanged\n",
        "    self.activation = activation\n",
        "\n",
        "    if batchnorm:\n",
        "      self.post_act = torch.nn.BatchNorm2d(out_channels)\n",
        "    else:\n",
        "      self.post_act = torch.nn.Identity()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.post_act(self.activation(self.conv2d(x)))\n",
        "\n",
        "\n",
        "class LitCNN(utils.LoggedImageClassifierModule):\n",
        "  \"\"\"A simple CNN Model, with under-the-hood wandb\n",
        "  and pytorch-lightning channels (logging, metrics, etc.).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config, max_images_to_display=32):  # make the model\n",
        "    super().__init__(max_images_to_display=max_images_to_display)\n",
        "\n",
        "    # first, convolutional component\n",
        "    self.conv_layers = torch.nn.Sequential(  # specify our LEGOs. edit this by adding to the list!\n",
        "      # hidden conv layer\n",
        "      Convolution(in_channels=1, kernel_size=config[\"kernel_size\"],\n",
        "                  activation=config[\"activation\"],\n",
        "                  out_channels=config[\"conv.channels\"][0],\n",
        "                  batchnorm=config[\"batchnorm\"]),\n",
        "      # hidden conv layer\n",
        "      Convolution(in_channels=config[\"conv.channels\"][0], kernel_size=config[\"kernel_size\"],\n",
        "                  activation=config[\"activation\"],\n",
        "                  out_channels=config[\"conv.channels\"][1],\n",
        "                  batchnorm=config[\"batchnorm\"]),\n",
        "      # pooling often follows 2 convs\n",
        "      torch.nn.MaxPool2d(config[\"pool_size\"]),\n",
        "    )\n",
        "\n",
        "\n",
        "    # need a fixed-size input for fully-connected component,\n",
        "    #  so apply a \"re-sizing\" layer, to size set in config\n",
        "    self.resize_layer = torch.nn.AdaptiveAvgPool2d(\n",
        "      (config[\"final_height\"], config[\"final_width\"]))\n",
        "\n",
        "    # now, we can apply our fully-connected component\n",
        "    final_size = config[\"final_height\"] * config[\"final_width\"] * config[\"conv.channels\"][-1]\n",
        "    self.fc_layers = torch.nn.Sequential( # specify our LEGOs. edit this by adding to the list!\n",
        "      FullyConnected(in_features=final_size, activation=config[\"activation\"],\n",
        "                     out_features=config[\"fc1.size\"], batchnorm=False),\n",
        "      FullyConnected(in_features=config[\"fc1.size\"], activation=config[\"activation\"],\n",
        "                     out_features=config[\"fc2.size\"], batchnorm=False),\n",
        "      FullyConnected(in_features=config[\"fc2.size\"],  # \"read-out\" layer\n",
        "                     out_features=10, batchnorm=False),\n",
        "    )\n",
        "\n",
        "    self.output_layer = torch.nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    # for quantization\n",
        "    self.quant = torch.quantization.QuantStub()  # quantize inputs\n",
        "    self.dequant = torch.quantization.DeQuantStub()   # dequantize outputs\n",
        "\n",
        "    self.loss = config[\"loss\"]\n",
        "    self.optimizer = config[\"optimizer\"]\n",
        "    self.optimizer_params = config[\"optimizer.params\"]\n",
        "    config.update({f\"channels_{ii}\": channels\n",
        "                   for ii, channels in enumerate(config[\"conv.channels\"])})\n",
        "\n",
        "  def forward(self, x):  # produce outputs\n",
        "    x = self.quant(x)  # apply quantization, if applicable\n",
        "\n",
        "    # first apply convolutional layers\n",
        "    for layer in self.conv_layers: \n",
        "      x = layer(x)\n",
        "\n",
        "    # then convert to a fixed-size vector\n",
        "    x = self.resize_layer(x)\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "    # then apply the fully-connected layers\n",
        "    for layer in self.fc_layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = self.dequant(x)  # remove quantization, if applicable\n",
        "    return x\n",
        "\n",
        "  def training_step(self, batch, idx):\n",
        "    xs, ys = batch\n",
        "    y_hats = self.output_layer(self.forward(xs))\n",
        "    loss = self.loss(y_hats, ys)\n",
        "\n",
        "    logging_scalars = {\"loss\": loss}\n",
        "    for metric in self.training_metrics:\n",
        "        self.add_metric(metric, logging_scalars, y_hats, ys)\n",
        "\n",
        "    self.do_logging(xs, ys, idx, y_hats, logging_scalars)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, idx):\n",
        "    xs, ys = batch\n",
        "    y_hats = self.output_layer(self.forward(xs))\n",
        "    loss = self.loss(y_hats, ys)\n",
        "\n",
        "    logging_scalars = {\"loss\": loss}\n",
        "    for metric in self.validation_metrics:\n",
        "        self.add_metric(metric, logging_scalars, y_hats, ys)\n",
        "\n",
        "    self.do_logging(xs, ys, idx, y_hats, logging_scalars, step=\"val\")\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def configure_optimizers(self):  # ‚ö°: setup for .fit\n",
        "    return self.optimizer(self.parameters(), **self.optimizer_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEWFQ0CTTfpu"
      },
      "source": [
        "config = {\n",
        "  \"quantization\": \"post\",  # \"post\" | \"qat\" | \"none\"\n",
        "  \"batch_size\": 1024,\n",
        "  \"max_epochs\": 1,\n",
        "  \"batchnorm\": True,\n",
        "  \"kernel_size\": 7,\n",
        "  \"conv.channels\": [128, 256],\n",
        "  \"pool_size\": 2,\n",
        "  \"final_height\": 10,\n",
        "  \"final_width\": 10,\n",
        "  \"fc1.size\": 1024,\n",
        "  \"fc2.size\": 512,\n",
        "  \"activation\": torch.nn.ReLU(),\n",
        "  \"loss\": torch.nn.NLLLoss(),\n",
        "  \"optimizer\": torch.optim.Adam,\n",
        "  \"optimizer.params\": {\"lr\": 0.0001},\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Q-1SMQsmBA"
      },
      "source": [
        "  # üì∏ set up the dataset of images\n",
        "  dmodule = utils.MNISTDataModule(batch_size=config[\"batch_size\"], validation_size=1024)\n",
        "  dmodule.prepare_data()\n",
        "  dmodule.setup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoEiDoUEA8yV"
      },
      "source": [
        "def train(network, dmodule, config):\n",
        "  callbacks = []\n",
        "\n",
        "  # QAT:\n",
        "  #  if doing quant-aware training, add to callbacks\n",
        "  if config[\"quantization\"] == \"qat\":\n",
        "    callbacks.append(QuantizationAwareTraining(input_compatible=False))\n",
        "\n",
        "  with wandb.init(config=config, project=\"quantize\", entity=\"wandb\", job_type=\"train\") as run:\n",
        "    # üëü configure Trainer \n",
        "    trainer = pl.Trainer(\n",
        "      gpus=1, max_epochs=config[\"max_epochs\"], log_every_n_steps=1,\n",
        "      logger=pl.loggers.WandbLogger(log_model=True, save_code=True),\n",
        "      callbacks=callbacks,\n",
        "      progress_bar_refresh_rate=50)\n",
        "                        \n",
        "    # üèÉ‚Äç‚ôÄÔ∏è run the Trainer on the model\n",
        "    trainer.fit(network, dmodule)\n",
        "\n",
        "  # STATIC:\n",
        "  #  if doing static post-training quantization, apply it now\n",
        "  if config[\"quantization\"] == \"post\":\n",
        "    xs, _ = next(iter(dmodule.train_dataloader()))\n",
        "    network = run_static_quantization(network, xs)  # see below for implementation\n",
        "\n",
        "  return network\n",
        "\n",
        "\n",
        "def profile(network, dmodule, config):\n",
        "\n",
        "  # ‚è±Ô∏è time the model and check the validation accuracy\n",
        "  with wandb.init(config=config, project=\"quantize\", entity=\"wandb\", job_type=\"profile\") as run:\n",
        "    val_trainer = pl.Trainer(\n",
        "      gpus=0,  # profile on CPU, not GPU\n",
        "      max_epochs=1, logger=pl.loggers.WandbLogger(log_model=True, save_code=True),\n",
        "      progress_bar_refresh_rate=50\n",
        "    )\n",
        "\n",
        "    network.eval()\n",
        "    start = time.process_time()\n",
        "    val_trainer.validate(network, val_dataloaders=dmodule.val_dataloader())\n",
        "    runtime = time.process_time() - start\n",
        "\n",
        "    # report metrics to wandb\n",
        "    wandb.summary[\"runtime\"] = runtime\n",
        "    wandb.summary[\"size_mb\"] = print_model_size(network)\n",
        "    wandb.summary[\"params\"] = network.count_params()\n",
        "\n",
        "  return network \n",
        "\n",
        "\n",
        "def train_and_profile(dmodule, config):\n",
        "  network = LitCNN(config)\n",
        "\n",
        "  network = train(network, dmodule, config)\n",
        "  network = profile(network, dmodule, config)\n",
        "\n",
        "  return network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-ykgfvwuWx6"
      },
      "source": [
        "xs, ys = next(iter(dmodule.train_dataloader()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59rilyZHmUjX"
      },
      "source": [
        "# Baseline: No Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1QwsXapsOqN"
      },
      "source": [
        "config[\"quantization\"] = \"none\"\n",
        "\n",
        "baseline_nn = train_and_profile(dmodule, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTTZedK7mZyn"
      },
      "source": [
        "# Post-Training Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm4DkKk6ni0Z"
      },
      "source": [
        "def run_static_quantization(network, xs, qconfig=\"fbgemm\"):\n",
        "  \"\"\"Return a quantized version of supplied network.\n",
        "\n",
        "  Runs forward pass of network with xs, so make sure they're on\n",
        "  the same device. Returns a copy of the network, so watch memory consumption.\n",
        "\n",
        "  Note that this uses torch.quantization, rather than PyTorchLightning.\n",
        "\n",
        "  network: torch.Module, network to be quantized.\n",
        "  xs: torch.Tensor, valid inputs for network.forward.\n",
        "  qconfig: string, \"fbgemm\" to quantize for server/x86, \"qnnpack\" for mobile/ARM\n",
        "  \"\"\"\n",
        "  # set up quantization\n",
        "  network.qconfig = torch.quantization.get_default_qconfig(qconfig)\n",
        "  network.eval()\n",
        "\n",
        "  # attach methods for collecting activation statistics to set quantization bounds\n",
        "  qnetwork = torch.quantization.prepare(network)\n",
        "  \n",
        "  # run inputs through network, collect stats\n",
        "  qnetwork.forward(xs)\n",
        "  \n",
        "  # convert network to uint8 using quantization statistics\n",
        "  qnetwork = torch.quantization.convert(qnetwork)\n",
        "\n",
        "  return qnetwork"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvDDzke8sWXl"
      },
      "source": [
        "config[\"quantization\"] = \"post\"\n",
        "static_quant_nn = train_and_profile(dmodule, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVI7mPFImW1l"
      },
      "source": [
        "# Quantization-Aware Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjxmkyOM6g0o"
      },
      "source": [
        "config[\"quantization\"] = \"qat\"\n",
        "\n",
        "qat_nn = train_and_profile(dmodule, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyRfRMyO_K3m"
      },
      "source": [
        "# Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YIrWS1W_N0V"
      },
      "source": [
        "#### 1. Comparing Model Size, Runtime, and Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqIgJ68w_Rdf"
      },
      "source": [
        "Quantization improves two critical performance characteristics\n",
        "relevant to running models on edge CPUs:\n",
        "1. reducing the model memory footprint by ~4x\n",
        "(from 32 bits to ~8 bits per parameter),\n",
        "2. reducing the latency of large matrix multiplications,\n",
        "typically by a factor of 2 or less.\n",
        "\n",
        "Compare the model sizes (in MB)\n",
        "and the runtimes for the three models.\n",
        "Do you observe the typical improvements in this case?\n",
        "\n",
        "The biggest drawback to quantization is that\n",
        "quantization can reduce accuracy,\n",
        "especially when statically quantizing large models.\n",
        "Do you observe any accuracy penalty for quantization?\n",
        "\n",
        "_Note_: quantization-aware training is less stable,\n",
        "in this case, and gives a wider range of accuracies.\n",
        "Check the [Project page](https://wandb.ai/wandb/quantize/workspace)\n",
        "to see other runs and get a sense of the distribution,\n",
        "or repeatedly run the notebook with the same parameters.\n",
        "\n",
        "Generally, QAT is only beneficial in performance terms for certain models,\n",
        "especially MobileNet-style architectures.\n",
        "\n",
        "Check out [this video](https://www.youtube.com/watch?v=c3MT2qV5f9w)\n",
        "for a more thorough discussion\n",
        "on this and other details of quantization in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzJjNb36_SEw"
      },
      "source": [
        "#### 2. A Closer Look at Quantization-Aware Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DnOKxSl_W-7"
      },
      "source": [
        "In QAT,\n",
        "we add extra \"fake-quantization\" operations to the\n",
        "graph during training,\n",
        "then drop them once the model is quantized.\n",
        "\n",
        "Head to the Weights & Biases run page for a training (not profiling!)\n",
        "run that used QAT\n",
        "and find the model's compute graph (Files tab, `graph.png`),\n",
        "where the operations that make up the model are represented.\n",
        "Compare the graph to that from a run without QAT.\n",
        "What differences do you see?\n",
        "\n",
        "Adding these extra modules can increase the runtime for training\n",
        "(which occurs at full precision).\n",
        "Compare the runtimes for training runs\n",
        "with and without QAT. Do you see a difference?\n",
        "Compare this to the runtimes for training runs\n",
        "with and without static quantization. Is there a difference here?\n",
        "\n",
        "Also, note that training occurs on the GPU,\n",
        "rather than the CPU on which the quantized models run,\n",
        "and it operates on a training set that is 5x larger than\n",
        "the validation set used during profiling.\n",
        "Compare the runtimes of the\n",
        "(see the Overview tab on the W&B run page).\n",
        "Are they 5x longer?\n",
        "Which provides more of a speedup:\n",
        "running on the GPU or applying quantization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m88N0ZezUhY"
      },
      "source": [
        "#### 3. CHALLENGE: Fusing Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ5uoC7k6xGc"
      },
      "source": [
        "Additional performance improvements can be obtained by \"fusing\"\n",
        "multiple modules together into a single module,\n",
        "enabling some operations to occur more quickly.\n",
        "\n",
        "The cell below creates a new, \"fused\" model\n",
        "that combines the ReLU activation computation\n",
        "with the matrix multiplication layers.\n",
        "\n",
        "Do you see any reduction in the runtime?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgwHtEodzvwI"
      },
      "source": [
        "config[\"quantization\"] = \"post+fusion\"\n",
        "\n",
        "# copy over parameters etc. from baseline_nn to new network\n",
        "state = baseline_nn.state_dict()\n",
        "fused_nn = LitCNN(config)\n",
        "fused_nn.load_state_dict(state)\n",
        "\n",
        "for layer in fused_nn.conv_layers:\n",
        "  if isinstance(layer, Convolution):\n",
        "    torch.quantization.fuse_modules(  # \"fuses\" multiple modules into one\n",
        "      layer, [\"conv2d\", \"activation\",], inplace=True)\n",
        "\n",
        "for layer in fused_nn.fc_layers:\n",
        "  if isinstance(layer, FullyConnected):\n",
        "    if isinstance(layer.activation, torch.nn.ReLU):\n",
        "      torch.quantization.fuse_modules(\n",
        "        layer, [\"linear\", \"activation\",], inplace=True)\n",
        "\n",
        "fused_nn = run_static_quantization(fused_nn, xs)\n",
        "\n",
        "fused_nn = profile(fused_nn, dmodule, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI5Tkhqy4ONF"
      },
      "source": [
        "Improvements are more obvious for much larger tensors\n",
        "(try resizing the inputs with a `torch.nn.AdaptiveAvgPool2d` layer\n",
        "before applying the `conv_layers`!)\n",
        "and when more modules are fused.\n",
        "\n",
        "Batchnorm can be fused when it occurs after convolutional or before ReLU layers\n",
        "(see the docstring for `torch.quantization.fuse_modules`).\n",
        "In that order, all three modules can be fused into one.\n",
        "\n",
        "Change the definition of the `Convolution` layer so that batchnorm\n",
        "comes inbetween the convolutional layer and the activation,\n",
        "then add it to the list of `modules_to_fuse` above.\n",
        "Does this bring any additional speedup?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jVvH4K2H3Mq"
      },
      "source": [
        "# Endnote\n",
        "\n",
        "The most common error with quantized models involves improper quantization of inputs and/or outputs. Expand this section for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41SoJlXFJA1T"
      },
      "source": [
        "\n",
        "Quantized `uint8` tensors are a fundamentally different type from regular floating point tensors,\n",
        "and so different operations are implemented for each.\n",
        "For example, a quantized tensor cannot be quantized again,\n",
        "while a floating point tensor cannot be un-quantized;\n",
        "if the weights are not quantized but the inputs are quantized,\n",
        "there are .\n",
        "This is akin to the differences between tensors on CPU and GPU --\n",
        "you can't add a CPU tensor to a GPU tensor, for example,\n",
        "or pass\n",
        "\n",
        "The error messages look like so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3njUW_wqI12Y"
      },
      "source": [
        "```\n",
        "RuntimeError: Could not run 'aten::{OP_NAME}' with arguments from the {'QuantizedCPU'/'CPU'} backend\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgi78GBII9mC"
      },
      "source": [
        "where `OP_NAME` is the name of an operator\n",
        "(e.g. including `relu` or `nll_loss` or `linear`).\n",
        "\n",
        "You can use `OP_NAME` to identify where in the model the mismatch is occurring,\n",
        "e.g. at the input or the output or, more rarely, in between.\n",
        "\n",
        "If the error mentions the `QuantizedCPU` backend,\n",
        "that means the inputs are quantized and the operator is not compatible with them.\n",
        "As of version 1.8 of PyTorch,\n",
        "most modules are not compatible with quantized tensors,\n",
        "nor is any of `torch.nn.functional`,\n",
        "but the most common modules are.\n",
        "This can be resolved by applying a `DeQuantStub`\n",
        "to the inputs\n",
        "(see `.dequant` in the module above).\n",
        "\n",
        "If the error mentions the `CPU` backend,\n",
        "that means the operation is quantized but the inputs are not.\n",
        "In general, this is resolved by\n",
        "apply a `QuantStub` to the inputs\n",
        "(see `.quant` in the module above)."
      ]
    }
  ]
}