{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "architecture_search.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNIb3+ZFbE5fKp4MVomX1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/lightning/cnn/architecture_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vabA9vVEP441"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnXGc-ofPy7M"
      },
      "source": [
        "# Image Classification Architecture Search on CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsc-QnQ_QEUp"
      },
      "source": [
        "This notebook allows you to join an on-going, parallelized architecture\n",
        "search for image classification (specifically, the\n",
        "[CIFAR-10 labeling task](https://www.cs.toronto.edu/~kriz/cifar.html)),\n",
        "just by executing the cells.\n",
        "Read the instructions at the bottom to see how to launch a search of your own.\n",
        "\n",
        "> Note that [Colab restricts GPU usage](https://research.google.com/colaboratory/faq.html),\n",
        "especially when run non-interactively,\n",
        "so if you leave this notebook running\n",
        "for more than a few hours in a short period of time,\n",
        "you're likely to see your access curtailed\n",
        "unless you're a paid user.\n",
        "\n",
        "The cells below define a way to sample and train a random architecture that combines 0 or more convolutional layers and 0 or more fully-connected layers,\n",
        "followed by a fully-connected classifier,\n",
        "using a provided random seed for reproducibility. \n",
        "\n",
        "The results of this architecture search will be logged to\n",
        "[this Weights & Biases dashboard](https://wandb.ai/wandb/archsearch-cifar10/sweeps/bmhxqxr0),\n",
        "where you can see which architectures perform best and search for patterns\n",
        "in the submitted runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgSPSuFKKpcj"
      },
      "source": [
        "# Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHvuvWqGGFXK"
      },
      "source": [
        "%%capture\n",
        "!pip install -qqq pytorch_lightning torchviz wandb\n",
        "\n",
        "repo_url = \"https://raw.githubusercontent.com/wandb/edu/main/\"\n",
        "utils_path = \"lightning/utils.py\"\n",
        "# Download a util file of helper methods\n",
        "!curl {repo_url + utils_path} > utils.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xfwvxr0Mamk"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import wandb\n",
        "\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opS3Y_hDRzC3"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--9sZhXVOila"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U-UgDsgT27A"
      },
      "source": [
        "from math import floor\n",
        "\n",
        "class CIFAR10DataModule(pl.LightningDataModule):\n",
        "  \"\"\"Dataloaders and setup for the CIFAR10 dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, batch_size, train_size=0.8, debug=False):\n",
        "    \"\"\"\n",
        "\n",
        "    Arguments:\n",
        "    batch_size: int. Size of batches in training, validation, and test\n",
        "    train_size: int or float. If int, number of examples in training set,\n",
        "                If float, fraction of examples in training set.\n",
        "    debug:  bool. If True, cut dataset size by a factor of 10.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.data_dir = \"./data\"\n",
        "    self.seed = 117\n",
        "\n",
        "    self.train_size = train_size\n",
        "    self.batch_size = batch_size \n",
        "    self.debug = debug\n",
        "\n",
        "    self.transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "  def prepare_data(self):\n",
        "    \"\"\"Download the dataset.\n",
        "    \"\"\"\n",
        "    torchvision.datasets.CIFAR10(root=self.data_dir, train=True,\n",
        "                                 download=True, transform=self.transform)\n",
        "\n",
        "    torchvision.datasets.CIFAR10(root=self.data_dir, train=False,\n",
        "                                 download=True, transform=self.transform)\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    \"\"\"Set up training and test data and perform our train/val split.\n",
        "    \"\"\"\n",
        "    if stage in (None, \"fit\"):\n",
        "      cifar10_full = torchvision.datasets.CIFAR10(self.data_dir, train=True,\n",
        "                                                  transform=self.transform)\n",
        "      if self.debug:                                            \n",
        "        cifar10_full.data = cifar10_full.data[::10]\n",
        "        cifar10_full.targets = cifar10_full.labels[::10]\n",
        "\n",
        "      total_size, *self.dims = cifar10_full.data.shape\n",
        "      train_size, val_size = self.get_split_sizes(self.train_size, total_size)\n",
        "\n",
        "      split_generator = torch.Generator().manual_seed(self.seed)\n",
        "      self.train, self.val = torch.utils.data.random_split(\n",
        "          cifar10_full, [train_size, val_size], split_generator)\n",
        "\n",
        "    if stage in (None, \"test\"):\n",
        "      self.test = torchvision.datasets.CIFAR10(self.data_dir, train=False,\n",
        "                                               transform=self.transform)\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    trainloader = torch.utils.data.DataLoader(self.train, batch_size=self.batch_size,\n",
        "                                              shuffle=True, num_workers=2, pin_memory=True)\n",
        "    return trainloader\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    valloader = torch.utils.data.DataLoader(self.val, batch_size=self.batch_size,\n",
        "                                            shuffle=False, num_workers=2, pin_memory=True)\n",
        "    return valloader\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    testloader = torch.utils.data.DataLoader(self.test, batch_size=self.batch_size,\n",
        "                                             shuffle=False, num_workers=2, pin_memory=True)\n",
        "    return testloader\n",
        "\n",
        "  @staticmethod\n",
        "  def get_split_sizes(train_size, total_size):\n",
        "    if isinstance(train_size, float):\n",
        "      train_size = floor(total_size * train_size)\n",
        "\n",
        "    if isinstance(train_size, int):\n",
        "      val_size = total_size - train_size\n",
        "\n",
        "    return train_size, val_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fARC45YuOdlO"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFxVxKmeKJY7"
      },
      "source": [
        "###\n",
        "# Shape Handling and Inference\n",
        "###\n",
        "\n",
        "# when building a random architecture, we have to take care to track the shapes\n",
        "#  programmatically\n",
        "\n",
        "def sequential_output_shape(self, h_w):\n",
        "  \"\"\"Utility function for computing the output shape of a torch.nn.Sequential\"\"\"\n",
        "  for element in self:\n",
        "    try:\n",
        "      h_w = element.output_shape(h_w)\n",
        "    except AttributeError:  # optimistically assume any layer without the method doesn't change shape\n",
        "      pass\n",
        "  \n",
        "  return h_w\n",
        "\n",
        "\n",
        "def sequential_feature_dim(self):\n",
        "\n",
        "  for element in reversed(self):\n",
        "    try:\n",
        "      feature_dim = element.feature_dim()\n",
        "      if feature_dim is not None:\n",
        "        return feature_dim\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "\n",
        "def conv2d_output_shape(self, h_w):\n",
        "  \"\"\"Utility function for computing output shape of 2d convolutional operators.\"\"\"\n",
        "\n",
        "  props = self.kernel_size, self.stride, self.padding, self.dilation  # grab operator properties\n",
        "  props = [tuple((p, p)) if not isinstance(p, tuple) else p for p in props]  # diagonalize into tuples as needed\n",
        "  props = list(zip(*props))  # \"transpose\" operator properties -- list indices are height/width rather than property id\n",
        "\n",
        "  h = conv1d_output_shape(h_w[0], *props[0])  # calculate h from height parameters of props\n",
        "  w = conv1d_output_shape(h_w[1], *props[1])  # calculate w from width parameters of props\n",
        "\n",
        "  assert (h > 0) & (w > 0), \"Invalid parameters\"\n",
        "\n",
        "  return h, w\n",
        "\n",
        "\n",
        "def conv1d_output_shape(lngth, kernel_size, stride, padding, dilation):\n",
        "  \"\"\"Computes the change in dimensions for a 1d convolutional operator.\"\"\"\n",
        "  return floor( ((lngth + (2 * padding) - ( dilation * (kernel_size - 1) ) - 1 )/ stride) + 1)\n",
        "\n",
        "\n",
        "torch.nn.AdaptiveAvgPool2d.output_shape = lambda self, h_w: self.output_size\n",
        "torch.nn.Linear.output_shape = lambda self, inp: self.out_features\n",
        "torch.nn.Conv2d.output_shape = conv2d_output_shape\n",
        "torch.nn.MaxPool2d.output_shape = conv2d_output_shape\n",
        "torch.nn.Sequential.output_shape = sequential_output_shape\n",
        "\n",
        "torch.nn.Linear.feature_dim = lambda self: self.out_features\n",
        "torch.nn.Conv2d.feature_dim = lambda self: self.out_channels\n",
        "torch.nn.Sequential.feature_dim = sequential_feature_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy2li233QmPd"
      },
      "source": [
        "class CNN(utils.LoggedImageClassifierModule):\n",
        "  \"\"\"A simple CNN Model, with under-the-hood wandb\n",
        "  and pytorch-lightning features (logging, metrics, etc.).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, labels, config):\n",
        "    super().__init__(labels=labels)\n",
        "\n",
        "    self.loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    self.optimizer = config[\"optimizer\"]\n",
        "    self.optimizer_params = config[\"optimizer.params\"]\n",
        "\n",
        "    self.input_channels = 3\n",
        "    self.num_classes = 10\n",
        "\n",
        "    self.resizing_shape = (128, 128)\n",
        "    self.resize_layer = torch.nn.AdaptiveAvgPool2d(self.resizing_shape)\n",
        "\n",
        "    # Build conv body \n",
        "    conv_config = filter_to_subconfig(config, \"conv\")\n",
        "    self.conv = build_conv_from_config(\n",
        "        conv_config, self.input_channels)\n",
        "\n",
        "    # Infer shape of Conv -> FC transtion\n",
        "    self.conv_feature_dim = self.conv.feature_dim()\n",
        "    self.final_shape = self.conv.output_shape(self.resizing_shape)\n",
        "    self.final_size = self.final_shape[0] * self.final_shape[1] * self.conv_feature_dim\n",
        "\n",
        "    # Build FC block\n",
        "    fc_config = filter_to_subconfig(config, \"fc\")\n",
        "    self.classifier = build_fc_from_config(\n",
        "        fc_config, self.final_size)\n",
        "\n",
        "    # Add classifier head\n",
        "    self.classifier.add_module(\"classification\",  # handle empty linear case\n",
        "        torch.nn.Linear(self.classifier.output_shape(self.final_size), self.num_classes))\n",
        "\n",
        "  def forward(self, xs):\n",
        "    xs = self.resize_layer(xs)\n",
        "\n",
        "    xs = self.conv(xs)\n",
        "\n",
        "    xs = xs.view(-1, self.final_size)\n",
        "\n",
        "    xs = self.classifier(xs)\n",
        "\n",
        "    return xs\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return self.optimizer(self.parameters(), **self.optimizer_params)\n",
        "\n",
        "##\n",
        "# Building Networks from Configuration Dictionaries\n",
        "##\n",
        "\n",
        "# This section defines the logic for building modules from a configuration\n",
        "#  and for hooking them together\n",
        "\n",
        "def build_conv_from_config(config, in_channels):\n",
        "  conv = []\n",
        "  for block in range(config[\"n_blocks\"]):\n",
        "    block_config = config[f\"block_{block}\"]\n",
        "    conv_block = build_block_from_config(block_config, in_channels)\n",
        "    in_channels = conv_block.feature_dim()\n",
        "    conv.append(conv_block)\n",
        "\n",
        "  conv = torch.nn.Sequential(*conv)\n",
        "  conv.feature_dim = lambda : in_channels\n",
        "\n",
        "  return conv\n",
        "\n",
        "\n",
        "def build_fc_from_config(fc_config, in_features):\n",
        "  fc = []\n",
        "  for layer in range(fc_config[\"n_layers\"]):\n",
        "    layer_config = fc_config[f\"layer_{layer}\"]\n",
        "    fc_layer = torch.nn.Linear(in_features=in_features, **layer_config)\n",
        "    in_features = fc_layer.out_features\n",
        "    fc.append(fc_layer)\n",
        "    if fc_config[\"batchnorm_pre\"]:\n",
        "      fc.append(torch.nn.BatchNorm1d(in_features))\n",
        "    fc.append(fc_config[\"activation\"]())\n",
        "    if fc_config[\"batchnorm\"] and not fc_config[\"batchnorm_pre\"]:\n",
        "      fc.append(torch.nn.BatchNorm1d(in_features))\n",
        "    if fc_config[\"dropout\"]:\n",
        "      fc.append(torch.nn.Dropout(fc_config[\"dropout\"]))\n",
        "\n",
        "  fc = torch.nn.Sequential(*fc)\n",
        "  fc.feature_dim = lambda : in_features\n",
        "\n",
        "  return fc\n",
        "\n",
        "\n",
        "def build_block_from_config(block_config, in_channels):\n",
        "  conv_block = []\n",
        "  for layer in range(block_config[\"n_convs\"]):\n",
        "    conv_config = block_config[f\"layer_{layer}\"]\n",
        "    conv = torch.nn.Conv2d(in_channels, **conv_config)\n",
        "    in_channels = conv.out_channels\n",
        "    conv_block.append(conv)\n",
        "    if block_config[\"batchnorm_pre\"]:\n",
        "      conv_block.append(torch.nn.BatchNorm2d(in_channels))\n",
        "    conv_block.append(block_config[\"activation\"]())\n",
        "    if block_config[\"batchnorm\"] and not block_config[\"batchnorm_pre\"]:\n",
        "      conv_block.append(torch.nn.BatchNorm2d(in_channels))\n",
        "    if block_config[\"dropout\"]:\n",
        "      conv_block.append(torch.nn.Dropout2d(block_config[\"dropout\"]))\n",
        "\n",
        "  conv_block = torch.nn.Sequential(*conv_block)\n",
        "  conv_block.feature_dim = lambda : in_channels\n",
        "  return conv_block\n",
        "\n",
        "\n",
        "def filter_to_subconfig(config, prefix):\n",
        "  return config[prefix] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGOKO9JuzFYy"
      },
      "source": [
        "###\n",
        "# Generating a Random Architecture from a Fixed Seed\n",
        "###\n",
        "\n",
        "# This section maps a seed value to an architecture.\n",
        "#  The seed can be any valid Python seed; the public sweep uses integers.\n",
        "\n",
        "import random\n",
        "def randbool(): return bool(random.randint(0, 1))\n",
        "\n",
        "def generate_random_config(seed):\n",
        "  p_batchnorm = 0.67\n",
        "  max_dropout = 0.5\n",
        "  random.seed(seed)\n",
        "\n",
        "  config = {}\n",
        "  config[\"conv\"], config[\"fc\"] = {}, {}\n",
        "\n",
        "  config[\"conv\"][\"n_blocks\"] = random.choice([0, 1, 1, 2, 2, 4, 4, 4])\n",
        "  config[\"fc\"][\"n_layers\"] = random.choice([0, 1, 2, 2, 2, 4, 8])\n",
        "\n",
        "  config[\"conv\"][\"batchnorm\"] = random.random() < p_batchnorm\n",
        "  config[\"conv\"][\"batchnorm_pre\"] = randbool() if config[\"conv\"][\"batchnorm\"] else None\n",
        "\n",
        "  config[\"fc\"][\"batchnorm\"] = random.random() < p_batchnorm\n",
        "  config[\"fc\"][\"batchnorm_pre\"] = randbool() if config[\"fc\"][\"batchnorm\"] else None\n",
        "\n",
        "  config[\"fc\"][\"dropout\"] = random.random() * max_dropout if randbool() else None\n",
        "  config[\"conv\"][\"dropout\"] = random.random() * max_dropout if randbool() else None\n",
        "\n",
        "  config[\"conv\"][\"activation\"] = random.choice([torch.nn.ReLU, torch.nn.GELU, torch.nn.Sigmoid, torch.nn.SiLU])\n",
        "  config[\"fc\"][\"activation\"] = random.choice([torch.nn.ReLU, torch.nn.GELU, torch.nn.Sigmoid, torch.nn.SiLU])\n",
        "\n",
        "  for block in range(config[\"conv\"][\"n_blocks\"]):\n",
        "    block_config = generate_random_conv_block_config(shared_config=config[\"conv\"], index=block)\n",
        "    config[\"conv\"][f\"block_{block}\"] = block_config\n",
        "\n",
        "  for layer in range(config[\"fc\"][\"n_layers\"]):\n",
        "    layer_config = generate_random_fc_layer_config(shared_config=config[\"fc\"])\n",
        "    config[\"fc\"][f\"layer_{layer}\"] = layer_config\n",
        "\n",
        "  return config\n",
        "\n",
        "\n",
        "def generate_random_conv_block_config(shared_config, index):\n",
        "  block_config = {}\n",
        "  block_config[\"activation\"] = shared_config[\"activation\"]\n",
        "  block_config[\"batchnorm\"], block_config[\"batchnorm_pre\"] = shared_config[\"batchnorm\"], shared_config[\"batchnorm_pre\"]\n",
        "  block_config[\"dropout\"] = shared_config[\"dropout\"]\n",
        "\n",
        "  block_config[\"n_convs\"] = random.randint(1, 2)\n",
        "  block_config[\"n_channels\"] = random.choice([16, 32, 128])\n",
        "\n",
        "  for layer in range(block_config[\"n_convs\"]):\n",
        "    block_config[f\"layer_{layer}\"] = generate_random_conv_config(n_channels=block_config[\"n_channels\"])\n",
        "    \n",
        "  return block_config\n",
        "\n",
        "\n",
        "def generate_random_fc_layer_config(shared_config):\n",
        "  fc_layer_config = {}\n",
        "  fc_layer_config[\"out_features\"] = random.choice([16, 32, 128])\n",
        "  return fc_layer_config\n",
        "\n",
        "\n",
        "def generate_random_conv_config(n_channels):\n",
        "  conv_config = {}\n",
        "  conv_config[\"out_channels\"] = n_channels\n",
        "  conv_config[\"kernel_size\"] = generate_random_tuple_diag_bias(lambda : random.choice([1, 3, 3, 3, 5, 7]))\n",
        "  conv_config[\"stride\"] = generate_random_tuple_diag_bias(lambda : random.choice([1, 1, 1, 2, 2, 3]))\n",
        "  conv_config[\"dilation\"] = generate_random_tuple_diag_bias(lambda : random.choice([1, 1, 1, 1, 2]))\n",
        "\n",
        "  return conv_config\n",
        "\n",
        "\n",
        "def generate_random_tuple_diag_bias(sampler):\n",
        "  tupl = sampler()\n",
        "  if randbool():\n",
        "    tupl = (tupl, sampler())\n",
        "  else:\n",
        "    tupl = (tupl, tupl)\n",
        "\n",
        "  return tupl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSxCtOPRQIsB"
      },
      "source": [
        "# Define Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGVOkgx3ss2g"
      },
      "source": [
        "def train():\n",
        "  labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "            \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "  with wandb.init() as run:\n",
        "\n",
        "    wandb.config.update({\"seed\": 117})\n",
        "    config = generate_random_config(wandb.config.seed)\n",
        "    config.update({\n",
        "        \"optimizer\": torch.optim.Adam,\n",
        "        \"optimizer.params\": {\"lr\": 0.0003},\n",
        "        \"batch_size\": 128,\n",
        "        \"max_epochs\": 2,\n",
        "    })\n",
        "    wandb.config.update(config)\n",
        "\n",
        "    dm = CIFAR10DataModule(batch_size=config[\"batch_size\"])\n",
        "    cnn = CNN(labels, config)\n",
        "    \n",
        "    # logs the input weights to Weights & Biases\n",
        "    filter_logger = utils.FilterLogCallback(image_size=(3,) + cnn.resizing_shape,\n",
        "                                            log_input=True, log_output=False)\n",
        "  \n",
        "    # ðŸ‘Ÿ configure Trainer \n",
        "    trainer = pl.Trainer(gpus=1,  # use the GPU for .forward\n",
        "                        logger=pl.loggers.WandbLogger(\n",
        "                          log_model=True, save_code=True),  # log to Weights & Biases\n",
        "                        max_epochs=config[\"max_epochs\"], log_every_n_steps=1,\n",
        "                        callbacks=[filter_logger],\n",
        "                        progress_bar_refresh_rate=50)\n",
        "                        \n",
        "    # ðŸƒâ€â™€ï¸ run the Trainer on the model\n",
        "    trainer.fit(cnn, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2ko3gcoPAWd"
      },
      "source": [
        "# Join the Parallel Architecture Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8R3o5DcOgEr"
      },
      "source": [
        "Execute this cell to start running an \"agent\" that can participate\n",
        "in the architecture search.\n",
        "\n",
        "The results from the large public sweep for this project are\n",
        "[here](https://wandb.ai/wandb/archsearch-cifar10/sweeps/bmhxqxr0).\n",
        "\n",
        "> Note: this cell will run forever unless stopped.\n",
        "If you leave the notebook running for longer than an hour or two,\n",
        "it will be automatically shut down by Google and,\n",
        "especially if this occurs more than once in a short time,\n",
        "you may see your access to Colab GPUs restricted.\n",
        "To avoid this, change the `count` argument to an integer,\n",
        "somewhere near `20`, and the cell will finish running in 20 - 30 minutes,\n",
        "after executing that many training runs.\n",
        "\n",
        "You can also launch your own personal version of this search.\n",
        "Skip over this cell and read the cells following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1L-GRdl1-ZU"
      },
      "source": [
        "wandb.agent(sweep_id=\"bmhxqxr0\", function=train, count=None,\n",
        "            entity=\"wandb\", project=\"archsearch-cifar10\")\n",
        "# default id bmhxqxr0 for the public sweep "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mJ5UA0CtCr5"
      },
      "source": [
        "# Sweep Init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9XB2TfxPlaK"
      },
      "source": [
        "To start up a separate\n",
        "architecture search of your own,\n",
        "run the following two cells\n",
        "and then change the `sweep_id` in the cell above\n",
        "to the output of the second cell below\n",
        "before executing it.\n",
        "\n",
        "You can change the `entity` from `wandb` to your username\n",
        "if you want the sweep to be among your personal (and optionally private)\n",
        "projects.\n",
        "You'll want to make the same change in the `wandb.agent` cell as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b_k4uJG0EWt"
      },
      "source": [
        "sweep_config = {\"method\": \"random\",\n",
        "                \"metric\": \"validation/accuracy\",\n",
        "                \"goal\": \"maximize\",\n",
        "                \"parameters\": {\n",
        "                  \"seed\":{ \n",
        "                    \"distribution\": \"int_uniform\",\n",
        "                    \"min\": 0,\n",
        "                    \"max\": 10000000\n",
        "                    }\n",
        "                  },\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GuKk-CZtD5K"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"wandb\", project=\"archsearch-cifar10\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}