{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c216tB5gbTG8"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# A Convolutional Network for MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN1M5zGebb9J"
      },
      "source": [
        "## Installing and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu5Gebw6uvDF"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning==1.3.8 torchviz wandb\n",
        "!git clone https://github.com/wandb/lit_utils\n",
        "!cd \"/content/lit_utils\" && git pull\n",
        "\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "import lit_utils as lu\n",
        "\n",
        "lu.utils.filter_warnings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGBSszF7IZSC"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM1iZhbQbfyy"
      },
      "source": [
        "## Defining the `Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElHWRKqcu8oL"
      },
      "source": [
        "class LitCNN(lu.nn.modules.LoggedImageClassifierModule):\n",
        "  \"\"\"A simple CNN Model, with under-the-hood wandb and pytorch-lightning features (logging, metrics, etc.).\"\"\"\n",
        "\n",
        "  def __init__(self, config):  # make the model\n",
        "    super().__init__()\n",
        "\n",
        "    # first, convolutional component\n",
        "    self.conv_layers = torch.nn.Sequential(*[  # specify our LEGOs. edit this by adding to the list!\n",
        "      # hidden conv layer\n",
        "      lu.nn.conv.Convolution2d(\n",
        "        in_channels=1, kernel_size=config[\"kernel_size\"],\n",
        "        activation=config[\"activation\"],\n",
        "        out_channels=config[\"conv.channels\"][0]),\n",
        "      # hidden conv layer\n",
        "      lu.nn.conv.Convolution2d(\n",
        "        in_channels=config[\"conv.channels\"][0], kernel_size=config[\"kernel_size\"],\n",
        "        activation=config[\"activation\"],\n",
        "        out_channels=config[\"conv.channels\"][1]),\n",
        "      # pooling often follows 2 convs\n",
        "      torch.nn.MaxPool2d(config[\"pool_size\"]),\n",
        "    ])\n",
        "\n",
        "\n",
        "    # need a fixed-size input for fully-connected component,\n",
        "    #  so apply a \"re-sizing\" layer, to size set in config\n",
        "    self.resize_layer = torch.nn.AdaptiveAvgPool2d(\n",
        "      (config[\"final_height\"], config[\"final_width\"]))\n",
        "\n",
        "    # now, we can apply our fully-connected component\n",
        "    final_size = config[\"final_height\"] * config[\"final_width\"] * config[\"conv.channels\"][-1]\n",
        "    self.fc_layers = torch.nn.Sequential(*[ # specify our LEGOs. edit this by adding to the list!\n",
        "      lu.nn.fc.FullyConnected(\n",
        "        in_features=final_size, activation=config[\"activation\"],\n",
        "        out_features=config[\"fc.size\"][0]),\n",
        "      lu.nn.fc.FullyConnected(\n",
        "        in_features=config[\"fc.size\"][0], activation=config[\"activation\"],\n",
        "        out_features=config[\"fc.size\"][1]),\n",
        "      lu.nn.fc.FullyConnected(\n",
        "        in_features=config[\"fc.size\"][-1],  # \"read-out\" layer\n",
        "        out_features=10),\n",
        "    ])\n",
        "\n",
        "    self.loss = config[\"loss_fn\"]\n",
        "    self.optimizer = config[\"optimizer\"]\n",
        "    self.optimizer_params = config[\"optimizer.params\"]\n",
        "    config.update({f\"channels_{ii}\": channels\n",
        "                   for ii, channels in enumerate(config[\"conv.channels\"])})\n",
        "\n",
        "  def forward(self, x):  # produce outputs\n",
        "    # first apply convolutional layers\n",
        "    for layer in self.conv_layers: \n",
        "      x = layer(x)\n",
        "\n",
        "    # then convert to a fixed-size vector\n",
        "    x = self.resize_layer(x)\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "    # then apply the fully-connected layers\n",
        "    for layer in self.fc_layers: # snap together the LEGOs\n",
        "      x = layer(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq_qDw3Ubv8d"
      },
      "source": [
        "## Choosing hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUXHdz2BPko3"
      },
      "source": [
        "config = {\n",
        "  \"batch_size\": 256,\n",
        "  \"train_size\": 1024,  # reducing to a small subset to observe overfitting; set to 50000 for full dataset\n",
        "  \"max_epochs\": 15,\n",
        "  \"kernel_size\": 7,\n",
        "  \"conv.channels\": [256, 512],\n",
        "  \"pool_size\": 2,\n",
        "  \"final_height\": 8,\n",
        "  \"final_width\": 8,\n",
        "  \"fc.size\": [4096, 2048],\n",
        "  \"activation\": torch.nn.ReLU(),\n",
        "  \"loss_fn\": torch.nn.CrossEntropyLoss(),  # cross-entropy loss\n",
        "  \"optimizer\": torch.optim.Adam,\n",
        "  \"optimizer.params\": {\"lr\": 0.0001},\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yJ_L28JJbF3"
      },
      "source": [
        "dmodule = lu.datamodules.MNISTDataModule(batch_size=config[\"batch_size\"])\n",
        "lcnn = LitCNN(config)\n",
        "dmodule.prepare_data()\n",
        "dmodule.setup()\n",
        "dmodule.training_data = torch.utils.data.Subset(  \n",
        "  dmodule.training_data, indices=range(config[\"train_size\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo9wuMQZe4y2"
      },
      "source": [
        "### Debugging Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVoS6dMzlaE9"
      },
      "source": [
        "# for debugging purposes (checking shapes, etc.), make these available\n",
        "dloader = dmodule.train_dataloader()  # set up the Loader\n",
        "\n",
        "example_batch = next(iter(dloader))  # grab a batch from the Loader\n",
        "example_x, example_y = example_batch[0].to(\"cuda\"), example_batch[1].to(\"cuda\")\n",
        "\n",
        "print(f\"Input Shape: {example_x.shape}\")\n",
        "print(f\"Target Shape: {example_y.shape}\")\n",
        "\n",
        "lcnn.to(\"cuda\")\n",
        "outputs = lcnn.forward(example_x)\n",
        "print(f\"Output Shape: {outputs.shape}\")\n",
        "print(f\"Loss : {lcnn.loss(outputs, example_y)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi1VaHnue9jX"
      },
      "source": [
        "### Running `.fit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0RIrZI-LNeN"
      },
      "source": [
        "with wandb.init(project=\"lit-cnn\", entity=\"wandb\", config=config):\n",
        "  # ðŸªµ configure logging\n",
        "  cbs=[lu.callbacks.WandbCallback(),  # callbacks add extra features, like better logging\n",
        "       lu.callbacks.FilterLogCallback(image_size=(28, 28), log_input=True),  # this one logs the weights as images\n",
        "       lu.callbacks.ImagePredLogCallback(labels=dmodule.classes, on_train=True)  # and this one logs the inputs and outputs\n",
        "       ]\n",
        "  wandblogger = pl.loggers.WandbLogger(save_code=True)\n",
        "  if hasattr(lcnn, \"_wandb_watch_called\") and not lcnn._wandb_watch_called:\n",
        "    wandblogger.watch(lcnn)  # track gradients\n",
        "\n",
        "  # ðŸ‘Ÿ configure Trainer \n",
        "  trainer = pl.Trainer(gpus=1,  # use the GPU for .forward\n",
        "                       logger=wandblogger,  # log to Weights & Biases\n",
        "                       callbacks=cbs,  # use callbacks to log lots of run data\n",
        "                       max_epochs=config[\"max_epochs\"], log_every_n_steps=1,\n",
        "                       progress_bar_refresh_rate=50)\n",
        "\n",
        "  # ðŸƒâ€â™€ï¸ run the Trainer on the model\n",
        "  trainer.fit(lcnn, datamodule=dmodule)\n",
        "\n",
        "  # ðŸ§ª test the model on unseen data\n",
        "  trainer.test(lcnn, datamodule=dmodule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGhZuCABR8r0"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "#### **Exercise**: Compare the validation loss and training loss, what do you see? Do you notice any overfitting? What can you do reduce overfitting? \n",
        "> _Hint:_ look at the `dropout` keyword argument of the `lu.nn.conv.Convolution2d` and `lu.nn.fc.FullyConnected` modules. Where do you think it will be most effective at reducing over-fitting?\n",
        "\n",
        "#### **Exercise**: Notice your model's parameter count and compare it to the number of datapoints (`config[\"train_size\"]`). Similarly, compare the total size of the network's parameters (`size_mb`) to the total size of the dataset (for a training set size of `1024`, it's about 1 MB). Can you make the parameter count and disk size smaller without reducing performance?\n",
        "> _Hint:_ try reducing the size of the weight matrix for the fully-connected layer. What are the two ways to control the size of that matrix?\n",
        "\n",
        "#### **Exercise**: How would you make this network deeper? Add layers to the `conv`olutional component, the`f`ully-`c`onnected component, and both. Try to do so while not increasing the parameter count (i.e. reduce the number of channels and the output size of the fully-connected components when you add more layers). Does this impact performance on the training set? What about on the validation and test sets?\n",
        "\n",
        "#### **Exercise**: After increasing the depth enough, you should start to notice the training performance decrease, even to chance. Optimization of deeper networks is often more prone to error, but there are fixes. Look into the `batchnorm` argument of the `lu.nn.conv.Convolution2d` and `lu.nn.fc.FullyConnected` modules and the [Batch Norm layer](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html). Set `batchnorm` to `post` for a network that's deep enough to show optimization problems. Does this help?"
      ]
    }
  ]
}