{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/prompting/part_1_prompting/prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{beginner-llm-prompting-course} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompting Workshop with Weights and Biases - [Anish Shah](https://www.linkedin.com/in/anish-shah/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkukrJ2UjfZt"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install set-env-colab-kaggle-dotenv -q\n",
        "!pip install weave -U -q\n",
        "!pip install litellm -U -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !git clone https://github.com/wandb/llm-workshop-fc2024.git\n",
        "    %cd llm-workshop-fc2024/part_1_prompting\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu7OtFu3jekl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from text_formatting import render\n",
        "from set_env import set_env\n",
        "set_env(\"ANTHROPIC_API_KEY\")\n",
        "set_env(\"WANDB_API_KEY\")\n",
        "set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e8WkuQrkGvY"
      },
      "source": [
        "Welcome to the Prompting Workshop with Weights and Biases, led by Anish Shah. This workshop is designed to explore the fascinating world of prompt engineering, a crucial aspect of interacting with and leveraging the capabilities of large language models (LLMs). Throughout this session, we'll dive into various techniques for crafting effective prompts that can significantly enhance the performance of LLMs across a wide range of tasks.\n",
        "\n",
        "Whether you're new to AI and machine learning or looking to deepen your understanding of prompt engineering, this workshop will provide you with valuable insights and practical skills. By the end of this session, you'll be equipped to design and implement prompts that effectively communicate your intentions to LLMs, enabling more accurate and relevant responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section of the notebook focuses on setting up the environment and installing the required libraries:\n",
        "\n",
        "- It installs the [weave](https://wandb.github.io/weave/) which is used for tracking llm model operations\n",
        "- It installs the `litellm` which is used to standardize model interaction and also make it easy to swap model providers\n",
        "- Some accessory functions are provided for better rendering and environment variable setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import weave\n",
        "from litellm import completion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model and Prompt Configuration\n",
        "The code snippets define important configuration variables for the prompting workshop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# These variables store the names of different language models from Anthropic and OpenAI. \n",
        "# The \"SMART\" models (`claude-3-opus` and `gpt-4-turbo`) are more capable but slower, \n",
        "# while the \"FAST\" models (`claude-3-haiku` and `gpt-3.5-turbo`) are faster but less powerful.\n",
        "ANTHROPIC_SMART_MODEL_NAME = \"claude-3-opus-20240229\"\n",
        "ANTHROPIC_FAST_MODEL_NAME = \"claude-3-haiku-20240307\"\n",
        "OPENAI_SMART_MODEL_NAME = \"gpt-4-turbo-2024-04-09\"\n",
        "OPENAI_FAST_MODEL_NAME = \"gpt-3.5-turbo\"\n",
        "\n",
        "# These variables point to two different markdown files containing prompt engineering guides. \n",
        "# `AMAN_PROMPT_GUIDE` refers to Aman Chadha's guide, while `LILIAN_PROMPT_GUIDE` refers to Lilian Weng's guide.\n",
        "AMAN_PROMPT_GUIDE = \"aman_prompt_engineering.md\"\n",
        "LILIAN_PROMPT_GUIDE = \"lilianweng_prompt_engineering.md\"\n",
        "\n",
        "# Here, the `MODEL_NAME` variable is set to use Anthropic's fast model (`claude-3-haiku`), \n",
        "# and the `PROMPT_GUIDE` variable selects Lilian Weng's prompt engineering guide.\n",
        "MODEL_NAME = ANTHROPIC_SMART_MODEL_NAME\n",
        "PROMPT_GUIDE = LILIAN_PROMPT_GUIDE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These configuration variables allow workshop participants to easily switch between different models and prompt guides throughout the workshop by modifying the assigned values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializing Weave\n",
        "\n",
        "This line initializes the Weave library for the \"prompting-workshop\" project. Weave is a toolkit for developing Generative AI applications, providing features like logging, debugging, evaluations, and organization of LLM workflows. \n",
        "\n",
        "Initializing Weave at the start allows you to leverage its capabilities throughout your project, such as decorating Python functions with `@weave.op()` to enable automatic tracing and versioning.\n",
        "\n",
        "By specifying the project name \"prompting-workshop\", you are setting up a dedicated workspace for this workshop within Weave. This helps keep the workshop-related experiments, models, and data organized and separate from other projects.\n",
        "\n",
        "Weave brings structure and best practices to the experimental nature of Generative AI development, making it easier to track, reproduce, and share your work. Initializing it early in the notebook ensures you can take full advantage of its features as you progress through the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzV38KVSby6D"
      },
      "outputs": [],
      "source": [
        "weave.init(\"prompting-workshop-test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining the get_completion function\n",
        "\n",
        "This code defines a function called `get_completion` that is decorated with `@weave.op()`. The `@weave.op()` decorator is provided by Weave and enables automatic tracing and versioning of the function.\n",
        "\n",
        "The `get_completion` function takes several parameters:\n",
        "- `system_message`: The system message to provide context or instructions to the language model.\n",
        "- `messages`: A list of messages representing the conversation history.\n",
        "- `model_name`: The name of the language model to use (defaults to `MODEL_NAME`).\n",
        "- `max_tokens`: The maximum number of tokens to generate in the response (defaults to 4096).\n",
        "- `temperature`: The sampling temperature for controlling the randomness of the generated text (defaults to 0).\n",
        "\n",
        "Inside the function, it calls the `completion` function (from the `litellm` library) with the provided parameters to generate a completion from the language model. The `temperature` parameter is set to 0, which is recommended for evaluations and RAG (Retrieval-Augmented Generation) systems to ensure deterministic results.\n",
        "\n",
        "The generated response is then printed as JSON using `response.json()`, and the JSON response is returned by the function.\n",
        "\n",
        "By using the `@weave.op()` decorator, Weave automatically tracks and versions the inputs and outputs of the `get_completion` function, making it easier to reproduce and analyze the results later in the workshop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfuTRXK0b1vm"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def get_completion(system_message: str, messages: list, model: str, max_tokens: int = 4096, temperature: float = 0, **kwargs) -> dict:\n",
        "    \"\"\"\n",
        "    Generates a completion using the specified model, taking into account the system message, conversation history, and additional arguments.\n",
        "\n",
        "    Parameters:\n",
        "        system_message (str): A message providing context or instructions for the model.\n",
        "        messages (list): A list of dictionaries representing the conversation history, where each dictionary has keys 'role' and 'content'.\n",
        "        model (str): The identifier of the model to use for generating completions.\n",
        "        max_tokens (int, optional): The maximum number of tokens to generate in the completion. Defaults to 4096.\n",
        "        temperature (float, optional): The sampling temperature to control the randomness of the generated text. Defaults to 0.\n",
        "        **kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the generated completion as JSON.\n",
        "    \"\"\"\n",
        "    # Adjust messages format based on the model type\n",
        "    if \"gpt\" in model.lower():\n",
        "        formatted_messages = [{\"role\": \"system\", \"content\": system_message}] + messages\n",
        "    else:\n",
        "        kwargs[\"system\"] = system_message  # For non-gpt models, use system_message directly in kwargs\n",
        "\n",
        "    # Common arguments for the completion function\n",
        "    completion_args = {\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": formatted_messages if \"gpt\" in model.lower() else messages,\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "    # Generate and return the completion\n",
        "    response = completion(**completion_args)\n",
        "    return response.json()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use Case: Building a Prompting Assistant\n",
        "\n",
        "In this section, we explore the practical application of prompt engineering by building a bot that helps users understand prompting techniques and answers questions based on the provided information. This use case demonstrates the power of prompt engineering in creating helpful AI assistants that can make complex topics more accessible and engaging.\n",
        "\n",
        "By leveraging the knowledge contained in a comprehensive guide on prompting techniques, we can develop a bot that provides accurate and relevant answers to user queries. Through careful crafting of system messages and prompt templates, we ensure that the bot's responses are not only informative but also easy to understand, even for beginners.\n",
        "\n",
        "Throughout this use case, workshop participants will learn how to:\n",
        "\n",
        "- Incorporate context to improve the relevance and accuracy of the model's responses\n",
        "- Use system messages to guide the model's behavior and output style\n",
        "- Standardize inputs and outputs for consistent and reusable prompting assistants\n",
        "- Experiment with different configurations to optimize the bot's performance\n",
        "\n",
        "By engaging with this use case, participants will gain hands-on experience in applying prompt engineering techniques to build a practical and helpful AI assistant. They will develop a deeper understanding of how to effectively communicate with language models and tailor their outputs to specific audiences and use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Raw Prompting\n",
        "\n",
        "We start by sending a question to the language model without any additional context, using a basic `prompt_llm` function. This demonstrates the model's limitations when lacking the necessary information to provide relevant answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def prompt_llm(question: str, **kwargs) -> str:\n",
        "    \"\"\"\n",
        "    Sends a question to the language model and returns its response.\n",
        "\n",
        "    This function prepares a message with the user's question, handles additional\n",
        "    arguments for the language model, and invokes the get_completion function to\n",
        "    obtain a response. The response's content is then returned.\n",
        "\n",
        "    Parameters:\n",
        "        question (str): The question intended for the language model.\n",
        "        **kwargs: A dictionary of additional keyword arguments. Expected keys include 'system_message', 'model', 'max_tokens', and 'temperature'.\n",
        "\n",
        "    Returns:\n",
        "        str: The language model's response to the question.\n",
        "    \"\"\"\n",
        "    # Prepare the user's question for the language model\n",
        "    messages = [{\"role\": \"user\", \"content\": question}]\n",
        "    \n",
        "    # Extract additional parameters, applying defaults if necessary\n",
        "    system_message = kwargs.pop('system_message', \"\")\n",
        "    model = kwargs.pop('model', MODEL_NAME)\n",
        "    max_tokens = kwargs.pop('max_tokens', 4096)\n",
        "    temperature = kwargs.pop('temperature', 0)\n",
        "\n",
        "    # Compile arguments for the completion request\n",
        "    completion_args = {\n",
        "        \"system_message\": system_message,\n",
        "        \"messages\": messages,\n",
        "        \"model\": model,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    completion_args.update(kwargs)  # Properly include any other additional arguments\n",
        "\n",
        "    # Request a completion from the language model\n",
        "    response = get_completion(**completion_args)\n",
        "    \n",
        "    # Extract and return the content of the model's response\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_prompt_response = prompt_llm(\n",
        "    \"Explain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "render(raw_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model's response to the raw prompt is inadequate because it lacks the necessary context to provide a meaningful answer. Without any background information or specific details about prompting techniques, the model can only generate a generic, high-level response that fails to address the question effectively, many times providing no response at all.\n",
        "\n",
        "This poor performance highlights the importance of providing relevant context when prompting language models. By supplying the model with additional information related to the topic at hand, we can guide it towards generating more accurate, detailed, and useful responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Prompting with Context\n",
        "\n",
        "We can provide the necessary context to the language model by including it directly alongside the question. In this example, we use a comprehensive guide on prompting techniques written by [Lilian Weng](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/). This guide is particularly useful as it condenses many great papers and articles into a single-page resource, covering various prompting techniques.\n",
        "\n",
        "To incorporate the context, we:\n",
        "\n",
        "1. Load the markdown file containing the prompting guide using the `load_markdown_file` function.\n",
        "2. Concatenate the loaded context with the question in the `context_prompt_response` variable.\n",
        "3. Pass the combined context and question to the `prompt_llm` function to generate a response.\n",
        "\n",
        "By providing the model with relevant context, we expect to receive more accurate and informative answers to our questions about prompting techniques.\n",
        "\n",
        "Note: As an alternative, you can also use a more extensive guide by [Aman Chadha](https://aman.ai/primers/ai/prompt-engineering/) for additional context and information on prompt engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_markdown_file(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Reads and returns the content of a markdown file specified by its path.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to the markdown file to be read.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the markdown file as a string.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        markdown_content = file.read()\n",
        "    return markdown_content\n",
        "context = load_markdown_file(PROMPT_GUIDE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: Anthropic has an amazingly large context size and as a result we can luckily just shove the whole document into the prompt in this situation. This can get quite expensive however so it typically makes more sense to use techniques that chunk the document into better sizes or use a RAG based pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_prompt_response = prompt_llm(\n",
        "    context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "render(context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This response is a significant improvement! We can see that by providing the model with relevant context, it can generate an answer that includes details about various prompting techniques covered in the workshop. The model effectively utilizes the information from the provided guide to address the question more comprehensively.\n",
        "\n",
        "However, there is still room for improvement. The model tends to regurgitate the technical information from the guide without simplifying or explaining the concepts in an easily understandable manner. The response may be too complex or jargon-heavy for beginners or those new to the topic of prompt engineering.\n",
        "\n",
        "To address this issue, we need to guide the model towards providing explanations that are more accessible and beginner-friendly. This is where the next step of conditioning the model's responses with a carefully crafted system prompt comes into play. By instructing the model to break down the technical details and present the information in a more digestible format, we can ensure that the responses are not only informative but also easy to understand for a broader audience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Condition Responses with a System Prompt\n",
        "\n",
        "To ensure that the bot explains the information in a way that is easy to understand, we can provide a system prompt that guides the model to present the content in a beginner-friendly manner. Here’s why this system prompt is effective:\n",
        "\n",
        "1. **Objective Clarity**: It directly states the task — simplifying prompt engineering concepts with examples. This aligns with the principle of having a clear and specific objective, which helps the LLM focus on the exact task ([source](https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca)).\n",
        "\n",
        "2. **Tone Specification**: Setting a friendly and educational tone guides the LLM on the desired interaction style, making the information approachable and digestible.\n",
        "\n",
        "3. **Context Awareness**: By acknowledging the user's basic AI knowledge, the prompt tailors the complexity of the content, ensuring it is suitable for beginners without being overly simplistic.\n",
        "\n",
        "4. **Guidance on Style**: Instructing the use of analogies and simple examples helps in breaking down complex topics into understandable segments, which is crucial for teaching technical subjects effectively.\n",
        "\n",
        "5. **Verification of Output**: Emphasizing clarity and relevance ensures that the responses are not only correct but also useful and directly applicable to the user’s needs.\n",
        "\n",
        "6. **Highlighting Benefits**: Mentioning the benefits of simplifying technical concepts engages users by showing the value of what they are learning, enhancing their motivation and the educational impact.\n",
        "\n",
        "### General Approach to Constructing Effective System Prompts\n",
        "\n",
        "When constructing system prompts for LLMs, consider the following steps to ensure effectiveness and clarity:\n",
        "\n",
        "- **Define the Objective**: Clearly state what you want the LLM to achieve. This should be specific and concise.\n",
        "- **Set the Tone and Style**: Indicate how the response should feel or sound. This helps the LLM adjust its language and approach.\n",
        "- **Provide Necessary Context**: Include any background information that will help the LLM understand the scope and depth of the response required.\n",
        "- **Incorporate Guidance for Content**: Direct the LLM on how to structure its response or what elements to include, such as examples or analogies.\n",
        "- **Specify Output Format**: If necessary, define how the response should be formatted. This is particularly important for tasks requiring a specific output structure.\n",
        "- **Use Clear and Direct Language**: Avoid ambiguity by using straightforward and direct language. This reduces the chances of misinterpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"\"\"\n",
        "Objective: Simplify prompt engineering concepts for easy understanding. Provide clear examples for each technique.\n",
        "Tone: Friendly and educational, suitable for beginners.\n",
        "Context: Assume basic AI knowledge; avoid deep technical jargon.\n",
        "Guidance: Use metaphors and simple examples to explain concepts. Keep explanations concise and applicable.\n",
        "Verification: Ensure clarity and relevance in responses, with practical examples.\n",
        "Benefits: Help users grasp prompt engineering basics, enhancing their AI interaction experience.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_and_context_prompt_response = prompt_llm(\n",
        "    system_message=system_message,\n",
        "    question=context + \"\\n\\nExplain the latest prompting techniques and provide an example of each\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "render(system_and_context_prompt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! Now we're able to get a response that is easy to understand and provides a lot of context. The model has successfully broken down the technical concepts into beginner-friendly explanations, using simple language, analogies, and examples to convey the key points. This approach makes the information more accessible and engaging for those new to prompt engineering, fostering a deeper understanding of the subject matter.\n",
        "\n",
        "The next step is to standardize the inputs and outputs in a way that allows us to ask different questions and pass different context in the future. By creating a consistent structure for our prompts and responses, we can streamline the development of LLM applications and make it easier to experiment with various configurations. This standardization will enable us to quickly iterate on our prompts, test different contexts, and fine-tune our models to achieve the best possible results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: System Prompts - Inputs\n",
        "\n",
        "To make it easier to experiment with different parameters and retrieve our best models, we can wrap our prompting logic in a collection of modular functions decorated with `@weave.op()`. This allows us to track and version our operations, making it easier to reproduce and analyze our results. We can also define a standard input structure for our system prompts, ensuring consistency across different prompts and use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"{context}\\n{question}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def format_prompt(prompt_template: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Formats a prompt template with provided keyword arguments.\n",
        "\n",
        "    This function takes a template string and a dictionary of keyword arguments,\n",
        "    then formats the template string using these arguments.\n",
        "\n",
        "    Parameters:\n",
        "        prompt_template (str): The template string to be formatted.\n",
        "        **kwargs (dict): Keyword arguments to format the template string with.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted prompt template.\n",
        "    \"\"\"\n",
        "    return prompt_template.format(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In this app we assume only a context and question are passed to the prompt template but that need not be true\n",
        "@weave.op()\n",
        "def llm_app(prompt_template: str, context: str, question: str, **kwargs):\n",
        "    \"\"\"\n",
        "    Generates a response using a formatted prompt based on a template, context, and question.\n",
        "\n",
        "    This function formats a given prompt template with the specified context and question, then\n",
        "    generates a response using the prompt_llm function with additional keyword arguments.\n",
        "\n",
        "    Parameters:\n",
        "        prompt_template (str): The template string used to format the prompt.\n",
        "        context (str): The context information to be included in the prompt.\n",
        "        question (str): The specific question to be asked in the prompt.\n",
        "        **kwargs (dict): Additional keyword arguments to be passed to the prompt_llm function.\n",
        "\n",
        "    Returns:\n",
        "        str: A string representing the generated response.\n",
        "    \"\"\"\n",
        "    formatted_prompt = format_prompt(prompt_template=prompt_template, context=context, question=question)\n",
        "    response = prompt_llm(\n",
        "        question=formatted_prompt,\n",
        "        **kwargs\n",
        "    )\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We defined our llm_app which in turns acts as the core of our prompting assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "Explain the differences between zero-shot, few-shot, and chain of thought \n",
        "prompting techniques? Please provide a clear explanation and a practical example \n",
        "for each technique within a structured format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_template_response = llm_app(\n",
        "    system_message=system_message,\n",
        "    prompt_template=prompt_template,\n",
        "    context=context,\n",
        "    question=question,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "render(input_template_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can easily and consistently swap system messages, context, and questions to get the best results. This allows us to test various combinations of system messages, context, and questions to find the most effective prompts for our specific use case.\n",
        "\n",
        "However, as we can see from the example, the current system prompt doesn't work as well with the newly asked question. This highlights the importance of tailoring the system prompt to the specific task at hand. Additionally, we may want to enforce more consistency in the format of our model's outputs. While using third-party packages like Instructor is beyond the scope of this workshop, we can achieve similar results by using proper tags in our prompt. By including specific tags or formatting instructions in the prompt, we can guide the model to respond in a way that is more consistent and easier to parse on our end\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: System Prompts - Outputs\n",
        "\n",
        "In this step, we focus on improving the consistency and structure of our model's outputs by modifying the prompt template. By including specific tags and formatting instructions in the prompt, we can guide the model to respond in a way that is easier to parse and process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the system message, we've added a new tag called `Format` which provides instructions for the model to respond within an <answer></answer> tag, with separate <explanation> and <example> tags for each concept. This structured format helps organize the information and makes it easier to extract and analyze the responses programmatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_prompt_with_output_indicator(system_message: str, prompt_template: str):\n",
        "    if \"gpt\" in MODEL_NAME:\n",
        "        system_format_msg = \"\"\"\n",
        "    Format: Respond within a structured JSON object, using the keys provided in the prompt to organize your response.\n",
        "    Provide a condensed answer under the 'condensed_answer' key, detailed explanations under 'explanation' keys, \n",
        "    and examples under 'example' keys within each explanation.\n",
        "    \"\"\"\n",
        "        prompt_format_msg = \"\"\"\n",
        "    You must respond in JSON format.\n",
        "    Your response should follow this structure:\n",
        "    {{ \n",
        "      \"answer\": {{\n",
        "        \"condensed_answer\": \"CONDENSED_ANSWER\",\n",
        "        \"explanation_1\": {{\n",
        "          \"detail\": \"EXPLANATION_1\",\n",
        "          \"example\": \"EXAMPLE_1\"\n",
        "        }},\n",
        "        \"explanation_2\": {{\n",
        "          \"detail\": \"EXPLANATION_2\",\n",
        "          \"example\": \"EXAMPLE_2\"\n",
        "        }},\n",
        "        ...\n",
        "      }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "    else:\n",
        "        system_format_msg = \"\"\"\n",
        "    Format: Respond within an <answer></answer> tag, with as many <explanation></explanation> tags as needed, \n",
        "    ensuring that the <detail></detail> and <example></example> tags are used within each <explanation></explanation> tag.\n",
        "    Provide a condensed answer for the question in the <condensed_answer></condensed_answer> tag.\n",
        "    \"\"\"\n",
        "        prompt_format_msg = \"\"\"\n",
        "    You must respond within an <answer></answer> XML tags.\n",
        "    Inside of the <answer> markdown tag, you must provide a format of\n",
        "    <answer>\n",
        "        <condensed_answer> CONDENSED_ANSWER </condensed_answer>\n",
        "        <explanation> \n",
        "          <detail> EXPLANATION </detail>\n",
        "          <example> EXAMPLE </example>\n",
        "        </explanation>\n",
        "        <explanation> \n",
        "          <detail> EXPLANATION </detail>\n",
        "          <example> EXAMPLE </example>\n",
        "        </explanation>\n",
        "        ...\n",
        "    </answer>\n",
        "    \"\"\"\n",
        "    formatted_system_message = system_message + \"\\n\" + system_format_msg\n",
        "    formatted_prompt_template = prompt_template + \"\\n\" + prompt_format_msg\n",
        "\n",
        "    return formatted_system_message, formatted_prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "formatted_system_message, formatted_prompt_template = update_prompt_with_output_indicator(system_message, prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, we've updated the `prompt_template` to include the new formatting instructions, ensuring that the model generates responses that adhere to the specified structure. By enforcing a consistent output format, we can streamline the processing of the model's responses and facilitate further analysis and evaluation of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"\"\"\n",
        "Explain the differences between zero-shot, few-shot, and chain of thought \n",
        "prompting techniques? Please provide a clear explanation and a practical example \n",
        "for each technique within a structured format.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_indicator_response = llm_app(\n",
        "    system_message=formatted_system_message,\n",
        "    prompt_template=formatted_prompt_template,\n",
        "    context=context,\n",
        "    question=question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "render(output_indicator_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Prompting Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_with_zero_shot_prompt(question: str):\n",
        "    zero_shot_instruction = \"Without using any specific examples, \"\n",
        "    return zero_shot_instruction + question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zero_shot_question = update_with_zero_shot_prompt(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zero_shot_response = llm_app(\n",
        "    system_message=formatted_system_message,\n",
        "    prompt_template=formatted_prompt_template,\n",
        "    context=context,\n",
        "    question=zero_shot_question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "render(zero_shot_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_with_few_shot_prompt(question: str):\n",
        "    if \"gpt\" in MODEL_NAME:\n",
        "        few_shot_examples = \"\"\"\n",
        "        Here are a few examples of prompting techniques in JSON format:\n",
        "        {{\n",
        "            \"answer\": {{\n",
        "                \"condensed_answer\": \"Different prompting techniques are used to guide language models in generating desired outputs.\",\n",
        "                \"explanation_1\": {{\n",
        "                    \"detail\": \"Translation prompts provide the model with a source language text and request the translation in a target language.\",\n",
        "                    \"example\": \"Translate the following English text to French: 'Hello, how are you?'\"\n",
        "                }},\n",
        "                \"explanation_2\": {{\n",
        "                    \"detail\": \"Sentiment classification prompts ask the model to determine the sentiment expressed in a given text.\",\n",
        "                    \"example\": \"Classify the sentiment of the following text: 'The movie was terrible.'\"\n",
        "                }},\n",
        "                \"explanation_3\": {{\n",
        "                    \"detail\": \"Factual question prompts require the model to provide an answer along with an explanation or reasoning.\",\n",
        "                    \"example\": \"What is the capital of Germany? Explain your reasoning.\"\n",
        "                }}\n",
        "            }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "    else:\n",
        "        few_shot_examples = \"\"\"\n",
        "        Here are a few examples of prompting techniques in XML format:\n",
        "        <answer>\n",
        "            <condensed_answer>Different prompting techniques are used to guide language models in generating desired outputs.</condensed_answer>\n",
        "            <explanation>\n",
        "                <detail>Translation prompts provide the model with a source language text and request the translation in a target language.</detail>\n",
        "                <example>Translate the following English text to Spanish: 'Good morning, how can I help you?'</example>\n",
        "            </explanation>\n",
        "            <explanation>\n",
        "                <detail>Sentiment classification prompts ask the model to determine the sentiment expressed in a given text.</detail>\n",
        "                <example>Classify the sentiment of the following text: 'I love this product!'</example>\n",
        "            </explanation>\n",
        "            <explanation>\n",
        "                <detail>Factual question prompts require the model to provide an answer along with an explanation or reasoning.</detail>\n",
        "                <example>What is the capital of Canada? Provide your thought process.</example>\n",
        "            </explanation>\n",
        "        </answer>\n",
        "        \"\"\"\n",
        "\n",
        "    return few_shot_examples + \"\\n\" + question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "few_shot_question = update_with_few_shot_prompt(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "few_shot_response = llm_app(\n",
        "    system_message=formatted_system_message,\n",
        "    prompt_template=formatted_prompt_template,\n",
        "    context=context,\n",
        "    question=few_shot_question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "render(few_shot_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chain of Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: We do not use the output indicators in this case as it will negate the chain of thought to instead enforce the formatting. It is important to explicitly incorporate the thought process desired in the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_with_chain_of_thought_prompt(system_message: str, prompt_template: str):\n",
        "    chain_of_thought_instruction = \"Let's explicitly think step by step. My thought process is:\\n\"\n",
        "    chain_of_thought_system_format = \"Format: You must explicitly define the thought process and knowledge from the context to come to your conclusion for the question.\"\n",
        "    return system_message + \"\\n\" + chain_of_thought_system_format, prompt_template + \"\\n\" + chain_of_thought_instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain_of_thought_system_message, chain_of_thought_prompt = update_with_chain_of_thought_prompt(system_message, prompt_template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chain_of_thought_response = llm_app(\n",
        "    system_message=chain_of_thought_system_message,\n",
        "    prompt_template=chain_of_thought_prompt,\n",
        "    context=context,\n",
        "    question=question,\n",
        "    # response_format={\"type\": \"json_object\"} # Comment this out for `Claude` models or `litellm.drop_params=True``\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "render(chain_of_thought_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
