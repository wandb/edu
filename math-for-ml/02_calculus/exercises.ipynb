{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math4ML Part II: Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hg3cotCfruZP"
   },
   "source": [
    "# Setup Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes setup code for the remaining sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --user -qq wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows interactive plotting\n",
    "%matplotlib widget \n",
    "\n",
    "# importing from standard library\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# importing libraries\n",
    "import autograd\n",
    "import autograd.numpy as np  # trick for automatic differentiation with numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchviz import make_dot as make_graph\n",
    "import wandb\n",
    "\n",
    "# importing course-specific modules\n",
    "import util\n",
    "import calculus as calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_models():\n",
    "    wandb.join()\n",
    "    model_names = [\"linear_model\", \"nonlinear_model\"]\n",
    "    for model_name in model_names:\n",
    "        if model_name in globals():\n",
    "            calc.models.cleanup(eval(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juTijuBZqn85"
   },
   "source": [
    "# Section 1. Derivatives and Optimization: Interactive Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trzfgwJcrJT3"
   },
   "source": [
    "## Visualizing the Gradient as an Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $f$ is defined as a function, $\\nabla f$,\n",
    "that, given an array $x$, returns an array of the same dimension and shape\n",
    "(scalar/vector/matrix/tensor, as appropriate)\n",
    "that can be used to define the best linear approximation to $f$ near the point $x$, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{f}_x(\\varepsilon) =  f(x) + \\langle\\nabla f(x), \\varepsilon\\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the definition of the gradient guarantees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x + \\varepsilon) = \\hat{f}_x(\\varepsilon) + o(\\|\\varepsilon\\|).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll visualize this pair of functions,\n",
    "adjusting the values of $x$ and $\\varepsilon$\n",
    "and observing the behavior of the linear approximation given by the gradient\n",
    "and the error of the approximation.\n",
    "\n",
    "The cell below first defines a few functions in `numpy`.\n",
    "You're welcome to define your own,\n",
    "simply tacking them onto the end of the list below.\n",
    "Thanks to the magic of automatic differentiation,\n",
    "just about any function implemented in `numpy`\n",
    "that takes in one number and returns one number\n",
    "will work: `np.sin`, `np.power`, etc.\n",
    "\n",
    "The following cell then creates an interactive plot for comparing the gradient-based linear approximation to the original function.\n",
    "\n",
    "A full description of the plot follows below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [lambda xs: np.cos(np.pi * xs),\n",
    "         lambda xs: np.exp(xs),\n",
    "         lambda xs: np.square(xs),\n",
    "         lambda xs: np.abs(xs),\n",
    "         lambda xs: np.where(xs > 0, xs, 0),\n",
    "         lambda xs: np.arctan(5 * xs)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = funcs[0]  # pick a function to visualize\n",
    "interactor = calc.grad_plot.setup(f,)\n",
    "interactor();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sliders allow you to independently adjust two parameters: $x$ (represented by a dark gray circle) and $x+\\varepsilon$ (light red circle). The plot then updates to display the approximation of $f$ given by the gradient at $x$ ($\\hat{f}$, gold line) and the error (vertical purple bar) between that approximation at $\\varepsilon$ and the true value: $\\hat{f}_x(\\varepsilon) - f(x + \\varepsilon)$. The value of $\\varepsilon$ is represented by a horizontal blue bar. \n",
    "\n",
    "For small differences $\\varepsilon$, the error and $\\varepsilon$ may not be visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should take some time to play with this visualization\n",
    "applied to the functions at `0`, `1`, and `2` before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the questions `funcs[0]`, `cos`ine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q For `funcs[0]`, the starting point is a maximum of the function. What impact does this have on the approximation given by the gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same function, adjust $x$ to a minimum of the function.\n",
    "\n",
    "#### Q Does the gradient look the same or different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient descent, we add a negative number times value of the gradient to the current point to obtain the next point.\n",
    "\n",
    "#### Q What would gradient descent do at each of the points tested above? Does this surprise you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still on `funcs[0]`, search for a point where the approximation given by the gradient is as close as possible to the true function. \n",
    "\n",
    "#### Q What do you notice about the function at this point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch to `funcs[1]`: the `exp`onential function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Which do you predict will have a larger error for $x+\\varepsilon = 0$: an approximation based on the gradient at $x=0.5$ or $x=-0.5$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your answer. Hint: you can click the numbers next to the sliders and type in a specific value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Try to formulate a theory of when the approximation based on the gradient will be good, in terms of the shape of the original function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the behavior of the gradients of `funcs[2]`, the `square` of $x$, with `funcs[3]`, the `abs`olute value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q At all points away from $0$, which of the two functions is better approximated by its gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What happens at $0$? Can you articulate why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also wish to check out `funcs[4]` while answering these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, `autograd` (and just about every automatic differentiation package)\n",
    "will return a value at $0$ for `funcs[3]` and `[4]`,\n",
    "even though mathematically the gradient is not defined there.\n",
    "\n",
    "This is because differentiation packages actually work with a slightly more generalized idea,\n",
    "[subderivatives](https://en.wikipedia.org/wiki/Subderivative).\n",
    "This will almost never matter, except that it makes most numerical algorithms \"do the right thing\" even when they are applied to gnarly functions that don't have gradients everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Relate your answers to your theory about when gradients make good approximations. If need be, update your theory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, return to `funcs[0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a new plot is generated, $x$ and $x+\\varepsilon$ are equal ($\\varepsilon$ is $0$).\n",
    "\n",
    "#### Q Use the definition of $o(\\varepsilon)$ to explain why the error is $0$. (Hint: what is $o(0)$?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change $x$ to `-0.9` and $x + \\varepsilon$ to `0.4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q How large is the error (by eye)? Explain this in terms of the meaning of $o(\\|\\varepsilon\\|)$. (Hint: is the value $0$ compatible with $o(\\varepsilon)$?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAc-8kXbq9LA"
   },
   "source": [
    "## Optimizing by Hand versus by Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a better intuitive feel for optimization by doing it yourself!\n",
    "\n",
    "The following cell will\n",
    "\n",
    "1. Generate some data and plot a line over it.\n",
    "1. Make the plot interactive, so you can adjust the slope and intercept (\"bias\") of the line with a slider.\n",
    "\n",
    "First, attempt to fit the data by eye: make it so that the line passes through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 2500\n",
    "input_values = np.linspace(-10, 10, num_datapoints)\n",
    "\n",
    "parameters = calc.models.make_linear_parameters()\n",
    "\n",
    "cleanup_models()\n",
    "linear_model = calc.models.LinearModel(input_values, parameters,\n",
    "                                       use_wandb=True, entity=\"charlesfrye\",\n",
    "                                       project=\"calculus\",\n",
    "                                       wandb_path=\"./calculus\")\n",
    "\n",
    "linear_model.plot()\n",
    "\n",
    "N = 30\n",
    "\n",
    "w, b = 0.6, 0.5  # slope, intercept\n",
    "noise_level = 3\n",
    "\n",
    "xs = np.random.standard_normal(N) * 2.5\n",
    "ys = [w * x + b + np.random.standard_normal() * noise_level for x in xs]\n",
    "\n",
    "linear_model.set_data(xs, ys)\n",
    "linear_model.make_interactive();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've found a fit you like, run the cell below this one. This will now cause the model to print the mean squared error whenever you change the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.show_MSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use machine learning to fit models to data, we need objective metrics like mean squared error in order to use mathematical optimization techniques like gradient descent. They are sometimes called objective functions, loss functions, or cost functions.\n",
    "\n",
    "They don't always match what look like good parameters by eye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Wiggle the parameters around a bit (you can do this more finely by clicking one of the sliders and then using the arrow keys). Does the MSE increase in any direction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed an output directing you to a\n",
    "[Weights & Biases](https://www.wandb.com/) \"run page\",\n",
    "where you can see a graph of the values of the parameters\n",
    "and their associated MSE loss.\n",
    "\n",
    "This might be helpful as you tackle the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Find the MSE-minimizing parameters. Try and articulate the strategy that you used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy you used need not look like gradient descent.\n",
    "\n",
    "Reset the parameters back to `0`\n",
    "(either on your own, or by re-running the figure-generating cell).\n",
    "Then, execute the cell below (and, if needed, scroll back up) to produce an animation of gradient descent adjusting the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.show_MSE = False\n",
    "linear_model.run_gd(n=25, delta_t=0.1, lr=5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the slope and intercept of the line changing in the graph\n",
    "(the values in the controlling sliders will not change)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q How does the behavior of gradient descent differ from your strategy for minimizing the error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below repeats the process above for one of two different nonlinear models.\n",
    "\n",
    "First, run the cell as is and fit the data by hand. Feel free to use the MSE to guide your optimization.\n",
    "Then, run the cell beneath it to run gradient descent on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_range = [-10, 10]\n",
    "input_values, parameters, transform = calc.models.setup_rectlin(theta_range=theta_range)\n",
    "\n",
    "# theta_range = [-6, 6]\n",
    "# input_values, parameters, transform = calc.models.setup_trig(np.sin, theta_range=theta_range)\n",
    "\n",
    "cleanup_models()\n",
    "nonlinear_model = calc.models.NonlinearModel(input_values, parameters, transform,\n",
    "                                             use_wandb=True, entity=\"charlesfrye\", project=\"calculus\",\n",
    "                                             wandb_path=\"./calculus\")\n",
    "\n",
    "N = 300\n",
    "theta = 2.1\n",
    "noise_level = 0.75\n",
    "\n",
    "xs = np.random.standard_normal(N) * 2\n",
    "\n",
    "ys = transform(theta, xs) + np.random.standard_normal(N) * noise_level\n",
    "\n",
    "nonlinear_model.plot()\n",
    "nonlinear_model.show_MSE = True\n",
    "nonlinear_model.set_data(xs, ys)\n",
    "nonlinear_model.make_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_model.show_MSE = False\n",
    "nonlinear_model.run_gd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the value of `theta` to `6`\n",
    "(either using the sliders or by hand)\n",
    "and then execute the cell ending in the call to `run_gd()`.\n",
    "\n",
    "#### Q What happens? Does gradient descent successfully reduce the model error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below will plot the error as a function of `theta` for the model you most recently ran above. This is the loss function for this model, the function we would like to optimize.\n",
    "\n",
    "#### Q Explain the behavior of gradient descent on this model. In your answer, try to make reference to the definition of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "thetas = np.linspace(*theta_range, num=500)\n",
    "plt.plot(thetas, [np.mean(np.square(ys - transform(theta, xs))) for theta in thetas], lw=3);\n",
    "plt.xlabel(\"theta\", fontsize=\"x-large\"); plt.ylabel(\"MSE loss\", fontsize=\"x-large\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the above questions for the `rect`ified `lin`ear model,\n",
    "comment out the first two lines and\n",
    "then uncomment the following two lines,\n",
    "which will set up a nonlinear model based on a `trig`onometric function.\n",
    "\n",
    "Follow all of the instructions again for this new dataset and model:\n",
    "fit the model by hand and with gradient descent,\n",
    "then answer the questions again.\n",
    "\n",
    "Compare and contrast the behavior of gradient descent on these two problems.\n",
    "Use the shape of the loss function, from the cell above,\n",
    "to aid in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKSRMd-lrVUG"
   },
   "source": [
    "## Section 2. How does Gradient Descent Behave?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, machine learning models depend on more than one parameter,\n",
    "and so to understand the behavior of gradient descent,\n",
    "we need to consider loss functions with multiple input dimensions,\n",
    "also known as loss *surfaces*.\n",
    "\n",
    "To draw a loss surface, we plot the value of the loss function at each combination of values for the inputs. Because we're able to, at most, make things in 3 dimensions, we'll have two input dimensions and leave the third for the loss.\n",
    "\n",
    "A surface is one way to generalize the familiar old idea of the graph of a function to functions that take more than one input.\n",
    "\n",
    "We live on a surface of this type, the surface of the Earth. If you want to think of it as a loss function, you could think of it as the loss function you'd have if you didn't like being at a high altitude, as a function of your latitude and longitude. Consider: what point on Earth would optimize this loss function?\n",
    "\n",
    "Let's visualize some surfaces that are closer to what you might see if you plotted the loss surface for a machine learning model you were optimizing. A list of loss functions is defined in the cells below, first in Python and then, for some, in mathematical notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1\n",
    "\n",
    "losses = [lambda x,y: np.square(x) + np.square(y),\n",
    "          lambda x,y: np.square(x) + 0.1 * np.square(y),\n",
    "          lambda x,y: 3 * np.square(x) + 0.1 * np.square(y),\n",
    "          lambda x,y: np.cos(3 *x) + 0.5 * (np.square(x) + np.square(y) + x),\n",
    "          lambda x,y: np.cos(3 * x) + np.square(x)+np.square(y),\n",
    "          lambda x,y: np.where(np.abs(x)+np.abs(y)<0.75,0,np.abs(x)+np.abs(y)-0.75),\n",
    "          lambda x,y: np.where(x>y+0.25,1.25,y)+np.square(x)+np.square(y),\n",
    "          lambda x,y: 0.1*np.random.standard_normal(size=x.shape),\n",
    "          lambda x,y: calc.surfaces.gauss_random_field(x, y, scale)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    l_0(x, y) &= x^2 + y^2\\\\\n",
    "    l_1(x, y) &= x^2 + 0.1 \\cdot y^2\\\\\n",
    "    l_2(x, y) &= 3x^2+ 0.1 \\cdot y^2\\\\\n",
    "    l_3(x, y) &= \\cos(3x) + 0.5 \\cdot (x^2 + y^2 + x)\\\\\n",
    "    l_4(x, y) &= \\cos(3x) + x^2 + y^2\\\\\n",
    "    l_5(x, y) &= \\left\\{\\begin{array}{rl}\n",
    "            \\|x\\|+ \\|y\\| - 0.75, & \\text{if } \\|x\\|+ \\|y\\| > 0.75\\\\\n",
    "            0, & \\text{otherwise }\n",
    "            \\end{array}\\right.\\\\\n",
    "    l_6(x, y) &= x^2 +y^2 + \\left\\{\\begin{array}{rl}\n",
    "            1.25, & \\text{if } x > y + 0.25\\\\\n",
    "            y, & \\text{otherwise }\n",
    "            \\end{array}\\right.\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell produces a 3-D plot of a single loss surface, chosen by indexing into the `losses` list.\n",
    "\n",
    "The plots are interactive, to the extent that you can change the perspective. You can rotate with by clicking and dragging the left mouse button and zoom by doing the same with the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses[0]\n",
    "\n",
    "N = 50\n",
    "\n",
    "mesh_extent = 1.5\n",
    "\n",
    "calc.surfaces.plot_loss_surface(loss, N, mesh_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following questions will ask you to visualize these loss surfaces and answer questions about them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For certain problems, gradient descent performs nicely. View `losses[0]`.\n",
    "\n",
    "#### Q Pick a few different starting points and follow the direction of steepest descent. Where do you end up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What's nice about this loss surface?\n",
    "*This question might be easier to answer once you've seen some of the other loss surfaces.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we follow gradients numerically, using a computer, we have to pick a scale for the \"size\" of steps we take. This can cause problems we might not anticipate with a view of gradient descent based on physical intuition.\n",
    "\n",
    "View `losses[1]` and then `losses[2]`.\n",
    "\n",
    "#### Q Why might picking a size of step cause issues with the surface `losses[1]` or `losses[2]`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other problems can't be solved by gradient descent effectively. Select `losses[3]`.\n",
    "\n",
    "#### Q Again, select multiple different starting points and follow the direction of steepest descent. What's different in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Why might this change in the behavior of gradient descent be a bad thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several similar cases to the above issue that are of interest. View `losses[4]` and then `losses[5]`.\n",
    "\n",
    "#### Q Compare and contrast loss surfaces `3`, `4`, and `5`. Which ones cause issues for optimization? Explain your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some issues are more theoretical than practical. View `losses[6]`.\n",
    "\n",
    "#### Q Can we still do gradient descent on this loss surface? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some loss functions, the right method of minimization can be hard to decide. View `losses[7]`.\n",
    "\n",
    "On this function, the value at each point is random and independent of the value at all other points.\n",
    "\n",
    "#### Q What strategies might you use to minimize this loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss surfaces for things like \"a neural network that maps [pictures of horses to almost identical pictures of zebras](https://github.com/junyanz/CycleGAN)\" are expected to be much more complicated than the ones we've looked at so far. The next class of loss functions is a simplified model of a neural network loss.\n",
    "\n",
    "View `losses[8]` with the parameters `N = 100`, `mesh_extent = 10`, and `scale = 1` in the plotting and loss definition cells (original values are `50`, `1.5`, and `1`, for reference).\n",
    "\n",
    "You can also attempt to increase these values to `N = 150`, `mesh_extent = 25`, and `scale = 2`. With these settings, the plot will take a bit of time to render on most machines and the plot will lag when interacted with. On some machines, this may consume too much memory and cause the plot to not render. If the plot successfully renders and you'd like to see more, increase the parameters to `N = 250`, `mesh_extent = 50`, and `scale = 3`.\n",
    "\n",
    "#### Q Do these loss surfaces look like promising candidates for gradient descent? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DA3TltsPFDW1"
   },
   "source": [
    "# Appendix: Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first set of exercises above, like much of contemporary machine learning,\n",
    "relied on automatic differentiation.\n",
    "\n",
    "The notes below briefly discuss computational graphs,\n",
    "the core abstraction behind automatic differentiation,\n",
    "and show how these graphs can be directly visualized in\n",
    "[`pytorch`](https://torch.org),\n",
    "a popular library for automatic differentiation focused on deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Visualizing Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eq1-UUb8Ee9u"
   },
   "source": [
    "### Simple Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider a very simple expression,\n",
    "courtesy of an example from Chris Olah's\n",
    "[blogpost on automatic differentiation](https://colah.github.io/posts/2015-08-Backprop/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_WHd0Mj-sMO"
   },
   "source": [
    "$$\n",
    "e = (a + b) * (b + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two \"input variables\" here: $a$ and $b$.\n",
    "\n",
    "To get `torch` to track these values as variables,\n",
    "not constants, we need to tell it\n",
    "that we will need their gradient later: `requires_grad` is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "s-p85ZcI9lN-",
    "outputId": "54b3cab7-73e2-4f9b-e2d9-1682025a167e"
   },
   "outputs": [],
   "source": [
    "a = torch.ones(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done that, we can write out the expression and then use\n",
    "`torchviz` to view the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "s-p85ZcI9lN-",
    "outputId": "54b3cab7-73e2-4f9b-e2d9-1682025a167e"
   },
   "outputs": [],
   "source": [
    "e = (a + b) * (b + 1)\n",
    "\n",
    "make_graph(e, {\"a\": a, \"b\": b})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every node of the graph is part of the computation of $e$:\n",
    "either a variable defined above or the result of applying an operation,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We8lUjYB88Fm"
   },
   "source": [
    "The contents and structure of this graph become a bit clearer\n",
    "if we consider how the value of $e$ is computed.\n",
    "\n",
    "We might do this mathematically by defining some\n",
    "intermediate variables.\n",
    "\n",
    "$$\n",
    "c = a + b \\\\\n",
    "d = b + 1 \\\\\n",
    "e = c * d\n",
    "$$\n",
    "\n",
    "The cell below defines these variables explicitly.\n",
    "It also uses some `torch` functions,\n",
    "like `torch.add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "id": "A61btQyh8KD0",
    "outputId": "d8363671-e4d0-4a34-d2df-bbca84c76c9b"
   },
   "outputs": [],
   "source": [
    "a = torch.ones(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "c = torch.add(a, b)\n",
    "d = torch.add(b, 1)\n",
    "\n",
    "e = torch.mul(c, d)\n",
    "\n",
    "make_graph(e, {\"a\": a, \"b\": b})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the graph change? Why should it, or why should it not?\n",
    "\n",
    "For both examples, try to map the symbols in the Python code onto the graph.\n",
    "\n",
    "Do any of the symbols in the Python code fail to show up in the graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's more than one way to calculate the same thing.\n",
    "\n",
    "Even if the result of the computation is the same,\n",
    "the graph can look quite different.\n",
    "\n",
    "That's because the graph captures the _procedure of computation_,\n",
    "not the result.\n",
    "\n",
    "For example, $e$ can also be written\n",
    "$$\n",
    "e = (a + b) * (b + 1)  = a * b + b * b + a + b\n",
    "$$\n",
    "\n",
    "The cell below defines $e$ this way,\n",
    "then produces a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "Pc2P_7UXSaSQ",
    "outputId": "59779ed5-da75-4add-e93e-774864e27d57"
   },
   "outputs": [],
   "source": [
    "a = torch.ones(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "e = a * b + b * b + a + b\n",
    "\n",
    "make_graph(e, {\"a\": a, \"b\": b})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the graph look the same or different? Are there more or fewer nodes? What does this imply about which of the ways of calculating $e$ above is faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, finding ways to shrink the size of computational graphs through refactoring is a core component of fast algorithms from machine learning\n",
    "(e.g. [belief propagation](https://en.wikipedia.org/wiki/Belief_propagation)\n",
    "and the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm),\n",
    "which underlie [error-correcting codes](https://en.wikipedia.org/wiki/Turbo_code#Bayesian_formulation))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIlHsrIqDJ68"
   },
   "source": [
    "### Examples from Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section shows the computational graphs for a few machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIlHsrIqDJ68"
   },
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below implements linear regression on two variables predicting a third\n",
    "(see shapes of `input_data` and `output_data`)\n",
    "and produces the graph.\n",
    "\n",
    "The `Addmm` operation combines `Add`ition and `m`atrix `m`ultiplication into a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "baoLWCoB8k9K",
    "outputId": "d0bd18fb-6d12-449e-ca97-5bca7db20984"
   },
   "outputs": [],
   "source": [
    "input_data = torch.randn(size=(100, 2))\n",
    "output_data = torch.randn(size=(100, 1))\n",
    "\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "weight = torch.randn((2, 1), requires_grad=True)\n",
    "\n",
    "mean_squared_error = ((output_data - torch.addmm(bias, input_data, weight)) ** 2).mean()\n",
    "\n",
    "make_graph(mean_squared_error, {\"bias\": bias, \"weight\": weight})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiOsus0gDD-C"
   },
   "source": [
    "#### A Barebones Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines a classic neural network,\n",
    "along with some dummy inputs `x` and targets `y`.\n",
    "\n",
    "You'll notice that the \"head\" of the graph,\n",
    "from `AddmmBackward` to `MeanBackward0`\n",
    "is identical to that from the linear regression example.\n",
    "\n",
    "Also, you'll notice that the graph has two repeated parts:\n",
    "one for the module `W0` and another for the module `W1`.\n",
    "\n",
    "These are core properties of neural nets:\n",
    "they are closely related to linear models\n",
    "and they are constructed of modular pieces.\n",
    "It is an important feature of computational graphs that these features are easy to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "colab_type": "code",
    "id": "uh0vHM5_As_y",
    "outputId": "cc6fda59-27da-4bf8-a2ac-c51120690d78"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module(\"W0\", nn.Linear(8, 16))\n",
    "model.add_module(\"tanh\", nn.Tanh())\n",
    "model.add_module(\"W1\", nn.Linear(16, 1))\n",
    "\n",
    "x = torch.randn(1, 8)\n",
    "y = torch.randn(1, 16)\n",
    "model_output = model(x)\n",
    "loss = ((model_output - y) ** 2).mean()\n",
    "\n",
    "make_graph(loss, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rM6oj_REPj8"
   },
   "source": [
    "#### ResNet50: An \"Industrial\" Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell sets up a much beefier, famous network:\n",
    "[ResNet50](https://arxiv.org/abs/1512.03385),\n",
    "suitable for difficult computer vision tasks in high definition.\n",
    "\n",
    "This network is extremely large (~23 million parameters),\n",
    "and so you may find that the visualization below breaks down\n",
    "(cut off at corners, text too small, etc.).\n",
    "\n",
    "This example is included to give a sense of what an \"industrial\"\n",
    "computational graph looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b804201f62ec4876a58f98e4c26c1b08",
      "2937c170c0ae4a3b9c4f039b77cf7f8b",
      "c40d592715574407949c8052783da7e2",
      "6a7b5465702a449b8f8455a0476a0564",
      "17cd1bdb77804165a392a5c2e8f1cd93",
      "93fb187a0f2042a9aaaf91670a8223fd",
      "ae9e91cf8ba846189d15fe22d717f89d",
      "79def2c48fd14d8c99841e11167c1a5e"
     ]
    },
    "colab_type": "code",
    "id": "pWh3yU9zD7c1",
    "outputId": "f2cbb268-067b-4013-8e88-d79237d77f95"
   },
   "outputs": [],
   "source": [
    "resnet = models.resnet50(pretrained=False)\n",
    "\n",
    "x = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False)\n",
    "out = resnet(x)\n",
    "\n",
    "make_graph(out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE8BnoOsTqFw"
   },
   "source": [
    "## Automating Calculus with Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJ37tDoDTvOB"
   },
   "source": [
    "The primary use of computational graphs in machine learning is to automate the calculation of gradients.\n",
    "\n",
    "If each operation has a defined gradient,\n",
    "we can calculate the gradient of a node $B$ with respect to a node $A$\n",
    "by two sumple rules:\n",
    "\n",
    "- Add the gradient computed by following each path from $A$ to $B$\n",
    "- Along each path from $A$ to $B$, multiply the gradients together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this to work, each function must\n",
    "1. be tracked and added to the graph\n",
    "2. have a matched \"opposite\" function that \n",
    "3. computes the gradient of its outputs with respect to its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `torch`, these properties are guaranteed by\n",
    "1. defining an object that sub-classes `torch.autograd.Function` and\n",
    "2. implements a `backward` method that\n",
    "3. is equal to the gradient of the `forward` method that applies the function.\n",
    "\n",
    "The lattermost is the responsibility of the programmer to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows how this is done with a very simple function that just returns its `inputs`.\n",
    "\n",
    "So that you can see when the methods are called, they also `print` a friendly message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvuQ_LGpXls6"
   },
   "outputs": [],
   "source": [
    "class DummyFunction(torch.autograd.Function):\n",
    "\n",
    "    # method for the original computation\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        print(\"hello from forward!\")\n",
    "        return input\n",
    "\n",
    "    # method for the gradient computation\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        print(\"hello from backward!\")\n",
    "        return grad_output\n",
    "\n",
    "dummy = DummyFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "tRXvN9QlYTmi",
    "outputId": "812134b6-f047-459f-da7e-16becb62301e"
   },
   "outputs": [],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "\n",
    "a = dummy(x)\n",
    "\n",
    "make_graph(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "PAYg0Ff2YdnB",
    "outputId": "3d7c6d6e-91d4-4907-f1d1-1f6a119a70cb"
   },
   "outputs": [],
   "source": [
    "torch.autograd.grad(a, x)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Colab - Math4ML II: Calculus.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06a2e02685114cf58bc215760c34dca3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FloatSliderView",
      "continuous_update": true,
      "description": "m",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_2956a04ae7a24cbf91234dc7544377e9",
      "max": 10,
      "min": -10,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": ".2f",
      "step": 0.1,
      "style": "IPY_MODEL_3803e4c702cf4e8fb6d23bf4bfc5e41a",
      "value": 0
     }
    },
    "12097194f936412fb14269ebd1f3cb69": {
     "model_module": "@jupyter-widgets/output",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_98542294f506402fb75610546fa35377",
      "msg_id": "",
      "outputs": []
     }
    },
    "17cd1bdb77804165a392a5c2e8f1cd93": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2937c170c0ae4a3b9c4f039b77cf7f8b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2956a04ae7a24cbf91234dc7544377e9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30a10f3d33024c319e3b32c10016fca7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3803e4c702cf4e8fb6d23bf4bfc5e41a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "552caebd65934cb5ade43e45627a1bfd": {
     "model_module": "jupyter-matplotlib",
     "model_name": "MPLCanvasModel",
     "state": {
      "_cursor": "pointer",
      "_dom_classes": [],
      "_figure_label": "Figure",
      "_height": 0,
      "_image_mode": "full",
      "_message": "",
      "_model_module": "jupyter-matplotlib",
      "_model_module_version": "^0.7.2",
      "_model_name": "MPLCanvasModel",
      "_rubberband_height": 0,
      "_rubberband_width": 0,
      "_rubberband_x": 0,
      "_rubberband_y": 0,
      "_view_count": null,
      "_view_module": "jupyter-matplotlib",
      "_view_module_version": "^0.7.2",
      "_view_name": "MPLCanvasView",
      "_width": 0,
      "footer_visible": true,
      "header_visible": true,
      "layout": "IPY_MODEL_993e3e5493e74e869a26170b4a68f872",
      "resizable": true,
      "toolbar": "IPY_MODEL_4cfbcc54edb2418fa50890c1a85b75ea",
      "toolbar_position": "left",
      "toolbar_visible": true
     }
    },
    "6a7b5465702a449b8f8455a0476a0564": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79def2c48fd14d8c99841e11167c1a5e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ae9e91cf8ba846189d15fe22d717f89d",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 200MB/s]"
     }
    },
    "79def2c48fd14d8c99841e11167c1a5e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822ab2411b874e42a64786a77584921e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [
       "widget-interact"
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_06a2e02685114cf58bc215760c34dca3",
       "IPY_MODEL_12097194f936412fb14269ebd1f3cb69"
      ],
      "layout": "IPY_MODEL_30a10f3d33024c319e3b32c10016fca7"
     }
    },
    "93fb187a0f2042a9aaaf91670a8223fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98542294f506402fb75610546fa35377": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae9e91cf8ba846189d15fe22d717f89d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b804201f62ec4876a58f98e4c26c1b08": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c40d592715574407949c8052783da7e2",
       "IPY_MODEL_6a7b5465702a449b8f8455a0476a0564"
      ],
      "layout": "IPY_MODEL_2937c170c0ae4a3b9c4f039b77cf7f8b"
     }
    },
    "b83058f6b6f445a3a7d2203358735ae4": {
     "model_module": "jupyter-matplotlib",
     "model_name": "MPLCanvasModel",
     "state": {
      "_cursor": "pointer",
      "_dom_classes": [],
      "_figure_label": "Figure",
      "_height": 0,
      "_image_mode": "full",
      "_message": "",
      "_model_module": "jupyter-matplotlib",
      "_model_module_version": "^0.7.2",
      "_model_name": "MPLCanvasModel",
      "_rubberband_height": 0,
      "_rubberband_width": 0,
      "_rubberband_x": 0,
      "_rubberband_y": 0,
      "_view_count": null,
      "_view_module": "jupyter-matplotlib",
      "_view_module_version": "^0.7.2",
      "_view_name": "MPLCanvasView",
      "_width": 0,
      "footer_visible": true,
      "header_visible": true,
      "layout": "IPY_MODEL_e574816d9654486abae7ebddd58d6a14",
      "resizable": true,
      "toolbar": "IPY_MODEL_abc39ada8ee94a9a92aecb24a263342b",
      "toolbar_position": "left",
      "toolbar_visible": true
     }
    },
    "c40d592715574407949c8052783da7e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93fb187a0f2042a9aaaf91670a8223fd",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17cd1bdb77804165a392a5c2e8f1cd93",
      "value": 102502400
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
