{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13953901-66c4-437c-b3a5-fadf8136d54c",
   "metadata": {},
   "source": [
    "# Visualize Bain Tumor Segmentation Data\n",
    "\n",
    "In this notebook we will learn:\n",
    "- MONAI transform API:\n",
    "  - MONAI Transforms for dictionary format data.\n",
    "  - Creating custom transforms using [`monai.transforms`](https://docs.monai.io/en/stable/transforms.html) API.\n",
    "- how we can visualize the brain tumor segmentation dataset using W&B image overlays.\n",
    "- how we can analyze our data using W&B Tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4023f9a-1e58-468d-8ea3-56a694fa89ec",
   "metadata": {},
   "source": [
    "## ðŸŒ´ Setup and Installation\n",
    "\n",
    "First, let us install the latest version of both MONAI and Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def9c4c-89b9-4f02-9853-91624690dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U monai wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48509346-08a2-41e3-bc98-1aea79fe42d3",
   "metadata": {},
   "source": [
    "## ðŸŒ³ Initialize a W&B Run\n",
    "\n",
    "We will start a new W&B run to start tracking our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d72b50-1a24-4a32-97c8-6f859cc203df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"brain-tumor-segmentation\",\n",
    "    entity=\"lifesciences\",\n",
    "    job_type=\"visualize_dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245625a8-be24-40f8-8cef-615917611a49",
   "metadata": {},
   "source": [
    "## ðŸ’¿ Loading and Transforming the Data\n",
    "\n",
    "We will now learn using the [`monai.transforms`](https://docs.monai.io/en/stable/transforms.html) API to create and apply transforms to our data.\n",
    "\n",
    "### Creating a Custom Transform\n",
    "\n",
    "First, we demonstrate the creation of a custom transform `ConvertToMultiChannelBasedOnBratsClassesd` using [`monai.transforms.MapTransform`](https://docs.monai.io/en/stable/transforms.html#maptransform) that converts labels to multi-channel tensors based on brats18 classes:\n",
    "- label 1 is the necrotic and non-enhancing tumor core\n",
    "- label 2 is the peritumoral edema\n",
    "- label 3 is the GD-enhancing tumor.\n",
    "\n",
    "The target classes for the semantic segmentation task after applying this transform on the dataset will be\n",
    "- Tumor core\n",
    "- Whole tumor\n",
    "- Enhancing tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8d513-cd46-43c1-839e-0ae15f750a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai.transforms import MapTransform\n",
    "\n",
    "\n",
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi-channels based on brats classes:\n",
    "    label 1 is the peritumoral edema\n",
    "    label 2 is the GD-enhancing tumor\n",
    "    label 3 is the necrotic and non-enhancing tumor core\n",
    "    The possible classes are TC (Tumor core), WT (Whole tumor), and ET (Enhancing tumor).\n",
    "\n",
    "    Reference: https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        data_dict = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 2 and label 3 to construct Tumor Core\n",
    "            result.append(torch.logical_or(data_dict[key] == 2, data_dict[key] == 3))\n",
    "            # merge labels 1, 2 and 3 to construct Whole Tumor\n",
    "            result.append(\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(data_dict[key] == 2, data_dict[key] == 3), data_dict[key] == 1\n",
    "                )\n",
    "            )\n",
    "            # label 2 is Enhancing Tumor\n",
    "            result.append(data_dict[key] == 2)\n",
    "            data_dict[key] = torch.stack(result, axis=0).float()\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a65c1-701c-4700-88a9-df30b51eb10e",
   "metadata": {},
   "source": [
    "Next, we compose all the necessary transforms for visualizing the data using [`monai.transforms.Compose`](https://docs.monai.io/en/stable/transforms.html#monai.transforms.Compose).\n",
    "\n",
    "**Note:** During training, we will apply a differnt set of transforms to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542d505-3713-43f1-adf1-c33ade5696b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "\n",
    "\n",
    "transforms = Compose(\n",
    "    [\n",
    "        # Load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # Ensure loaded images are in channels-first format\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        # Ensure the input data to be a PyTorch Tensor or numpy array\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        # Convert labels to multi-channels based on brats18 classes\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        # Change the input imageâ€™s orientation into the specified based on axis codes\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        # Resample the input images to the specified pixel dimension\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        # Normalize input image intensity\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925b068-31fd-4a66-873d-850e6cd7fd87",
   "metadata": {},
   "source": [
    "For loading the dataset, we first fetch it from the W&B dataset artifact that we had created earlier. This enables us to use the dataset as an input artifact to our visualization run, and establish the necessary lineage for our experiment.\n",
    "\n",
    "![](./assets/artifact_usage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bf7fc-dc39-4247-9001-cd8832045b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = wandb.use_artifact(\n",
    "    \"lifesciences/brain-tumor-segmentation/decathlon_brain_tumor:v0\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54920325-570f-4c19-a299-66d4ff21ca15",
   "metadata": {},
   "source": [
    "We now use the [`monai.apps.DecathlonDataset`](https://docs.monai.io/en/stable/apps.html#monai.apps.DecathlonDataset) to load our dataset and apply the transforms we defined on the data samples so that we can visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc829ac0-ee0a-4924-889d-72fba089ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.apps import DecathlonDataset\n",
    "\n",
    "\n",
    "# Create the dataset for the training split\n",
    "# of the brain tumor segmentation dataset\n",
    "train_dataset = DecathlonDataset(\n",
    "    root_dir=artifact_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=transforms,\n",
    "    section=\"training\",\n",
    "    download=True,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "# Create the dataset for the validation split\n",
    "# of the brain tumor segmentation dataset\n",
    "val_dataset = DecathlonDataset(\n",
    "    root_dir=artifact_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=transforms,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd917fe9-2fcd-482c-9e00-243d81e26dc7",
   "metadata": {},
   "source": [
    "## ðŸ“¸ Visualizing the Dataset\n",
    "\n",
    "Weights & Biases supports images, video, audio, and more. You can log rich media to explore your results and visually compare our runs, models, and datasets. Now, you will learn using the [segmentation mask overlay](https://docs.wandb.ai/guides/track/log/media#image-overlays-in-tables) system to visualize our data volumes. To log segmentation masks in [W&B tables](https://docs.wandb.ai/guides/tables), you must provide a [`wandb.Image`](https://docs.wandb.ai/ref/python/data-types/image) object containing the segmentation annotations for each row in the table.\n",
    "\n",
    "![](https://docs.wandb.ai/assets/images/viz-2-e3652d015abbf1d6d894e8edb1424eac.gif)\n",
    "\n",
    "An example is provided in the pseudocode below:\n",
    "\n",
    "```python\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "\n",
    "for id, img, label in zip(ids, images, labels):\n",
    "    mask_img = wandb.Image(\n",
    "        img,\n",
    "        masks={\n",
    "            \"ground-truth\": {\"mask_data\": label, \"class_labels\": class_labels}\n",
    "            # ...\n",
    "        },\n",
    "    )\n",
    "\n",
    "    table.add_data(id, img)\n",
    "\n",
    "wandb.log({\"Table\": table})\n",
    "```\n",
    "\n",
    "However, in our case, since the volume of the target classes might overlap one another, we will log them as separate overlays on the same image, so that we do not miss the relevant information.\n",
    "\n",
    "An example is provided in the pseudocode below:\n",
    "\n",
    "```python\n",
    "mask_img = wandb.Image(\n",
    "    img,\n",
    "    masks={\n",
    "        \"ground-truth/Tumor-Core\": {\n",
    "            \"mask_data\": label_tumor_core,\n",
    "            \"class_labels\": {0: \"background\", 1: \"Tumor Core\"}\n",
    "        },\n",
    "        \"ground-truth/Whole-Tumor\": {\n",
    "            \"mask_data\": label_tumor_core,\n",
    "            \"class_labels\": {0: \"background\", 2: \"Whole-Tumor\"}\n",
    "        },\n",
    "        \"ground-truth/Enhancing-Tumor\": {\n",
    "            \"mask_data\": label_tumor_core,\n",
    "            \"class_labels\": {0: \"background\", 3: \"Enhancing-Tumor\"}\n",
    "        },\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9acc72e-7091-40d7-a97b-023df48d9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def get_target_area_percentage(segmentation_map):\n",
    "    segmentation_map_list = segmentation_map.flatten().tolist()\n",
    "    return segmentation_map_list.count(1.0) * 100 / len(segmentation_map_list)\n",
    "\n",
    "\n",
    "def log_data_samples_into_tables(\n",
    "    sample_image: np.array,\n",
    "    sample_label: np.array,\n",
    "    split: str = None,\n",
    "    data_idx: int = None,\n",
    "    table: wandb.Table = None,\n",
    "):\n",
    "    \"\"\"Utility function for logging a data sample into a W&B Table\"\"\"\n",
    "    num_channels, _, _, num_slices = sample_image.shape\n",
    "    with tqdm(total=num_slices, leave=False) as progress_bar:\n",
    "        for slice_idx in range(num_slices):\n",
    "            ground_truth_wandb_images, tumor_area_percentages = [], []\n",
    "            for channel_idx in range(num_channels):\n",
    "                masks = {\n",
    "                    \"ground-truth/Tumor-Core\": {\n",
    "                        \"mask_data\": sample_label[0, :, :, slice_idx],\n",
    "                        \"class_labels\": {0: \"background\", 1: \"Tumor Core\"},\n",
    "                    },\n",
    "                    \"ground-truth/Whole-Tumor\": {\n",
    "                        \"mask_data\": sample_label[1, :, :, slice_idx] * 2,\n",
    "                        \"class_labels\": {0: \"background\", 2: \"Whole Tumor\"},\n",
    "                    },\n",
    "                    \"ground-truth/Enhancing-Tumor\": {\n",
    "                        \"mask_data\": sample_label[2, :, :, slice_idx] * 3,\n",
    "                        \"class_labels\": {0: \"background\", 3: \"Enhancing Tumor\"},\n",
    "                    },\n",
    "                }\n",
    "\n",
    "                ground_truth_wandb_images.append(\n",
    "                    wandb.Image(\n",
    "                        sample_image[channel_idx, :, :, slice_idx],\n",
    "                        masks=masks,\n",
    "                    )\n",
    "                )\n",
    "                tumor_area_percentages.append(\n",
    "                    {\n",
    "                        \"Tumor-Core-Area-Percentage\": get_target_area_percentage(\n",
    "                            sample_label[0, :, :, slice_idx]\n",
    "                        ),\n",
    "                        \"Whole-Tumor-Area-Percentage\": get_target_area_percentage(\n",
    "                            sample_label[1, :, :, slice_idx]\n",
    "                        ),\n",
    "                        \"Enhancing-Tumor-Area-Percentage\": get_target_area_percentage(\n",
    "                            sample_label[2, :, :, slice_idx]\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "            table.add_data(\n",
    "                split,\n",
    "                data_idx,\n",
    "                slice_idx,\n",
    "                *tumor_area_percentages,\n",
    "                *ground_truth_wandb_images\n",
    "            )\n",
    "            progress_bar.update(1)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919a2fc-7e44-4283-a862-93ff8cdcfa5f",
   "metadata": {},
   "source": [
    "Next, we iterate over our respective datasets and populate the table on our W&B dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85601b14-e693-4cca-b8cf-5e3ce863bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema of the table\n",
    "table = wandb.Table(\n",
    "    columns=[\n",
    "        \"Split\",\n",
    "        \"Data Index\",\n",
    "        \"Slice Index\",\n",
    "        \"Tumor-Area-Pixel-Percentages-Channel-0\",\n",
    "        \"Tumor-Area-Pixel-Percentages-Channel-1\",\n",
    "        \"Tumor-Area-Pixel-Percentages-Channel-2\",\n",
    "        \"Tumor-Area-Pixel-Percentages-Channel-3\",\n",
    "        \"Image-Channel-0\",\n",
    "        \"Image-Channel-1\",\n",
    "        \"Image-Channel-2\",\n",
    "        \"Image-Channel-3\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b6d4b-ebf6-4d9d-bc78-7ce489e0a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations for train_dataset\n",
    "for data_idx, sample in tqdm(\n",
    "    enumerate(train_dataset),\n",
    "    total=len(train_dataset),\n",
    "    desc=\"Generating Train Dataset Visualizations:\",\n",
    "):\n",
    "    sample_image = sample[\"image\"].detach().cpu().numpy()\n",
    "    sample_label = sample[\"label\"].detach().cpu().numpy()\n",
    "    table = log_data_samples_into_tables(\n",
    "        sample_image,\n",
    "        sample_label,\n",
    "        split=\"train\",\n",
    "        data_idx=data_idx,\n",
    "        table=table,\n",
    "    )\n",
    "\n",
    "# Generate visualizations for val_dataset\n",
    "for data_idx, sample in tqdm(\n",
    "    enumerate(val_dataset),\n",
    "    total=len(val_dataset),\n",
    "    desc=\"Generating Validation Dataset Visualizations:\",\n",
    "):\n",
    "    sample_image = sample[\"image\"].detach().cpu().numpy()\n",
    "    sample_label = sample[\"label\"].detach().cpu().numpy()\n",
    "    table = log_data_samples_into_tables(\n",
    "        sample_image,\n",
    "        sample_label,\n",
    "        split=\"val\",\n",
    "        data_idx=data_idx,\n",
    "        table=table,\n",
    "    )\n",
    "\n",
    "# Log the table to your dashboard\n",
    "wandb.log({\"tumor_segmentation_data\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83d9f8-b50f-48b8-a6c3-e8920ac44285",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
