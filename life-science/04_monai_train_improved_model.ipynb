{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93e6b75-b1d9-4703-8834-ab3fcd8f934b",
   "metadata": {},
   "source": [
    "# Train a Baseline Segmentation Model\n",
    "In this notebook we will learn:\n",
    "\n",
    "- We will learn how to use specific MONAI APIs to write our training workflow, including a SoTA neural network architecture and loss function and metrics for our task.\n",
    "- Use Weights & Biases for tracking our experiments and logging and verisioning our model checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95118e9e-e6d0-4bde-bd5b-af792ca8153a",
   "metadata": {},
   "source": [
    "## ðŸŒ´ Setup and Installation\n",
    "\n",
    "First, let us install the latest version of both MONAI and Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab1bc3-d503-4e4c-afd1-ad77d72ac472",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U monai wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b7942",
   "metadata": {},
   "source": [
    "## ðŸ¦„ Getting the Best Configs\n",
    "\n",
    "For training a model that is an improvement over the baseline model we would need to get the configs of the best performing run from the sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "def get_best_config_from_sweep(entity: str, project: str, sweep_id: str, metric: str):\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n",
    "    runs = sorted(\n",
    "        sweep.runs, key=lambda run: run.summary.get(metric, 0), reverse=True\n",
    "    )\n",
    "    best_run = runs[0]\n",
    "    return best_run.config\n",
    "\n",
    "\n",
    "config = get_best_config_from_sweep(\n",
    "    entity=\"lifesciences\",\n",
    "    project=\"brain-tumor-segmentation\",\n",
    "    sweep_id=\"580gsolt\",\n",
    "    metric=\"validation/mean_dice\",\n",
    ")\n",
    "config[\"initial_learning_rate\"] = 1e-4\n",
    "config[\"max_train_epochs\"] = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f71b2-ffb6-475c-a563-1cb631e33d84",
   "metadata": {},
   "source": [
    "Next, we will start a new W&B run to start tracking our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b258cfa2-3795-4502-8878-aff469ba5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import set_determinism\n",
    "\n",
    "wandb.init(\n",
    "    project=\"brain-tumor-segmentation\",\n",
    "    entity=\"lifesciences\",\n",
    "    job_type=\"train_improved\",\n",
    "    config=config,\n",
    ")\n",
    "config = wandb.config\n",
    "\n",
    "set_determinism(seed=config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2da2f5-b1a8-4215-927c-dc5884b0b41f",
   "metadata": {},
   "source": [
    "## ðŸ’¿ Loading and Transforming the Data\n",
    "\n",
    "We will now learn using the [`monai.transforms`](https://docs.monai.io/en/stable/transforms.html) API to create and apply transforms to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38e5d1-e3de-4bda-9b89-67193a6baca2",
   "metadata": {},
   "source": [
    "Next, we compose all the necessary transforms for visualizing the data using [`monai.transforms.Compose`](https://docs.monai.io/en/stable/transforms.html#monai.transforms.Compose).\n",
    "\n",
    "**Note:** During training, we will apply a differnt set of transforms to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9e73a-aeb1-49b2-b06b-d09857b74966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "from utils import ConvertToMultiChannelBasedOnBratsClassesd\n",
    "\n",
    "\n",
    "config.roi_size = [224, 224, 144]\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # Ensure loaded images are in channels-first format\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        # Ensure the input data to be a PyTorch Tensor or numpy array\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        # Convert labels to multi-channels based on brats18 classes\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        # Change the input imageâ€™s orientation into the specified based on axis codes\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        # Resample the input images to the specified pixel dimension\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        # Augmentation: Crop image with random size or specific size ROI\n",
    "        RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"], roi_size=config.roi_size, random_size=False\n",
    "        ),\n",
    "        \n",
    "        # Augmentation: Randomly flip the image on the specified axes\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        \n",
    "        # Normalize input image intensity\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        \n",
    "        # Augmentation: Randomly scale the image intensity\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # Ensure loaded images are in channels-first format\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        # Ensure the input data to be a PyTorch Tensor or numpy array\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        # Convert labels to multi-channels based on brats18 classes\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        # Change the input imageâ€™s orientation into the specified based on axis codes\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        # Resample the input images to the specified pixel dimension\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        # Normalize input image intensity\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df1019-7cf6-4baa-baa1-fa2085da17bb",
   "metadata": {},
   "source": [
    "For loading the dataset, we first fetch it from the W&B dataset artifact that we had created earlier. This enables us to use the dataset as an input artifact to our visualization run, and establish the necessary lineage for our experiment.\n",
    "\n",
    "![](./assets/artifact_usage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2d2c9-8e7f-479b-a822-deb0984d22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = wandb.use_artifact(\n",
    "    \"lifesciences/brain-tumor-segmentation/decathlon_brain_tumor:v0\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9de15-afa4-4ccf-9d4f-78b90ae3db8f",
   "metadata": {},
   "source": [
    "We now use the [`monai.apps.DecathlonDataset`](https://docs.monai.io/en/stable/apps.html#monai.apps.DecathlonDataset) to load our dataset and apply the transforms we defined on the data samples so that we use them for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16faedc0-f50a-4289-9fa4-6948549ea74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.apps import DecathlonDataset\n",
    "\n",
    "\n",
    "# Create the dataset for the training split\n",
    "# of the brain tumor segmentation dataset\n",
    "train_dataset = DecathlonDataset(\n",
    "    root_dir=artifact_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=train_transform,\n",
    "    section=\"training\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=config.num_workers,\n",
    ")\n",
    "\n",
    "# Create the dataset for the validation split\n",
    "# of the brain tumor segmentation dataset\n",
    "val_dataset = DecathlonDataset(\n",
    "    root_dir=artifact_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eb84cf-eea0-431b-a66e-9afd2b7144ba",
   "metadata": {},
   "source": [
    "We now create DataLoaders for the train and validation datasets respectively using [`monai.data.DataLoader`](https://docs.monai.io/en/stable/data.html#dataloader) which provides an iterable over the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad33748-060f-4652-87ed-f8b56de02824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import DataLoader\n",
    "\n",
    "\n",
    "# create the train_loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    ")\n",
    "\n",
    "# create the val_loader\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68b8b8-9527-482d-8f4f-da8c71dcbd87",
   "metadata": {},
   "source": [
    "## ðŸ¤– Creating the Model, Loss, and Optimizer\n",
    "\n",
    "We will be training a **SegResNet** model based on the paper [3D MRI brain tumor segmentation using auto-encoder regularization](https://arxiv.org/pdf/1810.11654.pdf). The [SegResNet](https://docs.monai.io/en/stable/networks.html#segresnet) model that comes implemented as a PyTorch Module as part of the [`monai.networks.nets`](https://docs.monai.io/en/stable/networks.html#nets) API that provides out-of-the-box implementations of SoTA neural network models for different medical imaging tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a5e5a-84ee-4fbe-9b06-1fcf9639d443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai.networks.nets import SegResNet\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# create model\n",
    "model = SegResNet(\n",
    "    blocks_down=config.model_blocks_down,\n",
    "    blocks_up=config.model_blocks_up,\n",
    "    init_filters=config.model_init_filters,\n",
    "    in_channels=config.model_in_channels,\n",
    "    out_channels=config.model_out_channels,\n",
    "    dropout_prob=config.model_dropout_prob,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20863f94-f7d0-4b82-8a55-41a1952d1cae",
   "metadata": {},
   "source": [
    "We will be using [Adam Optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) and the [cosine annealing schedule](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html) to schedule our learning rate. This approach is designed to help in finding global minima in the optimization landscape and to provide a form of reset mechanism during training, which can improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9b1a6-d127-4fe9-8831-a33b71dbe438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    config.initial_learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n",
    "\n",
    "# create learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config.max_train_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541dc8c6-009e-4115-81c5-d35386a92b6d",
   "metadata": {},
   "source": [
    "Next, we would define the loss as multi-label DiceLoss as proposed by the paper [V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation](https://arxiv.org/abs/1606.04797) using the [`monai.losses`](https://docs.monai.io/en/stable/losses.html) API and the corresponding dice metrics using the [`monai.metrics`](https://docs.monai.io/en/stable/metrics.html) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2276a62-24a9-4352-8058-eb807e16819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceLoss\n",
    "\n",
    "loss_function = DiceLoss(\n",
    "    smooth_nr=config.dice_loss_smoothen_numerator,\n",
    "    smooth_dr=config.dice_loss_smoothen_denominator,\n",
    "    squared_pred=config.dice_loss_squared_prediction,\n",
    "    to_onehot_y=config.dice_loss_target_onehot,\n",
    "    sigmoid=config.dice_loss_apply_sigmoid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d9b11-3df9-4ae8-96c0-e7c16468878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55423f02-8686-4a56-9778-09070b7edcc5",
   "metadata": {},
   "source": [
    "## ðŸ¦¾ Training the Model\n",
    "\n",
    "Finally, we proceed to writing the training and validation loop for the brain tumor segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df12631-8e09-48d3-b0b0-655775ec22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from monai.data import decollate_batch\n",
    "from utils import inference\n",
    "\n",
    "\n",
    "# Define custom x-axes\n",
    "wandb.define_metric(\"epoch/epoch_step\")\n",
    "wandb.define_metric(\"epoch/*\", step_metric=\"epoch/epoch_step\")\n",
    "wandb.define_metric(\"batch/batch_step\")\n",
    "wandb.define_metric(\"batch/*\", step_metric=\"batch/batch_step\")\n",
    "wandb.define_metric(\"validation/validation_step\")\n",
    "wandb.define_metric(\"validation/*\", step_metric=\"validation/validation_step\")\n",
    "\n",
    "# use automatic mixed-precision to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "batch_step = 0\n",
    "validation_step = 0\n",
    "metric_values = []\n",
    "metric_values_tumor_core = []\n",
    "metric_values_whole_tumor = []\n",
    "metric_values_enhanced_tumor = []\n",
    "\n",
    "epoch_progress_bar = tqdm(range(config.max_train_epochs), desc=\"Training:\")\n",
    "\n",
    "for epoch in epoch_progress_bar:\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    total_batch_steps = len(train_dataset) // train_loader.batch_size\n",
    "    batch_progress_bar = tqdm(train_loader, total=total_batch_steps, leave=False)\n",
    "\n",
    "    # Training Step\n",
    "    for batch_data in batch_progress_bar:\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        batch_progress_bar.set_description(f\"train_loss: {loss.item():.4f}:\")\n",
    "        ## Log batch-wise training loss to W&B\n",
    "        wandb.log({\"batch/batch_step\": batch_step, \"batch/train_loss\": loss.item()})\n",
    "        batch_step += 1\n",
    "\n",
    "    epoch_loss /= total_batch_steps\n",
    "    ## Log batch-wise training loss and learning rate to W&B\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch/epoch_step\": epoch,\n",
    "            \"epoch/mean_train_loss\": epoch_loss,\n",
    "            \"epoch/learning_rate\": lr_scheduler.get_last_lr()[0],\n",
    "        }\n",
    "    )\n",
    "    lr_scheduler.step()\n",
    "    epoch_progress_bar.set_description(f\"Training: train_loss: {epoch_loss:.4f}:\")\n",
    "\n",
    "    # Validation and model checkpointing step\n",
    "    if (epoch + 1) % config.validation_intervals == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(model, val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric_values.append(dice_metric.aggregate().item())\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_values_tumor_core.append(metric_batch[0].item())\n",
    "            metric_values_whole_tumor.append(metric_batch[1].item())\n",
    "            metric_values_enhanced_tumor.append(metric_batch[2].item())\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            # Log and versison model checkpoints using W&B artifacts.\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, \"model.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            wandb.log_model(\n",
    "                checkpoint_path,\n",
    "                name=f\"{wandb.run.id}-checkpoint\",\n",
    "                aliases=[f\"epoch_{epoch}\"],\n",
    "            )\n",
    "\n",
    "            # Log validation metrics to W&B dashboard.\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"validation/validation_step\": validation_step,\n",
    "                    \"validation/mean_dice\": metric_values[-1],\n",
    "                    \"validation/mean_dice_tumor_core\": metric_values_tumor_core[-1],\n",
    "                    \"validation/mean_dice_whole_tumor\": metric_values_whole_tumor[-1],\n",
    "                    \"validation/mean_dice_enhanced_tumor\": metric_values_enhanced_tumor[-1],\n",
    "                }\n",
    "            )\n",
    "            validation_step += 1\n",
    "\n",
    "\n",
    "# Finish the experiment\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
