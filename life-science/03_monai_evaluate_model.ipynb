{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Bain Tumor Segmentation Data\n",
    "\n",
    "In this notebook we will learn:\n",
    "- how we can evaluate a pre-trained model checkpoint for brain tumor segmentation using MONAI and Weights & Biases.\n",
    "- how we can visually compare the ground-truth labels with the predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ´ Setup and Installation\n",
    "\n",
    "First, let us install the latest version of both MONAI and Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U monai wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŒ³ Initialize a W&B Run\n",
    "\n",
    "We will start a new W&B run to start tracking our experiment. Note that we set the job type for this run as `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "wandb.init(\n",
    "    project=\"brain-tumor-segmentation\",\n",
    "    entity=\"lifesciences\",\n",
    "    job_type=\"evaluate\"\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "# Ensure deterministic behavior and reproducibility\n",
    "config.seed = 0\n",
    "set_determinism(seed=config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¿ Loading and Transforming the Data\n",
    "\n",
    "We will use the validation transforms from the previous lessons to load and transform the validation dataset using the Decathlon dataset artifact on W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ConvertToMultiChannelBasedOnBratsClassesd\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "\n",
    "\n",
    "transforms = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # Ensure loaded images are in channels-first format\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        # Ensure the input data to be a PyTorch Tensor or numpy array\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        # Convert labels to multi-channels based on brats18 classes\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        # Change the input imageâ€™s orientation into the specified based on axis codes\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        # Resample the input images to the specified pixel dimension\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        # Normalize input image intensity\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Fetch the brain tumor segmentation dataset artifact from W&B\n",
    "artifact = wandb.use_artifact(\n",
    "    \"lifesciences/brain-tumor-segmentation/decathlon_brain_tumor:latest\",\n",
    "    type=\"dataset\",\n",
    ")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "\n",
    "# Create the dataset for the test split\n",
    "# of the brain tumor segmentation dataset\n",
    "val_dataset = DecathlonDataset(\n",
    "    root_dir=artifact_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=transforms,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Loading the Model Checkpoint\n",
    "\n",
    "We are going to fetch the model checkpoints from the training run and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from monai.networks.nets import SegResNet\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "config.model_blocks_down = [1, 2, 2, 4]\n",
    "config.model_blocks_up = [1, 1, 1]\n",
    "config.model_init_filters = 16\n",
    "config.model_in_channels = 4\n",
    "config.model_out_channels = 3\n",
    "config.model_dropout_prob = 0.2\n",
    "\n",
    "# create model\n",
    "model = SegResNet(\n",
    "    blocks_down=config.model_blocks_down,\n",
    "    blocks_up=config.model_blocks_up,\n",
    "    init_filters=config.model_init_filters,\n",
    "    in_channels=config.model_in_channels,\n",
    "    out_channels=config.model_out_channels,\n",
    "    dropout_prob=config.model_dropout_prob,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Fetch the latest model checkpoint artifact from the training run\n",
    "model_artifact = wandb.use_artifact(\n",
    "    \"lifesciences/brain-tumor-segmentation/8vmqcqao-checkpoint:latest\",\n",
    "    type=\"model\",\n",
    ")\n",
    "model_artifact_dir = model_artifact.download()\n",
    "\n",
    "\n",
    "# Load the model checkpoint\n",
    "model.load_state_dict(torch.load(os.path.join(model_artifact_dir, \"model.pth\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Evaluating the Model\n",
    "\n",
    "First we define some instances of `monai.metrics.DiceMetric` for all the metrics that we will be evaluating the model against on the validation split of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import Activations, AsDiscrete\n",
    "\n",
    "# Dice score for each class\n",
    "tumor_core_dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "enhancing_tumor_dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "whole_tumor_dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "\n",
    "# Mean dice score across all classes\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "# transforms to postprocess the outputs of the model for evaluation and visualization\n",
    "postprocessing_transforms = Compose(\n",
    "    [Activations(sigmoid=True), AsDiscrete(threshold=0.5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write some utility functions for evaluating each data-point from the validation dataset by logging dice score for each target class and the ground-truth and predicted segmentation labels (for granular visual comparison and analysis) to a W&B Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def get_target_area_percentage(segmentation_map):\n",
    "    segmentation_map_list = segmentation_map.flatten().tolist()\n",
    "    return segmentation_map_list.count(1.0) * 100 / len(segmentation_map_list)\n",
    "\n",
    "\n",
    "def get_class_wise_dice_scores(sample_label, predicted_label, slice_idx):\n",
    "    sample_label = torch.from_numpy(sample_label).to(device)\n",
    "    predicted_label = torch.from_numpy(predicted_label).to(device)\n",
    "    tumor_core_dice_metric(\n",
    "        y_pred=torch.unsqueeze(predicted_label[1, :, :, slice_idx], dim=0),\n",
    "        y=torch.unsqueeze(sample_label[0, :, :, slice_idx], dim=0),\n",
    "    )\n",
    "    whole_tumor_dice_metric(\n",
    "        y_pred=torch.unsqueeze(predicted_label[1, :, :, slice_idx], dim=0),\n",
    "        y=torch.unsqueeze(sample_label[1, :, :, slice_idx], dim=0),\n",
    "    )\n",
    "    enhancing_tumor_dice_metric(\n",
    "        y_pred=torch.unsqueeze(predicted_label[2, :, :, slice_idx], dim=0),\n",
    "        y=torch.unsqueeze(sample_label[2, :, :, slice_idx], dim=0),\n",
    "    )\n",
    "    dice_scores = {\n",
    "        \"Tumor-Core\": tumor_core_dice_metric.aggregate().item(),\n",
    "        \"Enhancing-Tumor\": enhancing_tumor_dice_metric.aggregate().item(),\n",
    "        \"Whole-Tumor\": whole_tumor_dice_metric.aggregate().item(),\n",
    "    }\n",
    "    tumor_core_dice_metric.reset()\n",
    "    whole_tumor_dice_metric.reset()\n",
    "    enhancing_tumor_dice_metric.reset()\n",
    "    return dice_scores\n",
    "\n",
    "\n",
    "def log_predictions_into_tables(\n",
    "    sample_image,\n",
    "    sample_label,\n",
    "    predicted_label,\n",
    "    split: str = None,\n",
    "    data_idx: int = None,\n",
    "    table: wandb.Table = None,\n",
    "):\n",
    "    sample_image = sample_image.cpu().numpy()\n",
    "    sample_label = sample_label.cpu().numpy()\n",
    "    predicted_label = predicted_label.cpu().numpy()\n",
    "    _, _, _, num_slices = sample_image.shape\n",
    "    with tqdm(total=num_slices, leave=False) as progress_bar:\n",
    "        for slice_idx in range(num_slices):\n",
    "            tumor_core_dice_metric\n",
    "            wandb_images = [\n",
    "                wandb.Image(\n",
    "                    sample_image[0, :, :, slice_idx],\n",
    "                    masks={\n",
    "                        \"ground-truth/Tumor-Core\": {\n",
    "                            \"mask_data\": sample_label[0, :, :, slice_idx],\n",
    "                            \"class_labels\": {0: \"background\", 1: \"Tumor Core\"},\n",
    "                        },\n",
    "                        \"prediction/Tumor-Core\": {\n",
    "                            \"mask_data\": predicted_label[0, :, :, slice_idx] * 2,\n",
    "                            \"class_labels\": {0: \"background\", 2: \"Tumor Core\"},\n",
    "                        },\n",
    "                    },\n",
    "                ),\n",
    "                wandb.Image(\n",
    "                    sample_image[0, :, :, slice_idx],\n",
    "                    masks={\n",
    "                        \"ground-truth/Whole-Tumor\": {\n",
    "                            \"mask_data\": sample_label[1, :, :, slice_idx],\n",
    "                            \"class_labels\": {0: \"background\", 1: \"Whole Tumor\"},\n",
    "                        },\n",
    "                        \"prediction/Whole-Tumor\": {\n",
    "                            \"mask_data\": predicted_label[1, :, :, slice_idx] * 2,\n",
    "                            \"class_labels\": {0: \"background\", 2: \"Whole Tumor\"},\n",
    "                        },\n",
    "                    },\n",
    "                ),\n",
    "                wandb.Image(\n",
    "                    sample_image[0, :, :, slice_idx],\n",
    "                    masks={\n",
    "                        \"ground-truth/Enhancing-Tumor\": {\n",
    "                            \"mask_data\": sample_label[2, :, :, slice_idx],\n",
    "                            \"class_labels\": {0: \"background\", 1: \"Enhancing Tumor\"},\n",
    "                        },\n",
    "                        \"prediction/Enhancing-Tumor\": {\n",
    "                            \"mask_data\": predicted_label[2, :, :, slice_idx] * 2,\n",
    "                            \"class_labels\": {0: \"background\", 2: \"Enhancing Tumor\"},\n",
    "                        },\n",
    "                    },\n",
    "                ),\n",
    "            ]\n",
    "            tumor_area_percentage = {\n",
    "                \"Ground-Truth\": {\n",
    "                    \"Tumor-Core\": get_target_area_percentage(\n",
    "                        sample_label[0, :, :, slice_idx]\n",
    "                    ),\n",
    "                    \"Whole-Tumor\": get_target_area_percentage(\n",
    "                        sample_label[1, :, :, slice_idx]\n",
    "                    ),\n",
    "                    \"Enhancing-Tumor\": get_target_area_percentage(\n",
    "                        sample_label[2, :, :, slice_idx]\n",
    "                    ),\n",
    "                },\n",
    "                \"Prediction\": {\n",
    "                    \"Tumor-Core\": get_target_area_percentage(\n",
    "                        predicted_label[0, :, :, slice_idx]\n",
    "                    ),\n",
    "                    \"Whole-Tumor\": get_target_area_percentage(\n",
    "                        predicted_label[1, :, :, slice_idx]\n",
    "                    ),\n",
    "                    \"Enhancing-Tumor\": get_target_area_percentage(\n",
    "                        predicted_label[2, :, :, slice_idx]\n",
    "                    ),\n",
    "                },\n",
    "            }\n",
    "            dice_scores = get_class_wise_dice_scores(\n",
    "                sample_label, predicted_label, slice_idx\n",
    "            )\n",
    "            table.add_data(\n",
    "                split,\n",
    "                data_idx,\n",
    "                slice_idx,\n",
    "                dice_scores,\n",
    "                tumor_area_percentage,\n",
    "                *wandb_images\n",
    "            )\n",
    "            progress_bar.update(1)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the prediction table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_table = wandb.Table(\n",
    "    columns=[\n",
    "        \"Split\",\n",
    "        \"Data Index\",\n",
    "        \"Slice Index\",\n",
    "        \"Dice-Score\",\n",
    "        \"Tumor-Area-Pixel-Percentage\",\n",
    "        \"Prediction/Tumor-Core\",\n",
    "        \"Prediction/Whole-Tumor\",\n",
    "        \"Prediction/Enhancing-Tumor\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we loop over the validation dataset and log the evaluation table and the mean dice scores for each class across the entore validation set to W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import inference\n",
    "\n",
    "total_tumor_core_dice_score = 0.0\n",
    "total_whole_tumor_dice_score = 0.0\n",
    "total_enhancing_tumor_dice_score = 0.0\n",
    "\n",
    "config.inference_roi_size = (240, 240, 160)\n",
    "\n",
    "# Perform inference and visualization\n",
    "with torch.no_grad():\n",
    "    for data_idx, sample in tqdm(enumerate(val_dataset), total=len(val_dataset), desc=\"Evaluating:\"):\n",
    "        test_input, test_labels = (\n",
    "            torch.unsqueeze(sample[\"image\"], 0).to(device),\n",
    "            torch.unsqueeze(sample[\"label\"], 0).to(device),\n",
    "        )\n",
    "        test_output = inference(model, test_input, config.inference_roi_size)\n",
    "        test_output = postprocessing_transforms(test_output[0])\n",
    "        dice_metric_batch(y_pred=torch.unsqueeze(test_output, dim=0), y=test_labels)\n",
    "        metric_batch = dice_metric_batch.aggregate()\n",
    "        evaluation_table = log_predictions_into_tables(\n",
    "            sample_image=torch.squeeze(test_input),\n",
    "            sample_label=torch.squeeze(test_labels),\n",
    "            predicted_label=test_output,\n",
    "            data_idx=data_idx,\n",
    "            split=\"validation\",\n",
    "            table=evaluation_table,\n",
    "        )\n",
    "        total_tumor_core_dice_score += metric_batch[0].item()\n",
    "        total_whole_tumor_dice_score += metric_batch[1].item()\n",
    "        total_enhancing_tumor_dice_score += metric_batch[2].item()\n",
    "\n",
    "    wandb.log({\"Tumor-Segmentation-Evaludation\": evaluation_table})\n",
    "    wandb.summary[\"Tumor-Score-Dice-Score\"] = total_tumor_core_dice_score / len(val_dataset)\n",
    "    wandb.summary[\"Whole-Tumor-Dice-Score\"] = total_whole_tumor_dice_score / len(val_dataset)\n",
    "    wandb.summary[\"Enhancing-Tumor-Dice-Score\"] = total_enhancing_tumor_dice_score / len(val_dataset)\n",
    "\n",
    "# End the experiment\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
