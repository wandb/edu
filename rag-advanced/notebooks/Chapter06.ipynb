{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/main/rag-advanced/notebooks/Chapter06.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-course-06} -->\n",
    "\n",
    "## Response Synthesis and Prompting\n",
    "\n",
    "Response synthesis is a critical component of RAG systems, responsible for generating coherent and accurate answers based on retrieved information. In this chapter, we'll explore techniques to improve response quality through iterative prompt engineering and model selection.\n",
    "\n",
    "Key concepts we'll cover:\n",
    "1. Baseline prompt evaluation\n",
    "2. Iterative prompt improvement\n",
    "3. Impact of model selection on response quality\n",
    "4. Comparative analysis of different prompting strategies\n",
    "\n",
    "This hands-on experience will deepen your understanding of advanced RAG concepts and prepare you to implement these techniques in your own projects.\n",
    "\n",
    "Let's begin by setting up our environment and importing the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, execute the following cell to clone the repository and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/wandb/edu.git\n",
    "%cd edu/rag-advanced\n",
    "!pip install -qqq -r requirements.txt\n",
    "%cd notebooks\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the setup complete, we can now proceed with the chapter content.\n",
    "\n",
    "Initial steps:\n",
    "1. Log in to Weights & Biases (W&B)\n",
    "2. Configure environment variables for API access\n",
    "\n",
    "To obtain your Cohere API key, visit the [Cohere API dashboard](https://dashboard.cohere.com/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Please enter your COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave version 0.51.10 is available!  To upgrade, please run:\n",
      " $ pip install weave --upgrade\n",
      "Logged in as Weights & Biases user: parambharat.\n",
      "View Weave data at https://wandb.ai/parambharat/rag-course/weave\n"
     ]
    }
   ],
   "source": [
    "WANDB_PROJECT = \"rag-course\"\n",
    "\n",
    "weave_client = weave.init(WANDB_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "We'll start by loading the semantically chunked data from Chapter 3. As a reminder, semantic chunking is an technique that groups related sentences together, preserving context and improving retrieval relevance.\n",
    "\n",
    "This chunked data will serve as the input for the knowledge base for our RAG pipeline, allowing us to compare the effectiveness of our response synthesis techniques against a baseline system.\n",
    "\n",
    "Let's load the data and take a look at the first few chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data from Chapter 3\n",
    "chunked_data = weave.ref(\"weave:///rag-course/rag-course/object/chunked_data:Ij9KThmiZQ9ljpCm8rVXTJlCaAbY2qC0zX6UJkBWHQ0\").get()\n",
    "# uncomment the next line to get the chunked data from weave from your own project instead\n",
    "# chunked_data = weave.ref(\"chunked_data:latest\").get()\n",
    "\n",
    "chunked_data.rows[:2]\n",
    "chunked_data = list(map(dict, chunked_data.rows[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the query enhancer, hybrid retriever, response generator and RAG pipeline from the previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "from scripts.query_enhancer import QueryEnhancer\n",
    "from scripts.rag_pipeline import QueryEnhancedRAGPipeline\n",
    "from scripts.response_generator import QueryEnhanedResponseGenerator\n",
    "from scripts.retriever import HybridRetrieverReranker\n",
    "\n",
    "query_enhancer = QueryEnhancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt iteration\n",
    "\n",
    "Prompt engineering is a crucial skill in developing effective RAG systems. By carefully crafting prompts, we can guide the model to produce more accurate, relevant, and coherent responses. We'll explore several iterations of prompt improvements:\n",
    "\n",
    "1. Baseline prompt\n",
    "2. Adding precise instructions\n",
    "3. Including response format examples\n",
    "4. Incorporating model reasoning\n",
    "\n",
    "For each iteration, we'll evaluate the impact on response quality using our established metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WeaveDict({'question': 'How can I access the run object from the Lightning WandBLogger function?', 'answer': \"In PyTorch Lightning, the `WandbLogger` is used to log metrics, model weights, and other data to Weights & Biases during training. To access the `wandb.Run` object from within a `LightningModule` when using `WandbLogger`, you can use the `Trainer.logger.experiment` attribute. This attribute provides direct access to the underlying `wandb.Run` object, allowing you to interact with the Weights & Biases API directly.\\n\\nHere's how you can access the `wandb.Run` object using `WandbLogger` in PyTorch Lightning:\\n\\n```python\\nfrom pytorch_lightning import Trainer, LightningModule\\nfrom pytorch_lightning.loggers import WandbLogger\\n\\nclass MyModel(LightningModule):\\n    def training_step(self, batch, batch_idx):\\n        # Your training logic here\\n        loss = ...\\n\\n        # Log metrics\\n        self.log('train_loss', loss)\\n\\n        # Access the wandb.Run object\\n        run = self.trainer.logger.experiment\\n        # Now you can use `run` to interact with wandb API, e.g., to log additional data\\n        run.log({'additional_metric': value})\\n\\n        return loss\\n\\n# Setup the logger\\nwandb_logger = WandbLogger(project='my_project', entity='my_entity')\\n\\n# Setup the trainer\\ntrainer = Trainer(logger=wandb_logger)\\n\\n# Initialize your model\\nmodel = MyModel()\\n\\n# Train the model\\ntrainer.fit(model)\\n```\\n\\nIn this example:\\n- A `WandbLogger` is instantiated and passed to the `Trainer`.\\n- Inside the `LightningModule`, you can access the `wandb.Run` object via `self.trainer.logger.experiment`.\\n- You can then use this `run` object to log additional data or interact with the Weights & Biases API directly.\\n\\nThis approach allows you to leverage the full capabilities of Weights & Biases directly from your training loop in PyTorch Lightning.\", 'contexts': [{'content': 'Using PyTorch Lightning\\'s WandbLogger\\nPyTorch Lightning has multiple `WandbLogger` (Pytorch) (Fabric) classes that can be used to seamlessly log metrics, model weights, media and more. Just instantiate the WandbLogger and pass it to Lightning\\'s `Trainer` or `Fabric`.\\nwandb_logger = WandbLogger()\\ntrainer = Trainer(logger=wandb_logger)\\nfabric = L.Fabric(loggers=[wandb_logger])\\nfabric.launch()\\nfabric.log_dict({\\n    \"important_metric\": important_metric\\n})\\nLogger arguments\\nBelow are some of the most used parameters in WandbLogger, see the PyTorch Lightning for a full list and description\\n(Pytorch)\\n(Fabric)\\nParameter\\nDescription\\n`project`\\nDefine what wandb Project to log to\\n`name`\\nGive a name to your wandb run\\n`log_model`\\nLog all models if `log_model=\"all\"` or at end of training if `log_model=True`\\n`save_dir`\\nPath where data is saved\\nLog your hyperparameters\\nclass LitModule(LightningModule):\\n    def __init__(self, *args, **kwarg):\\n        self.save_hyperparameters()\\nwandb_logger.log_hyperparams(\\n    {\\n        \"hyperparameter_1\": hyperparameter_1,\\n        \"hyperparameter_2\": hyperparameter_2,\\n    }\\n)\\nLog additional config parameters\\n# add one parameter\\nwandb_logger.experiment.config[\"key\"] = value\\n# add multiple parameters\\nwandb_logger.experiment.config.update({key1: val1, key2: val2})\\n# use directly wandb module\\nwandb.config[\"key\"] = value\\nwandb.config.update()\\nLog gradients, parameter histogram and model topology\\nYou can pass your model object to `wandblogger.watch()` to monitor your models\\'s gradients and parameters as you train. See the PyTorch Lightning `WandbLogger` documentation', 'source': 'guides/integrations/lightning.md', 'score': 0.9996229999999999, 'relevance': 2}, {'content': 'PyTorch Lightning\\nTry in a Colab Notebook here ‚Üí\\nPyTorch Lightning provides a lightweight wrapper for organizing your PyTorch code and easily adding advanced features such as distributed training and 16-bit precision. W&B provides a lightweight wrapper for logging your ML experiments. But you don\\'t need to combine the two yourself: Weights & Biases is incorporated directly into the PyTorch Lightning library via the WandbLogger.\\n‚ö° Get going lightning-fast with just a few lines.\\nfrom lightning.pytorch.loggers import WandbLogger\\nfrom lightning.pytorch import Trainer\\nwandb_logger = WandbLogger(log_model=\"all\")\\ntrainer = Trainer(logger=wandb_logger)\\n:::info\\nUsing wandb.log(): Please note that the `WandbLogger` logs to W&B using the Trainer\\'s `global_step`. If you are making additional calls to `wandb.log` directly in your code, do not use the `step` argument in `wandb.log()`. \\nInstead, log the Trainer\\'s `global_step` like your other metrics, like so:\\n`wandb.log({\"accuracy\":0.99, \"trainer/global_step\": step})`\\n:::\\nimport lightning as L\\nfrom wandb.integration.lightning.fabric import WandbLogger\\nwandb_logger = WandbLogger(log_model=\"all\")\\nfabric = L.Fabric(loggers=[wandb_logger])\\nfabric.launch()\\nfabric.log_dict({\"important_metric\": important_metric})\\nSign up and Log in to wandb\\na) Sign up for a free account\\nb) Pip install the `wandb` library\\nc) To log in in your training script, you\\'ll need to be signed in to you account at www.wandb.ai, then you will find your API key on the Authorize page.\\nIf you are using Weights and Biases for the first time you might want to check out our quickstart\\npip install wandb\\nwandb login\\n!pip install wandb\\nimport wandb\\nwandb.login()', 'source': 'guides/integrations/lightning.md', 'score': 0.9393594, 'relevance': 2}, {'content': 'Check out interactive examples!\\nYou can follow along in our video tutorial with our tutorial colab here\\nFrequently Asked Questions\\nHow does W&B integrate with Lightning?\\nThe core integration is based on the Lightning loggers API, which lets you write much of your logging code in a framework-agnostic way. `Logger`s are passed to the Lightning Trainer and are triggered based on that API\\'s rich hook-and-callback system. This keeps your research code well-separated from engineering and logging code.\\nWhat does the integration log without any additional code?\\nWe\\'ll save your model checkpoints to W&B, where you can view them or download them for use in future runs. We\\'ll also capture system metrics, like GPU usage and network I/O, environment information, like hardware and OS information, code state (including git commit and diff patch, notebook contents and session history), and anything printed to the standard out.\\nWhat if I really need to use wandb.run in my training setup?\\nYou will have to essentially expand the scope of the variable you need to access yourself. In other words, making sure that the initial conditions are the same on all processes.\\nif os.environ.get(\"LOCAL_RANK\", None) is None:\\n    os.environ[\"WANDB_DIR\"] = wandb.run.dir\\nThen, you can use `os.environ[\"WANDB_DIR\"]` to set up the model checkpoints directory. This way, `wandb.run.dir` can be used by any non-zero rank processes as well.', 'source': 'guides/integrations/lightning.md', 'score': 0.9999046300000001, 'relevance': 1}, {'content': 'With implicit wandb integrations\\nIf you\\'re using a framework integration we support, you can also pass in the callback directly:\\n@wandb_log\\ndef train_model(\\n    train_dataloader_path: components.InputPath(\"dataloader\"),\\n    test_dataloader_path: components.InputPath(\"dataloader\"),\\n    model_path: components.OutputPath(\"pytorch_model\")\\n):\\n    from pytorch_lightning.loggers import WandbLogger\\n    from pytorch_lightning import Trainer\\n    trainer = Trainer(logger=WandbLogger())\\n    ...  # do training', 'source': 'guides/integrations/other/kubeflow-pipelines-kfp.md', 'score': 0.9520419, 'relevance': 1}, {'content': 'Log images, text and more\\nThe `WandbLogger` has `log_image`, `log_text` and `log_table` methods for logging media.\\nYou can also directly call `wandb.log` or `trainer.logger.experiment.log` to log other media types such as Audio, Molecules, Point Clouds, 3D Objects and more.\\n# using tensors, numpy arrays or PIL images\\nwandb_logger.log_image(key=\"samples\", images=[img1, img2])\\n# adding captions\\nwandb_logger.log_image(key=\"samples\", images=[img1, img2], caption=[\"tree\", \"person\"])\\n# using file path\\nwandb_logger.log_image(key=\"samples\", images=[\"img_1.jpg\", \"img_2.jpg\"])\\n# using .log in the trainer\\ntrainer.logger.experiment.log(\\n    {\"samples\": [wandb.Image(img, caption=caption) for (img, caption) in my_images]},\\n    step=current_trainer_global_step,\\n)\\n# data should be a list of lists\\ncolumns = [\"input\", \"label\", \"prediction\"]\\nmy_data = [[\"cheese\", \"english\", \"english\"], [\"fromage\", \"french\", \"spanish\"]]\\n# using columns and data\\nwandb_logger.log_text(key=\"my_samples\", columns=columns, data=my_data)\\n# using a pandas DataFrame\\nwandb_logger.log_text(key=\"my_samples\", dataframe=my_dataframe)\\n# log a W&B Table that has a text caption, an image and audio\\ncolumns = [\"caption\", \"image\", \"sound\"]\\n# data should be a list of lists\\nmy_data = [\\n    [\"cheese\", wandb.Image(img_1), wandb.Audio(snd_1)],\\n    [\"wine\", wandb.Image(img_2), wandb.Audio(snd_2)],\\n]\\n# log the Table\\nwandb_logger.log_table(key=\"my_samples\", columns=columns, data=data)\\nYou can use Lightning\\'s Callbacks system to control when you log to Weights & Biases via the WandbLogger, in this example we log a sample of our validation images and predictions:', 'source': 'guides/integrations/lightning.md', 'score': 0.8296300999999999, 'relevance': 1}, {'content': \"Run\\nView source on GitHub\\nA unit of computation logged by wandb. Typically, this is an ML experiment.\\nRun(\\n    settings: Settings,\\n    config: Optional[Dict[str, Any]] = None,\\n    sweep_config: Optional[Dict[str, Any]] = None,\\n    launch_config: Optional[Dict[str, Any]] = None\\n) -> None\\nCreate a run with `wandb.init()`:\\nimport wandb\\nrun = wandb.init()\\nThere is only ever at most one active `wandb.Run` in any process,\\nand it is accessible as `wandb.run`:\\nimport wandb\\nassert wandb.run is None\\nwandb.init()\\nassert wandb.run is not None\\nanything you log with `wandb.log` will be sent to that run.\\nIf you want to start more runs in the same script or notebook, you'll need to\\nfinish the run that is in-flight. Runs can be finished with `wandb.finish` or\\nby using them in a `with` block:\\nimport wandb\\nwandb.init()\\nwandb.finish()\\nassert wandb.run is None\\nwith wandb.init() as run:\\n    pass  # log data here\\nassert wandb.run is None\", 'source': 'ref/python/run.md', 'score': 0.99928474, 'relevance': 0}, {'content': 'Examples:\\nSet where the run is logged\\nYou can change where the run is logged, just like changing\\nthe organization, repository, and branch in git:\\nimport wandb\\nuser = \"geoff\"\\nproject = \"capsules\"\\ndisplay_name = \"experiment-2021-10-31\"\\nwandb.init(entity=user, project=project, name=display_name)\\nAdd metadata about the run to the config\\nPass a dictionary-style object as the `config` keyword argument to add\\nmetadata, like hyperparameters, to your run.\\nimport wandb\\nconfig = {\"lr\": 3e-4, \"batch_size\": 32}\\nconfig.update({\"architecture\": \"resnet\", \"depth\": 34})\\nwandb.init(config=config)\\nRaises\\n`Error`\\nif some unknown or internal error happened during the run initialization.\\n`AuthenticationError`\\nif the user failed to provide valid credentials.\\n`CommError`\\nif there was a problem communicating with the WandB server.\\n`UsageError`\\nif the user provided invalid arguments.\\n`KeyboardInterrupt`\\nif user interrupts the run.\\nReturns\\nA `Run` object.', 'source': 'ref/python/init.md', 'score': 0.99253935, 'relevance': 0}, {'content': 'Runs\\nA single unit of computation logged by W&B is called a run. You can think of a W&B run as an atomic element of your whole project. You should initiate a new run when you:\\nTrain a model\\nChange a hyperparameter\\nUse a different model\\nLog data or a model as a W&B Artifact\\nDownload a W&B Artifact\\nFor example, during a sweep, W&B explores a hyperparameter search space that you specify. Each new hyperparameter combination created by the sweep is implemented and recorded as a unique run. \\n:::tip\\nSome key things to consider when you create and manage runs:\\n* Anything you log with `wandb.log` is recorded in that run.  For more information on how log objects in W&B, see Log Media and Objects. \\n* Each run is associated to a specific W&B project.\\n* View runs and their properties within the run\\'s project workspace on the W&B App UI.\\n* There is only at most one active wandb.Run in any process,\\nand it is accessible as `wandb.run`.\\n:::\\nCreate a run\\nCreate a W&B run with wandb.init():\\nimport wandb\\nrun = wandb.init()\\nWe recommend you specify a project name and a W&B entity when you create a new run. W&B creates a new project (if the project does not already exist) within the W&B entity you provide. If the project already exists, W&B stores the run in that project.\\nFor example, the following code snippet initializes a run that is stored in a project called `model_registry_example` that is scoped within a `wandbee` entity:\\nimport wandb\\nrun = wandb.init(entity=\"wandbee\", \\\\\\n        project=\"model_registry_example\")\\nW&B prints the name of the run that is created along with a URL path to find out more information about that specific run. \\nFor example, the code snippet above produces this output:', 'source': 'guides/runs/intro.md', 'score': 0.9912548, 'relevance': 0}, {'content': 'Get the command that ran the run\\nEach run captures the command that launched it on the run overview page. To pull this command down from the API, you can run:\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nmeta = json.load(run.file(\"wandb-metadata.json\").download())\\nprogram = [\"python\"] + [meta[\"program\"]] + meta[\"args\"]', 'source': 'guides/track/public-api-guide.md', 'score': 0.9897908999999999, 'relevance': 0}, {'content': 'You can then log the parent_span to W&B like as below. \\nrun = wandb.init(name=\"manual_span_demo\", project=\"wandb_prompts_demo\")\\nrun.log({\"trace\": trace_tree.WBTraceTree(parent_span)})\\nrun.finish()\\nClicking on the W&B Run link generated will take you to a workspace where you can inspect the Trace created.', 'source': 'tutorials/prompts.md', 'score': 0.98942953, 'relevance': 0}]}), WeaveDict({'question': 'is there a method to auto delete log files from my disk once a log completes?', 'answer': 'Currently, Weights & Biases does not provide a built-in method to automatically delete local log files once a log completes directly through its API or settings. However, you can manage log files manually or implement a custom solution to handle this task.\\n\\nHere are a couple of approaches you might consider:\\n\\n### 1. Manual Cleanup\\nAfter your experiments are complete and you\\'ve ensured that all data is synced with the W&B servers, you can manually delete the log files from your disk.\\n\\n### 2. Custom Script for Automatic Cleanup\\nYou can write a custom script that runs at the end of your experiments to delete the log files. Here‚Äôs a simple example using Python:\\n\\n```python\\nimport os\\nimport shutil\\n\\ndef clean_log_dir(directory):\\n    if os.path.exists(directory):\\n        shutil.rmtree(directory)\\n        print(f\"Log directory {directory} has been removed\")\\n    else:\\n        print(\"Directory does not exist\")\\n\\n# Example usage\\nlog_directory = \\'./wandb\\'\\nclean_log_dir(log_directory)\\n```\\n\\nThis script checks if the directory exists and then removes it along with all its contents. You would need to call this function at the end of your experiment or training session.\\n\\n### 3. Using Python atexit\\nIf you want the cleanup to happen automatically when your Python script exits, you can use the `atexit` module which allows you to register cleanup functions. Here\\'s how you might integrate it:\\n\\n```python\\nimport atexit\\nimport shutil\\n\\ndef cleanup_logs():\\n    log_dir = \\'./wandb\\'\\n    if os.path.exists(log_dir):\\n        shutil.rmtree(log_dir)\\n        print(f\"Cleaned up {log_dir}\")\\n\\natexit.register(cleanup_logs)\\n```\\n\\nThis will ensure that the log directory is cleaned up whenever the Python interpreter terminates normally.\\n\\n### Best Practices\\n- **Ensure Data Integrity**: Always make sure that all your data is properly synced to the W&B servers before deleting any local files to avoid data loss.\\n- **Regular Monitoring**: Keep an eye on the disk space used by W&B logs, especially when running multiple or long-running experiments.\\n- **Use W&B Artifacts**: For managing outputs and datasets more systematically, consider using [W&B Artifacts](https://docs.wandb.ai/guides/artifacts) which also supports versioning and is integrated with the W&B dashboard.\\n\\nThese methods provide flexibility depending on your specific needs and environment.', 'contexts': [{'content': 'wandb artifact cache cleanup\\nUsage\\n`wandb artifact cache cleanup [OPTIONS] TARGET_SIZE`\\nSummary\\nClean up less frequently used files from the artifacts cache\\nOptions\\nOption\\nDescription\\n--remove-temp / --no-remove-temp\\nRemove temp files', 'source': 'ref/cli/wandb-artifact/wandb-artifact-cache/wandb-artifact-cache-cleanup.md', 'score': 0.0001323819, 'relevance': 2}, {'content': 'Log Media and Objects in Experiments\\nLog Media and Objects in Experiments\\nLog a dictionary of metrics, media, or custom objects to a step with the W&B Python SDK. W&B collects the key-value pairs during each step and stores them in one unified dictionary each time you log data with `wandb.log()`. Data logged from your script is saved locally to your machine in a directory called `wandb`, then synced to the W&B cloud or your private server. \\n:::info\\nKey-value pairs are stored in one unified dictionary only if you pass the same value for each step. W&B writes all of the collected keys and values to memory if you log a different value for `step`.\\n:::\\nEach call to `wandb.log` is a new `step` by default. W&B uses steps as the default x-axis when it creates charts and panels. You can optionally create and use a custom x-axis or capture a custom summary metric. For more information, see Customize log axes.\\n:::caution\\nUse `wandb.log()` to log consecutive values for each `step`: 0, 1, 2, and so on. It is not possible to write to a specific history step. W&B only writes to the \"current\" and \"next\" step.\\n:::\\nAutomatically logged data\\nW&B automatically logs the following information during a W&B Experiment:\\nSystem metrics: CPU and GPU utilization, network, etc. These are shown in the System tab on the run page. For the GPU, these are fetched with nvidia-smi.\\nCommand line: The stdout and stderr are picked up and show in the logs tab on the run page.\\nTurn on Code Saving in your account\\'s Settings page to log:\\nGit commit: Pick up the latest git commit and see it on the overview tab of the run page, as well as a `diff.patch` file if there are any uncommitted changes.\\nDependencies: The `requirements.txt` file will be uploaded and shown on the files tab of the run page, along with any files you save to the `wandb` directory for the run.', 'source': 'guides/track/log/intro.md', 'score': 1.35253e-05, 'relevance': 2}, {'content': \"Storage\\nIf you are approaching or exceeding your storage limit, there are multiple paths forward to manage your data. The path that's best for you will depend on your account type and your current project setup.\\nManage storage consumption\\nW&B offers different methods of optimizing your storage consumption:\\nUse\\xa0reference artifacts\\xa0to track files saved outside the W&B system, instead of uploading them to W&B storage.\\nUse an external cloud storage bucket for storage. (Enterprise only)\\nDelete data\\nYou can also choose to delete data to remain under your storage limit. There are several ways to do this:\\nDelete data interactively with the app UI.\\nSet a TTL policy on Artifacts so they are automatically deleted.\", 'source': 'guides/app/features/storage.md', 'score': 0.00024156630000000002, 'relevance': 1}, {'content': 'File count\\nKeep the total number of files uploaded for a single run under 1,000. You can use W&B Artifacts when you need to log a large number of files. Exceeding 1,000 files in a single run can slow down your run pages.\\nPython script performance\\nThere are a few ways that your performance of your python script is reduced:\\nThe size of your data is too large. Large data sizes could introduce a >1 ms overhead to the training loop.\\nThe speed of your network and the how the W&B backend is configured\\nCalling `wandb.log` more than a few times per second. This is due to a small latency added to the training loop every time `wandb.log` is called.\\n:::info\\nIs frequent logging slowing your training runs down? Check out this Colab for methods to get better performance by changing your logging strategy.\\n:::\\nW&B does not assert any limits beyond rate limiting. The W&B Python SDK automatically completes an exponential \"backoff\" and \"retry\" requests that exceed limits. W&B Python SDK responds with a ‚ÄúNetwork failure‚Äù on the command line. For unpaid accounts, W&B may reach out in extreme cases where usage exceeds reasonable thresholds.\\nRate limits\\nW&B SaaS Cloud API implements a rate limit to maintain system integrity and ensure availability. This measure prevents any single user from monopolizing available resources in the shared infrastructure, ensuring that the service remains accessible to all users. You may encounter a lower rate limit for a variety of reasons. \\n:::note\\nRate limits are subject to change.\\n:::\\nThe `wandb.log` calls in your script utilize a metrics logging API to log your training data to W&B. This API is engaged through either online or offline syncing. In either case, it imposes a rate limit quota limit in a rolling time window. This includes limits on total request size and request rate, where latter refers to the number of requests in a time duration. \\nRate limits are applied to each W&B project. So if you have 3 projects in a team, each project has its own rate limit quota. Users on Teams and Enterprise plans have higher rate limits than those on the Free plan.', 'source': 'guides/track/limits.md', 'score': 1.59366e-05, 'relevance': 1}, {'content': 'End a run\\nW&B automatically ends runs and logs data from that run to your W&B project. You can end a run manually with the run.finish command. For example:\\nimport wandb\\nrun = wandb.init()\\nrun.finish()\\n:::info\\nW&B suggests that you use the wandb.finish method at the end of the child process if you call wandb.init from a child process.\\n:::', 'source': 'guides/runs/intro.md', 'score': 0.0003500686, 'relevance': 0}, {'content': '3. View your OpenAI API inputs and responses\\nClick on the Weights & Biases run link generated by `autolog` in step 1. This will redirect you to your project workspace in the W&B App.\\nSelect a run you created to view the trace table, trace timeline and the model architecture of the OpenAI LLM used.\\n4. Disable autolog\\nWe recommend that you call `disable()` to close all W&B processes when you are finished using the OpenAI API.\\nautolog.disable()\\nNow your inputs and completions will be logged to Weights & Biases, ready for analysis or to be shared with colleagues.', 'source': 'guides/integrations/other/openai-api.md', 'score': 0.0002492325, 'relevance': 0}, {'content': 'log\\nView source\\nlog(\\n    data: Dict[str, Any],\\n    step: Optional[int] = None,\\n    commit: Optional[bool] = None,\\n    sync: Optional[bool] = None\\n) -> None', 'source': 'ref/python/run.md', 'score': 0.000179528, 'relevance': 0}, {'content': \"Audit logs\\nUse W&B Server audit logs to track user activity within your teams, and to conform to your enterprise governance requirements. The audit logs are JSON-formatted, and their access mechanism(s) depend on your W&B Server deployment type:\\nW&B Server Deployment type\\nAudit logs access mechanism(s)\\nSelf-managed\\nSynced to instance-level bucket every 10 minutes. Also available using the API.\\nDedicated Cloud with secure storage connector (BYOB)\\nSynced to instance-level bucket (BYOB) every 10 minutes. Also available using the API.\\nDedicated Cloud with W&B managed storage (without BYOB)\\nOnly available using the API.\\nOnce you've access to your audit logs, analyze those using your preferred tools, such as Pandas, Amazon Redshift, Google BigQuery, Microsoft Fabric, and more. You may need to transform the JSON-formatted audit logs into a format relevant to the tool before analysis. Information on how to transform your audit logs for specific tools is outside the scope of W&B documentation.\\n:::tip\\nAudit Log Retention: If a compliance, security or risk team in your organization requires audit logs to be retained for a specific period of time, W&B recommends to periodically transfer the logs from your instance-level bucket to a long-term retention storage. If you're instead using the API to access the audit logs, you can implement a simple script that runs periodically (like daily or every few days) to fetch any logs that may have been generated since the time of the last script run, and store those in a short-term storage for analysis or directly transfer to a long-term retention storage.\\n:::\\n:::note\\nAudit logs are not available for W&B Multi-tenant Cloud yet.\\n:::\", 'source': 'guides/hosting/monitoring-usage/audit-logging.md', 'score': 3.42685e-05, 'relevance': 0}, {'content': 'Examples of logging behavior\\nKind of Variable\\nbehavior\\nExample\\nData Type\\nInstance\\nAuto-logged\\n`self.accuracy`\\n`float`\\nInstance\\nLogged if `datasets=True`\\n`self.df`\\n`pd.DataFrame`\\nInstance\\nNot logged if `datasets=False`\\n`self.df`\\n`pd.DataFrame`\\nLocal\\nNever logged\\n`accuracy`\\n`float`\\nLocal\\nNever logged\\n`df`\\n`pd.DataFrame`\\nDoes this track artifact lineage?\\nYes! If you have an artifact that is an output of step A and an input to step B, we automatically construct the lineage DAG for you.\\nFor an example of this behavior, please see this notebook and its corresponding W&B Artifacts page', 'source': 'guides/integrations/other/metaflow.md', 'score': 2.90834e-05, 'relevance': 0}, {'content': 'Log Images, Tables, Text, Audio and More\\nIn addition to metrics, you can log plots, histograms, tables, text and media such as images, videos, audios, 3D and more.\\nSome considerations when logging data include:\\nHow often should the metric be logged? Should it be optional?\\nWhat type of data could be helpful in visualizing?\\nFor images, you can log sample predictions, segmentation masks etc to see the evolution over time.\\nFor text, you can log tables of sample predictions for later exploration.\\nRefer to Log Data with wandb.log for a full guide on logging media, objects, plots and more.\\nDistributed Training\\nFor frameworks supporting distributed environments, you can adapt any of the following workflows:\\nDetect which is the ‚Äúmain‚Äù process and only use `wandb` there. Any required data coming from other processes must be routed to the main process first. (This workflow is encouraged).\\nCall `wandb` in every process and auto-group them by giving them all the same unique `group` name\\nSee Log Distributed Training Experiments for more details\\nLogging Model Checkpoints And More\\nIf your framework uses or produces models or datasets, you can log them for full traceability and have wandb automatically monitor your entire pipeline through W&B Artifacts.\\nWhen using Artifacts, it might be useful but not necessary to let your users define:\\nThe ability to log model checkpoints or datasets (in case you want to make it optional)\\nThe path/reference of the artifact being used as input if any. For example ‚Äúuser/project/artifact‚Äù\\nThe frequency for logging Artifacts', 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 1.92232e-05, 'relevance': 0}]})]\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = weave.ref(\n",
    "    \"weave:///rag-course/dev/object/Dataset:Qj4IFICc2EbdXu5A5UuhkPiWgxM1GvJMIvXEyv1DYnM\"\n",
    ").get()\n",
    "\n",
    "print(eval_dataset.rows[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.response_metrics import ALL_METRICS as RESPONSE_METRICS\n",
    "\n",
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=RESPONSE_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = HybridRetrieverReranker()\n",
    "hybrid_retriever.index_data(chunked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Prompt Evaluation\n",
    "\n",
    "We are now ready to evaluate the performance of the RAG pipeline while iterating over different prompt improvemtns.\n",
    "For comparison, let's begin our evaluation of the baseline RAG pipeline.\n",
    "\n",
    "This simple prompt serves as our starting point. It provides basic instructions for the model to answer questions about W&B using only the provided context. However, it lacks specific guidance on response structure, tone, or level of detail. As we iterate, we'll see how more detailed prompts can improve response quality and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to the following question about W&B. Provide an helful and complete answer based only on the provided documents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "INITIAL_PROMPT = open(\"prompts/initial_system.txt\").read()\n",
    "\n",
    "print(INITIAL_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m1\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m2\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m3\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_diff'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.024515745669812943</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_levenshtein'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.42531210067331005</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_rouge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.18368507693245154</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_bleu'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.032245109236558635</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'llm_response_scorer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.41022229194641</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001B[1m{\u001B[0m\n",
       "    \u001B[32m'compute_diff'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.024515745669812943\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_levenshtein'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.42531210067331005\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_rouge'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.18368507693245154\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_bleu'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.032245109236558635\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'llm_response_scorer'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'score'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.0\u001B[0m\u001B[1m}\u001B[0m, \u001B[32m'correct'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'true_count'\u001B[0m: \u001B[1;36m0\u001B[0m, \u001B[32m'true_fraction'\u001B[0m: \u001B[1;36m0.0\u001B[0m\u001B[1m}\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'model_latency'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m30.41022229194641\u001B[0m\u001B[1m}\u001B[0m\n",
       "\u001B[1m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/parambharat/rag-course/r/call/01923380-e5be-7832-b188-261d828a7712\n"
     ]
    }
   ],
   "source": [
    "baseline_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=INITIAL_PROMPT, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class BaselineRAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "baseline_rag_pipeline = BaselineRAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=baseline_response_generator,\n",
    ")\n",
    "\n",
    "\n",
    "baseline_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(baseline_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: When designing your initial prompt, aim for clarity and simplicity. However, be prepared to iterate and refine based on the results.\n",
    "\n",
    "**Best Practice**: Always establish a baseline performance to measure improvements against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Prompt V1: Adding Precise Instructions\n",
    "\n",
    "In our first iteration, let's enhance the prompt by providing more detailed instructions to the AI assistant. We'll focus on:\n",
    "1. Defining a clear role for the AI as a W&B specialist\n",
    "2. Incorporating dynamic elements like language and intent recognition\n",
    "3. Outlining a structured approach to formulating responses\n",
    "4. Specifying formatting requirements, including markdown usage\n",
    "5. Addressing edge cases, such as insufficient information or off-topic queries\n",
    "\n",
    "By adding these elements, we aim to guide the model towards generating more coherent, relevant, and well-structured responses. This approach should help maintain accuracy while ensuring proper citation of sources. As we progress, we'll evaluate how these changes impact the quality of the generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specializing in answering questions about Weights & Biases (W&B). Your task is to provide accurate, concise, and helpful responses based on retrieved documentation snippets. Follow these instructions carefully:\n",
      "\n",
      "First, review the retrieved documentation snippets related to W&B\n",
      "Then, consider the user's query\n",
      "You should respond to the user in the following language:\n",
      "{language}\n",
      "We have identified the following intents based on the user's query:\n",
      "{intents}\n",
      "\n",
      "To formulate your response:\n",
      "1. Carefully read and understand the content of each retrieved snippet.\n",
      "2. Identify the most relevant information to answer the user's query.\n",
      "3. Pay special attention to code snippets, function names, class names, and method names.\n",
      "4. Provide a concise answer that addresses the user's query and the identified intents.\n",
      "5. Use information from the retrieved snippets to support your response.\n",
      "6. Explain code snippets, functions, classes, and methods when they are relevant to the query.\n",
      "7. Present function, class, and method names exactly as they appear in the retrieved snippets.\n",
      "8. Include relevant citations from the snippets to support your answer.\n",
      "\n",
      "Format your response as follows:\n",
      "- Use markdown formatting for your entire response.\n",
      "- Use appropriate markdown syntax for headings, lists, code blocks, and emphasis.\n",
      "- For code snippets, use triple backticks (```) with the appropriate language specifier (e.g., ```python).\n",
      "- For inline code or function/class/method names, use single backticks (`).\n",
      "- Include citations using square brackets with numbers, e.g., [1], [2], etc.\n",
      "\n",
      "If the retrieved snippets do not contain enough information to fully answer the query, state this clearly in your response and provide the best possible answer with the available information. If the query is unrelated to W&B, politely inform the user that you can only answer questions about Weights & Biases.\n",
      "\n",
      "Remember, your goal is to provide helpful, correct, and concise responses that fully address the user's query and identified intents while maintaining trustworthiness through proper citations and accurate representation of W&B documentation.\n"
     ]
    }
   ],
   "source": [
    "# Can we improve the prompt with mode precise instructions ?\n",
    "\n",
    "IMPROVED_PROMPT_V1 = open(\"prompts/improved_prompt_v1.txt\").read()\n",
    "\n",
    "print(IMPROVED_PROMPT_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Adding specific instructions and defining the AI's role can significantly improve response quality.\n",
    "\n",
    "**Best Practice**: Include guidelines for handling edge cases, such as insufficient information or off-topic queries, in your prompt design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m1\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m2\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m3\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_diff'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04110968040798733</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_levenshtein'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4397243203550191</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_rouge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2374894629971851</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_bleu'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05343899805584534</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'llm_response_scorer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.61173089345296</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001B[1m{\u001B[0m\n",
       "    \u001B[32m'compute_diff'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.04110968040798733\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_levenshtein'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.4397243203550191\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_rouge'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.2374894629971851\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_bleu'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.05343899805584534\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'llm_response_scorer'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'score'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.0\u001B[0m\u001B[1m}\u001B[0m, \u001B[32m'correct'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'true_count'\u001B[0m: \u001B[1;36m0\u001B[0m, \u001B[32m'true_fraction'\u001B[0m: \u001B[1;36m0.0\u001B[0m\u001B[1m}\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'model_latency'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m22.61173089345296\u001B[0m\u001B[1m}\u001B[0m\n",
       "\u001B[1m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/parambharat/rag-course/r/call/01923381-942a-7010-b336-5b2bb882ec4c\n"
     ]
    }
   ],
   "source": [
    "improved_v1_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=IMPROVED_PROMPT_V1, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class ImprovedV1RAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "improved_v1_rag_pipeline = ImprovedV1RAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=improved_v1_response_generator,\n",
    ")\n",
    "\n",
    "\n",
    "improved_v1_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(improved_v1_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Prompt V2: Including Response Format Examples\n",
    "\n",
    "In this iteration, we further refine our prompt by incorporating a concrete example of a well-structured response. This addition serves several purposes:\n",
    "\n",
    "1. It demonstrates the desired formatting and structure, including proper use of markdown and code blocks.\n",
    "2. It shows how to integrate citations and reference relevant documentation.\n",
    "3. It illustrates the appropriate level of detail and explanation expected in responses.\n",
    "4. It provides a model for balancing technical accuracy with user-friendly explanations.\n",
    "\n",
    "By including this example, we aim to guide the model towards producing more consistent, well-formatted, and informative responses. This approach should help improve the overall quality and usefulness of the generated answers, making them more accessible to users with varying levels of technical expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specializing in answering questions about Weights & Biases (W&B). Your task is to provide accurate, concise, and helpful responses based on the retrieved documentation snippets. Follow these instructions carefully:\n",
      "\n",
      "1. You will receive retrieved documentation snippets related to W&B. These snippets contain relevant information for answering the user's query.\n",
      "2. You will also be given a user query.\n",
      "3. You should respond to the user in the following language:\n",
      "{language}\n",
      "4. We have identified the following intents based on the user's query:\n",
      "{intents}\n",
      "\n",
      "5. Analyze the retrieved snippets:\n",
      "   - Carefully read and understand the content of each snippet.\n",
      "   - Identify the most relevant information to answer the user's query.\n",
      "   - Pay special attention to code snippets, function names, class names, and method names.\n",
      "\n",
      "6. Formulate your response:\n",
      "   - Provide a concise answer that addresses the user's query.\n",
      "   - Use information from the retrieved snippets to support your response.\n",
      "   - Explain code snippets, functions, classes, and methods when they are relevant to the query.\n",
      "   - Present function, class, and method names exactly as they appear in the retrieved snippets.\n",
      "   - Include relevant citations from the snippets to support your answer.\n",
      "\n",
      "7. Format your response:\n",
      "   - Use markdown formatting for your entire response.\n",
      "   - Enclose your final answer within <answer> tags.\n",
      "   - Use appropriate markdown syntax for headings, lists, code blocks, and emphasis.\n",
      "   - For code snippets, use triple backticks (```) with the appropriate language specifier (e.g., ```python).\n",
      "   - For inline code or function/class/method names, use single backticks (`).\n",
      "   - Include citations using square brackets with numbers, e.g., [1], [2], etc.\n",
      "\n",
      "8. Examples of good responses:\n",
      "\n",
      "<answer>\n",
      "# How to Log Metrics in W&B\n",
      "\n",
      "To log metrics in Weights & Biases (W&B), you can use the `wandb.log()` function. This function lets you track various metrics during your model's training process.\n",
      "\n",
      "Here's a basic example of how to use `wandb.log()`:\n",
      "\n",
      "```python\n",
      "import wandb\n",
      "\n",
      "# Initialize a W&B run\n",
      "wandb.init(project=\"my-project\")\n",
      "\n",
      "# Train your model and log metrics\n",
      "for epoch in range(num_epochs):\n",
      "    loss = train_epoch()\n",
      "    accuracy = evaluate_model()\n",
      "    \n",
      "    wandb.log({{\n",
      "        \"epoch\": epoch,\n",
      "        \"loss\": loss,\n",
      "        \"accuracy\": accuracy\n",
      "    }})\n",
      "```\n",
      "\n",
      "In this example, we're logging three metrics: the current epoch, the loss, and the accuracy [1]. You can log any number of metrics as key-value pairs in a dictionary.\n",
      "\n",
      "Remember to call `wandb.init()` at the beginning of your script to initialize a new run [2]. This sets up the connection to the W&B servers and creates a new experiment in your project.\n",
      "\n",
      "For more advanced logging, you can also log histograms, images, and other data types. The W&B documentation provides detailed information on these features [3].\n",
      "\n",
      "References:\n",
      "\n",
      "[1] https://docs.wandb.ai/guides/track/about\n",
      "[2] https://docs.wandb.ai/guides/track/visualize\n",
      "[3] https://docs.wandb.ai/guides/track/parameters-and-sweeps/about\n",
      "</answer>\n",
      "\n",
      "9. Handling edge cases:\n",
      "   - If the retrieved snippets do not contain enough information to fully answer the query, state this clearly in your response and provide the best possible answer with the available information.\n",
      "   - If the query is unrelated to W&B, politely inform the user that you can only answer questions about Weights & Biases.\n",
      "\n",
      "Remember, your goal is to provide helpful, correct, and concise responses that fully address the user's query while maintaining trustworthiness through proper citations and accurate representation of W&B documentation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can we improve the prompt with a example of the response format ?\n",
    "\n",
    "IMPROVED_PROMPT_V2 = open(\"prompts/improved_prompt_v2.txt\").read()\n",
    "print(IMPROVED_PROMPT_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Providing concrete examples in your prompt can help guide the model towards the desired output format and structure.\n",
    "\n",
    "**Best Practice**: When including examples, ensure they demonstrate key aspects like proper citation, use of markdown, and appropriate level of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m1\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m2\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m3\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_diff'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03242778779119392</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_levenshtein'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.41844546722727527</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_rouge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.252101547051129</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_bleu'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.06253030169650166</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'llm_response_scorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3333333333333333</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.034178892771404</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001B[1m{\u001B[0m\n",
       "    \u001B[32m'compute_diff'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.03242778779119392\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_levenshtein'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.41844546722727527\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_rouge'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.252101547051129\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_bleu'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.06253030169650166\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'llm_response_scorer'\u001B[0m: \u001B[1m{\u001B[0m\n",
       "        \u001B[32m'score'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m1.0\u001B[0m\u001B[1m}\u001B[0m,\n",
       "        \u001B[32m'correct'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'true_count'\u001B[0m: \u001B[1;36m1\u001B[0m, \u001B[32m'true_fraction'\u001B[0m: \u001B[1;36m0.3333333333333333\u001B[0m\u001B[1m}\u001B[0m\n",
       "    \u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'model_latency'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m24.034178892771404\u001B[0m\u001B[1m}\u001B[0m\n",
       "\u001B[1m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/parambharat/rag-course/r/call/01923382-1938-7ef2-be27-beb30b0bbb2e\n"
     ]
    }
   ],
   "source": [
    "improved_v2_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=IMPROVED_PROMPT_V2, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class ImprovedV2RAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "improved_v2_rag_pipeline = ImprovedV2RAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=improved_v2_response_generator,\n",
    ")\n",
    "improved_v2_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(improved_v2_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Prompt V3: Incorporating Model Reasoning\n",
    "\n",
    "In this iteration, we focus on enhancing the model's reasoning process and transparency:\n",
    "\n",
    "1. We introduce a structured approach to breaking down and addressing complex queries.\n",
    "2. The prompt now explicitly requests the model to explain its thought process for each step.\n",
    "3. We emphasize the importance of providing detailed explanations, including the relevance and functionality of code elements.\n",
    "4. The example response demonstrates a clear, step-by-step structure with explanations at each stage.\n",
    "5. We've added instructions for handling edge cases more comprehensively.\n",
    "\n",
    "By encouraging the model to \"show its work,\" we aim to produce more transparent, logical, and comprehensive responses. This approach can help users better understand the reasoning behind the answers, potentially leading to improved learning outcomes and increased trust in the AI assistant's capabilities. Additionally, this structured reasoning process may help the model catch and correct its own errors, leading to more accurate and reliable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specializing in Weights & Biases (W&B). Your task is to provide accurate, detailed, and helpful responses using retrieved documentation snippets. Follow these instructions:\n",
      "\n",
      "1. You will receive documentation snippets and a user query.\n",
      "2. Respond in the specified language: {language}\n",
      "3. Identified intents: {intents}\n",
      "\n",
      "### Process:\n",
      "1. **Break Down the Query:** Divide the user's query into smaller steps and explain this breakdown.\n",
      "2. **Analyze Snippets:**\n",
      "   - Read each snippet.\n",
      "   - Identify relevant information and explain its importance.\n",
      "   - For code/functions/classes/methods:\n",
      "     - Explain their purpose and functionality.\n",
      "     - Describe their relevance to the query.\n",
      "     - Provide a step-by-step breakdown if applicable.\n",
      "3. **Formulate Response:**\n",
      "   - Address each query step with detailed explanations.\n",
      "   - Use snippets to support your response.\n",
      "   - Break down code explanations into logical steps.\n",
      "   - Use exact names from snippets for functions/classes/methods.\n",
      "   - Include citations [1], [2], etc.\n",
      "4. **Format Response:**\n",
      "   - Use markdown for headings, lists, code blocks, and emphasis.\n",
      "   - Enclose the final answer in <answer> tags.\n",
      "   - Use triple backticks for code (e.g., ```python).\n",
      "   - Use inline code formatting for function/class/method names.\n",
      "5. **Structure Response:**\n",
      "   - Overview of approach.\n",
      "   - For each step:\n",
      "     - State the step.\n",
      "     - Explain your thought process.\n",
      "     - Provide relevant information.\n",
      "     - Summarize the step's contribution to the overall answer.\n",
      "   - Conclude with a summary.\n",
      "\n",
      "### Example:\n",
      "\n",
      "<answer>\n",
      "# Logging Metrics in W&B\n",
      "\n",
      "### Approach:\n",
      "1. Define metrics.\n",
      "2. Explain basic logging method.\n",
      "3. Provide a code example.\n",
      "4. Discuss advanced features.\n",
      "\n",
      "### 1. Define Metrics\n",
      "Metrics in W&B are numerical values tracked during model training/evaluation, such as loss and accuracy [1].\n",
      "\n",
      "### 2. Basic Logging Method\n",
      "Use `wandb.log()` to log metrics. It takes a dictionary of key-value pairs (metrics) and sends data to W&B servers for visualization [2].\n",
      "\n",
      "### 3. Code Example\n",
      "```python\n",
      "import wandb\n",
      "\n",
      "wandb.init(project=\"my-project\")\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "    loss = train_epoch()\n",
      "    accuracy = evaluate_model()\n",
      "    \n",
      "    wandb.log({{\n",
      "        \"epoch\": epoch,\n",
      "        \"loss\": loss,\n",
      "        \"accuracy\": accuracy\n",
      "    }})\n",
      "```\n",
      "- Initialize W&B with `wandb.init()`.\n",
      "- Log metrics with `wandb.log()` in each epoch [3].\n",
      "\n",
      "### 4. Advanced Features\n",
      "Log histograms, images, audio, and video for richer visualizations [4].\n",
      "```python\n",
      "wandb.log({{\"histogram\": wandb.Histogram(numpy_array)}})\n",
      "wandb.log({{\"image\": wandb.Image(numpy_array)}})\n",
      "```\n",
      "\n",
      "### Conclusion\n",
      "Consistently logging metrics with `wandb.log()` helps track model performance and make data-driven decisions.\n",
      "\n",
      "References:\n",
      "[1] https://docs.wandb.ai/guides/track/about\n",
      "[2] https://docs.wandb.ai/guides/track/visualize\n",
      "[3] https://docs.wandb.ai/guides/track/parameters-and-sweeps/about\n",
      "[4] https://docs.wandb.ai/guides/track/advanced-logging\n",
      "</answer>\n",
      "\n",
      "### Handling Edge Cases:\n",
      "- If snippets lack enough information:\n",
      "  - State this limitation.\n",
      "  - Provide the best partial answer.\n",
      "  - Suggest sources or methods to find missing info.\n",
      "- If the query is unrelated to W&B:\n",
      "  - Inform the user and explain why.\n",
      "  - Suggest how to rephrase the question to relate to W&B.\n",
      "\n",
      "Your goal is to provide helpful, correct, and detailed responses, maintaining trustworthiness through proper citations and accurate representation of W&B documentation. Always show your reasoning process.\n"
     ]
    }
   ],
   "source": [
    "# Can we further improve the prompt to inlcude model reasoning ?\n",
    "\n",
    "\n",
    "IMPROVED_PROMPT_V3 = open(\"prompts/improved_prompt_v3.txt\").read()\n",
    "\n",
    "print(IMPROVED_PROMPT_V3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Encouraging the model to explain its reasoning process can lead to more transparent and logical responses.\n",
    "\n",
    "**Best Practice**: Structure your prompt to guide the model through a step-by-step approach for complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m1\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m2\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m3\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_diff'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.018671903656512365</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_levenshtein'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.45571288954271477</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_rouge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.21987534290586783</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_bleu'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0810651393107268</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'llm_response_scorer'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.474895079930622</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001B[1m{\u001B[0m\n",
       "    \u001B[32m'compute_diff'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.018671903656512365\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_levenshtein'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.45571288954271477\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_rouge'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.21987534290586783\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_bleu'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.0810651393107268\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'llm_response_scorer'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'score'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.0\u001B[0m\u001B[1m}\u001B[0m, \u001B[32m'correct'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'true_count'\u001B[0m: \u001B[1;36m0\u001B[0m, \u001B[32m'true_fraction'\u001B[0m: \u001B[1;36m0.0\u001B[0m\u001B[1m}\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'model_latency'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m25.474895079930622\u001B[0m\u001B[1m}\u001B[0m\n",
       "\u001B[1m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/parambharat/rag-course/r/call/01923382-a37b-7dc2-b827-a68433478b20\n"
     ]
    }
   ],
   "source": [
    "improved_v3_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=IMPROVED_PROMPT_V3, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class ImprovedV3RAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "improved_v3_rag_pipeline = ImprovedV3RAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=improved_v3_response_generator,\n",
    ")\n",
    "\n",
    "improved_v3_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(improved_v3_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvement: Leveraging Advanced Language Models\n",
    "\n",
    "After iterating on our prompt engineering, we now take the next step by utilizing a more advanced language model (command-r-plus). This change demonstrates an important principle in RAG system development: the synergy between prompt design and model capability. By combining our refined prompt with a more sophisticated model, we aim to:\n",
    "\n",
    "1. Improve the overall quality and coherence of generated responses\n",
    "2. Enhance the model's ability to understand and follow complex instructions\n",
    "3. Potentially increase the accuracy and depth of domain-specific knowledge\n",
    "4. Better handle nuanced queries and edge cases\n",
    "\n",
    "This step allows us to explore how model selection interacts with prompt engineering to affect response quality. As we evaluate the results, we'll gain insights into the relative impact of prompt refinement versus model capabilities in our RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Don't rely solely on prompt engineering; consider the capabilities of different models in your iterative improvement process.\n",
    "\n",
    "**Best Practice**: Balance the trade-off between response quality and latency based on your specific use-case requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m1\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m2\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001B[1;36m3\u001B[0m of \u001B[1;36m3\u001B[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_diff'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01981201789421533</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_levenshtein'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.47279508308556123</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_rouge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.29985537393243006</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_bleu'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09161535881818354</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'llm_response_scorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6666666666666666</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.111299355824787</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001B[1m{\u001B[0m\n",
       "    \u001B[32m'compute_diff'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.01981201789421533\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_levenshtein'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.47279508308556123\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_rouge'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.29985537393243006\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'compute_bleu'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.09161535881818354\u001B[0m\u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'llm_response_scorer'\u001B[0m: \u001B[1m{\u001B[0m\n",
       "        \u001B[32m'score'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m0.6666666666666666\u001B[0m\u001B[1m}\u001B[0m,\n",
       "        \u001B[32m'correct'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'true_count'\u001B[0m: \u001B[1;36m0\u001B[0m, \u001B[32m'true_fraction'\u001B[0m: \u001B[1;36m0.0\u001B[0m\u001B[1m}\u001B[0m\n",
       "    \u001B[1m}\u001B[0m,\n",
       "    \u001B[32m'model_latency'\u001B[0m: \u001B[1m{\u001B[0m\u001B[32m'mean'\u001B[0m: \u001B[1;36m31.111299355824787\u001B[0m\u001B[1m}\u001B[0m\n",
       "\u001B[1m}\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/parambharat/rag-course/r/call/01923383-3118-7652-9bbe-0375b6e68038\n"
     ]
    }
   ],
   "source": [
    "# Can we further imporve by using a better model to generate the response ?\n",
    "\n",
    "improved_v4_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r-plus\", prompt=IMPROVED_PROMPT_V3, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class ImprovedV4RAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "improved_v4_rag_pipeline = ImprovedV4RAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=improved_v4_response_generator,\n",
    ")\n",
    "\n",
    "improved_v4_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(improved_v4_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Evaluations\n",
    "\n",
    "Comparing the performance of different RAG pipeline iterations is crucial for understanding the impact of our prompt engineering efforts. By comparing metrics across various versions, we can identify trends, improvements, and potential trade-offs. This comparative analysis helps us make informed decisions about which prompting strategies are most effective for our specific use case. It's important to consider both quantitative metrics (like accuracy scores) and qualitative aspects (such as response relevance) when assessing overall performance improvements.\n",
    "\n",
    "**Tip**: Use multiple evaluation metrics to get a comprehensive view of your system's performance.\n",
    "\n",
    "**Best Practice**: Regularly reassess and refine your prompts as you gather more data on user queries and system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_retriever_responses](../images/06_compare_prompts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing RAG Pipeline Iterations\n",
    "\n",
    "Here are a few key insights from the evaluation of the RAG pipeline iterations:\n",
    "\n",
    "1. **Response Quality Improvement**: The ImprovedV3 pipelines significantly outperformed earlier versions in LLM Response Scorer metrics (0.95 vs 0.75 for baseline), indicating substantial improvements in response quality and correctness.\n",
    "\n",
    "2. **Trade-off Between Quality and Latency**: While the later iterations (V3 and V4) produced higher quality responses, they also exhibited increased latency. This highlights a common trade-off in AI systems between performance and computational efficiency.\n",
    "\n",
    "3. **Incremental Gains**: Each iteration showed improvements in various metrics, demonstrating the value of iterative refinement in prompt engineering and model selection.\n",
    "\n",
    "4. **Metric Variability**: Some metrics (e.g., Levenshtein distance) showed unexpected increases in later iterations, reminding us that different evaluation metrics can capture different aspects of performance.\n",
    "\n",
    "### Learnings\n",
    "\n",
    "1. Prompt engineering can significantly impact response quality without changing the underlying model.\n",
    "2. Combining refined prompts with more advanced models (as in V4) can lead to synergistic improvements.\n",
    "3. The choice of evaluation metrics is crucial; a holistic view using multiple metrics provides a more comprehensive understanding of system performance.\n",
    "4. In real-world applications, the balance between response quality and latency must be carefully considered based on specific use-case requirements.\n",
    "\n",
    "This evaluation underscores the complexity of optimizing RAG systems and the importance of comprehensive, multi-faceted assessment approaches in AI development.\n",
    "\n",
    "\n",
    "**Overall Best Practice**: \"Iterative improvement is key in RAG system development. Continuously analyze results, gather feedback, and refine both prompts and model selection.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. Iterative Prompt Engineering: Systematic refinement of prompts can significantly enhance response quality without changing the underlying model.\n",
    "\n",
    "2. Structured Instructions: Clear, detailed prompts with specific roles, formatting guidelines, and edge case handling improve response coherence and relevance.\n",
    "\n",
    "3. Example Integration: Including well-crafted examples in prompts helps guide the model towards desired output structure and content quality.\n",
    "\n",
    "4. Reasoning Transparency: Prompting the model to explain its thought process leads to more logical, comprehensive, and trustworthy responses.\n",
    "\n",
    "5. Model-Prompt Synergy: Combining refined prompts with more advanced language models can yield synergistic improvements in response quality.\n",
    "\n",
    "6. Performance Trade-offs: Higher quality responses often come at the cost of increased latency. Balance these factors based on specific use-case requirements.\n",
    "\n",
    "7. Multifaceted Evaluation: Use a combination of metrics to comprehensively assess improvements, as different aspects of performance may not all improve uniformly.\n",
    "\n",
    "8. Continuous Optimization: RAG system development is an ongoing process. Regularly reassess and refine prompts based on performance data and user feedback.\n",
    "\n",
    "9. Scalability and Efficiency: As prompt complexity increases, consider the impact on system efficiency and scalability in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
