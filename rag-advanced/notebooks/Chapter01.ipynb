{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "\n",
    "**Set up a basic RAG pipeline (BM25/TFIDF + simple QA model)**\n",
    "\n",
    "In this chapter, we will learn about building a simple RAG pipeline. We will mainly focus on how to preprocess and chunk the data followed by building a simple retrieval engine without using any fancy \"Vector Index\". The idea is to show the inner working of a retrieval pipeline and make you understand the workflow from a user query to a generated response using an LLM.\n",
    "\n",
    "For this chapter you will need a Cohere API key.\n",
    "\n",
    "Create an `.env` file in the `notebooks` directory. The file's content is:\n",
    "\n",
    "```\n",
    "CO_API_KEY=\"<your-cohere-api-key>\"\n",
    "```\n",
    "`.env` combined with `dotenv` allows for better API keys management in notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import wandb\n",
    "import cohere\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will start a Weights and Biases (W&B) run.\n",
    "\n",
    "Throughout this notebook, W&B will be used to store, version, and download text files, creating a lineage. We will begin with an already uploaded raw data file as a [W&B Artifact](https://docs.wandb.ai/guides/artifacts), download it, inspect it, and devise a chunking strategy. Finally, we will upload the processed documents back to W&B as an Artifact, establishing a lineage (DAG) between the raw and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove this once we more to the final project\n",
    "# documents_artifact = wandb.Artifact(\n",
    "#     name=\"wandb_docs\",\n",
    "#     type=\"dataset\",\n",
    "#     description=\"W&B Documentation in Markdown format\",\n",
    "#     metadata={\n",
    "#         \"total_files\": 380,\n",
    "#         \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# documents_artifact.add_dir(\"../data/wandb_docs\")\n",
    "# run.log_artifact(documents_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "Use [W&B Artifacts](https://docs.wandb.ai/guides/artifacts) to track and version data as the inputs and outputs of your W&B Runs. For example, a model training run might take in a dataset as input and produce a trained model as output. W&B Artifact is a powerful object storage with rich UI functionalities.\n",
    "\n",
    "Below we are downloading an artifact named `wandb_docs` which will download 380 markdown files in your `../data/wandb_docs` directory. This is our data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/wandb_docs:latest\", type=\"dataset\"\n",
    ")\n",
    "data_dir = \"../data/wandb_docs\"\n",
    "\n",
    "docs_dir = documents_artifact.download(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the `../data/wandb_docs` directory below, we see that we have downloaded 380 files. The first 5 files are all in markdown (`.md` file format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = pathlib.Path(docs_dir)\n",
    "docs_files = sorted(docs_dir.rglob(\"*.md\"))\n",
    "\n",
    "print(f\"Number of files: {len(docs_files)}\\n\")\n",
    "print(\"First 5 files:\\n{files}\".format(files=\"\\n\".join(map(str, docs_files[:5]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at an example file below. We take the first element of the list (`docs_files`) and use a convenient `Path.read_text` method which returns the decoded contents of the pointed-to file as a string.\n",
    "\n",
    "üí° Looking at the example, we see some format to it. While building an ingestion pipeline, it is a good practice to look through few examples to see if there is any pattern to your data source. It helps to come up with better preprocessing steps and chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_files[0].read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are storing files as dictionaries with content (raw text) and metadata. Metadata is extra information for that data point which can be used to group together similar data points, or filter out a few data points. We will see in future chapters the importance of metadata and why it should not be ignored while building the ingestion pipeline.\n",
    "\n",
    "The metadata can be derived (`raw_tokens`) or is inherent (`source`) to the data point.\n",
    "\n",
    "Note that we are simply doing word counting and calling it `raw_tokens`. In practice we would be using the [tiktoken tokenizer](https://github.com/openai/tiktoken) to calculate the token counts but this naive calculation is an okay approximation for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll store the files as dictionaries with some content and metadata\n",
    "data = []\n",
    "for file in docs_files:\n",
    "    content = file.read_text()\n",
    "    data.append(\n",
    "        {\n",
    "            \"content\": content,\n",
    "            \"metadata\": {\n",
    "                \"source\": str(file.relative_to(docs_dir)),\n",
    "                \"raw_tokens\": len(content.split()),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the total number of tokens of your data source is a good practice. In this case, the total tokens is more than 200k. Surely, most LLM providers cannot process these many tokens. Building a RAG is justified in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = sum(map(lambda x: x[\"metadata\"][\"raw_tokens\"], data))\n",
    "print(f\"Total Tokens in dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly created list of dictionaries with `content` and `metadata` will now be logged as a W&B Artifact called `raw_data`. We started with the `wandb_docs` artifact and now we are are logging the processed data as `raw_data` artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's store the raw data in an artifact for future use and reproducibility\n",
    "raw_artifact = wandb.Artifact(\n",
    "    name=\"raw_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Raw wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_tokens,\n",
    "    },\n",
    ")\n",
    "with raw_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(raw_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the data\n",
    "\n",
    "Each document contains a large number of tokens, so we need to split it into smaller chunks to manage the token count per chunk. This approach serves three main purposes:\n",
    "\n",
    "* Most embedding models have a limit of 512 tokens per input (based on the tokenizer used during their training).\n",
    "\n",
    "* Chunking allows us to retrieve and send only the most relevant portions to our LLM, significantly reducing the total token count. This helps keep the LLM‚Äôs cost and processing time manageable.\n",
    "\n",
    "* When the text is small-sized, embedding models tend to generate better vectors as they can capture more fine-grained details and nuances in the text, resulting in more accurate representations.\n",
    "\n",
    "Below we are chunking each content (text) to a maximum length of 500 tokens (`CHUNK_SIZE`). We are not overlapping (`CHUNK_OVERLAP`) the content of one chunk with another chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are hyperparameters of our ingestion pipeline\n",
    "\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 0\n",
    "\n",
    "\n",
    "def split_into_chunks(\n",
    "    text: str, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP\n",
    ") -> List[str]:\n",
    "    \"\"\"Function to split the text into chunks of a maximum number of tokens\n",
    "    ensure that the chunks are of size CHUNK_SIZE and overlap by chunk_overlap tokens\n",
    "    use the `tokenizer.encode` method to tokenize the text\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start = end - chunk_overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `raw_data` artifact we pushed as Artifacts earlier. Let's download it. By using `use_artifact` method, we are starting a lineage from the `raw_data` artifact to the new artifact we will create further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/raw_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = raw_artifact.download()\n",
    "raw_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "raw_data = list(map(json.loads, raw_data_file.read_text().splitlines()))\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us chunk each document in the raw data Artifact. We create a new list of dictionaries with the chuked text (`content`) and with `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "for doc in raw_data:\n",
    "    chunks = split_into_chunks(doc[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        chunked_data.append(\n",
    "            {\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"source\": doc[\"metadata\"][\"source\"],\n",
    "                    \"raw_tokens\": len(chunk.split()),\n",
    "                },\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "We clean the chunks for special tokens that we find breaks the OpenAI's `client.chat.completions` api. The data cleaning step is crucial for most ML pipelien and even for a RAG/Agentic pipeline. Usually, higher quality chunks provided to an LLM generates a higher quality response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_tokenization_safe(content: str) -> str:\n",
    "    special_tokens_set = {\n",
    "        \"<|endofprompt|>\",\n",
    "        \"<|endoftext|>\",\n",
    "        \"<|fim_middle|>\",\n",
    "        \"<|fim_prefix|>\",\n",
    "        \"<|fim_suffix|>\",\n",
    "    }\n",
    "\n",
    "    def remove_special_tokens(text: str) -> str:\n",
    "        \"\"\"Removes special tokens from the given text.\n",
    "\n",
    "        Args:\n",
    "            text: A string representing the text.\n",
    "\n",
    "        Returns:\n",
    "            The text with special tokens removed.\n",
    "        \"\"\"\n",
    "        for token in special_tokens_set:\n",
    "            text = text.replace(token, \"\")\n",
    "        return text\n",
    "\n",
    "    cleaned_content = remove_special_tokens(content)\n",
    "    return cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "for doc in chunked_data:\n",
    "    cleaned_doc = doc.copy()\n",
    "    cleaned_doc[\"cleaned_content\"] = make_text_tokenization_safe(doc[\"content\"])\n",
    "    cleaned_doc[\"metadata\"][\"cleaned_tokens\"] = len(\n",
    "        cleaned_doc[\"cleaned_content\"].split()\n",
    "    )\n",
    "    cleaned_data.append(cleaned_doc)\n",
    "cleaned_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will store the cleaned data as an Artifact named `chunked_data`. The metadata that we are logging along with the artifact allows us to go back to it later and be able to reproduce the cleaning steps. It also allows us to pick the version of the `chunked_data` that we are willing to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_raw_tokens = sum(map(lambda x: x[\"metadata\"][\"raw_tokens\"], cleaned_data))\n",
    "total_cleaned_tokens = sum(map(lambda x: x[\"metadata\"][\"cleaned_tokens\"], cleaned_data))\n",
    "\n",
    "chunked_artifact = wandb.Artifact(\n",
    "    name=\"chunked_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Chunked wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(cleaned_data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_raw_tokens,\n",
    "        \"total_cleaned_tokens\": total_cleaned_tokens,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "    },\n",
    ")\n",
    "with chunked_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in cleaned_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(chunked_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the data\n",
    "\n",
    "One of the key ingredient of most retrieval system is to represent the given modality (text in our case) as a vector. This vector is a numerical representation representing the \"content\" of that modality (text). \n",
    "\n",
    "Text vectorization (text to vector) can be done using various techniques like [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model), [TF-IDF](https://en.wikipedia.org/wiki/Tf‚Äìidf) (Term Frequency-Inverse Document Frequency), and embeddings like [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), [GloVe](https://nlp.stanford.edu/projects/glove/), and transformer based architectures like BERT and more, which capture the semantic meaning and relationships between words or sentences. \n",
    "\n",
    "Below, we are downloading the `cleaned_data` artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![01_lineage_artifacs_data](../images/01_lineage_artifacts_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are creating a simple `Retriever` class. This class is responsible for vectorizing the chunks using the `index_data` method and provides a convenient method `search`, for querying the vector index using cosine distance similarity.\n",
    "\n",
    "- `index_data` will take a list of chunks and vectorize it using TF-IDF and store it as `index`. \n",
    "\n",
    "- `search` will take a `query` (question) and vectorize it using the same technique (TF-IDF in our case). It then computes the cosine distance between the query vector and the index (list of vectors) and pick the top `k` vectors from the index. These top `k` vectors represent the chunks that are closest (most relevant) to the `query`.\n",
    "\n",
    "---\n",
    "\n",
    "Note that the `Retriever` class is inherited from `weave.Model`. TODO: a bit on pydantic `BaseModel`?\n",
    "\n",
    "A Model is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates. By structuring your code to be compatible with this API, you benefit from a structured way to version your application so you can more systematically keep track of your experiments.\n",
    "\n",
    "To create a model in Weave, you need the following:\n",
    "\n",
    "- a class that inherits from weave.Model\n",
    "- type definitions on all attributes\n",
    "- a typed `predict`, `infer` or `forward` method with `@weave.op()` decorator.\n",
    "\n",
    "Imagine `weave.op()` to be a shameless use of `print` statement. If you have not initialized a weave run by doing `weave.init`, the code will work as it is without any tracking.\n",
    "\n",
    "The `predict` method decodated with `weave.op()` will track the model settings along with the inputs and outputs anytime you call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever(weave.Model):\n",
    "    vectorizer: TfidfVectorizer = TfidfVectorizer()\n",
    "    index: list = None\n",
    "    data: list = None\n",
    "\n",
    "    @weave.op()\n",
    "    def index_data(self, data):\n",
    "        self.data = data\n",
    "        docs = [doc[\"cleaned_content\"] for doc in data]\n",
    "        self.index = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    @weave.op()\n",
    "    def search(self, query, k=5):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        cosine_distances = cdist(\n",
    "            query_vec.todense(), self.index.todense(), metric=\"cosine\"\n",
    "        )[0]\n",
    "        top_k_indices = cosine_distances.argsort()[:k]\n",
    "        output = []\n",
    "        for idx in top_k_indices:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"source\": self.data[idx][\"metadata\"][\"source\"],\n",
    "                    \"text\": self.data[idx][\"cleaned_content\"],\n",
    "                    \"score\": 1 - cosine_distances[idx],\n",
    "                }\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str, k: int):\n",
    "        return self.search(query, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our Retriever in action. We will index our chunked data and then ask a question to retriev related chunks.\n",
    "\n",
    "We will not be using weave here to show that the code works as it is and that weave is a lightweight wrapper, the benefit of which we will show in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I use W&B to log metrics in my training script?\"\n",
    "search_results = retriever.search(query)\n",
    "for result in search_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a response\n",
    "\n",
    "There are two components of any RAG pipeline - a `Retriever` and a `ResponseGenerator`. Earlier, we designed a simple retriever. Here we are designing a simple `ResponseGenerator`. \n",
    "\n",
    "The `generate_response` method takes the user question along with the retrieved context (chunks) as inputs and makes a LLM call using the `model` and `prompt` (system prompt). This way the generated answer is grounded on the documentation (our usecase). In this course we are using Cohere's `command-r` model.\n",
    "\n",
    "As earlier, we have wrapped this `ResponseGenerator` class with weave for tracking the inputs and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGenerator(weave.Model):\n",
    "    model: str\n",
    "    prompt: str\n",
    "    client: cohere.Client = None\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_context(self, context: List[Dict[str, any]]) -> str:\n",
    "        return [{\"source\": item['source'], \"text\": item['text']} for item in context]\n",
    "    \n",
    "    @weave.op()\n",
    "    def generate_response(self, query: str, context: List[Dict[str, any]]) -> str:\n",
    "        contexts = self.generate_context(context)\n",
    "        response = self.client.chat(\n",
    "            preamble=self.prompt,\n",
    "            message=query,\n",
    "            model=self.model,\n",
    "            documents=contexts,\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str, context: List[Dict[str, any]]):\n",
    "        return self.generate_response(query, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the system prompt. Consider this to be set of instructions on what to do with the user question and the retrieved contexts. In practice, the system prompt can be very detailed and involved (depending on the usecase) but we are showing a simple prompt. Later we will iterate on it and show how improving the system prompt improves the quality of the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "Answer to the following question about W&B. Provide an helful and complete answer based only on the provided documents.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the response for the question \"How do I use W&B to log metrics in my training script?\". We have already retrieved the context in the previous section and passing both the question and the context to the `generate_response` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generator = ResponseGenerator(model=\"command-r\", prompt=PROMPT)\n",
    "answer = response_generator.generate_response(query, search_results)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Retrieval Augmented Generation (RAG) Pipeline\n",
    "\n",
    "Below we are bringing everything together. As stated a simple RAG pipeline constitute of a retriever and a response generator. \n",
    "\n",
    "The `__call__` method is calling the `predict` method which is decorated with `weave.op()`. It takes the user query, retrieves relevant context using the retriever and finally synthesize a response grounded on the data source (documentation in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline(weave.Model):\n",
    "    retriever: Retriever = None\n",
    "    response_generator: ResponseGenerator = None\n",
    "    top_k: int = 5\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str):\n",
    "        context = self.retriever.predict(query, self.top_k)\n",
    "        return self.response_generator.predict(query, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the `RAGPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever\n",
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)\n",
    "\n",
    "# Initialize the response generator\n",
    "response_generator = ResponseGenerator(model=\"command-r\", prompt=PROMPT)\n",
    "\n",
    "# Bring them together as a RAG pipeline\n",
    "rag_pipeline = RAGPipeline(\n",
    "    retriever=retriever,\n",
    "    response_generator=response_generator,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to use `weave.init(<project-name>)` to see what we get from using W&B Weave. Once intialized, we will start tracking the inputs and the outputs along with underlying attributes (model name, top_k, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = rag_pipeline.predict(\"How do I get get started with wandb?\")\n",
    "print(response, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link starting with a üç©. This is the trace timeline for all the executions that happened in our simple RAG application. Go to the link and drill down to find everything that got tracked.\n",
    "\n",
    "![weave trace timeline](../images/01_weave_trace_timeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add exercise for chapter 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
