{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2:\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/rag-finance-workshop/rag-advanced/notebooks/Chapter02.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-course-02} -->\n",
    "\n",
    "**Comprehensive Evaluation Strategies**\n",
    "\n",
    "In this chapter, we will evaluate the two main components of a RAG pipeline - retriever and response generator.\n",
    "\n",
    "Evaluating the retriever can be considered component evaluation. Depending on your RAG pipeline, there can be a few components and for ensuring robustness of your system, it is recommended to come up with evaluation for each components.\n",
    "\n",
    "We start off by setting up the required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone --branch rag-finance-workshop https://github.com/wandb/edu.git\n",
    "    %cd edu/rag-advanced\n",
    "    !pip install -qqq -r requirements.txt\n",
    "    %cd notebooks\n",
    "else:\n",
    "    print(\"Not running in Google Colab. Skipping git clone and pip install commands.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from set_env import set_env\n",
    "\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "print(\"API keys set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import weave\n",
    "\n",
    "from scripts.utils import display_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will also use W&B Weave for our evaluation purposes. The `weave.Evaluation` class is a light weight class that can be used to evaluate the performance of a `weave.Model` on a `weave.Dataset`. We will go into more details.\n",
    "\n",
    "We first initialize a weave client which can track both the traces and the evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJECT = \"rag-course-finance\"\n",
    "\n",
    "weave_client = weave.init(WANDB_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data for evaluation\n",
    "\n",
    "We are using a subset of evaluation data from Docugami.\n",
    "\n",
    "Learn more about how evaluation datasets are created here:\n",
    "\n",
    "- [How to Evaluate an LLM, Part 1: Building an Evaluation Dataset for our LLM System](https://wandb.ai/wandbot/wandbot-eval/reports/How-to-Evaluate-an-LLM-Part-1-Building-an-Evaluation-Dataset-for-our-LLM-System--Vmlldzo1NTAwNTcy)\n",
    "- [How to Evaluate an LLM, Part 2: Manual Evaluation of Wandbot, our LLM-Powered Docs Assistant](https://wandb.ai/wandbot/wandbot-eval/reports/How-to-Evaluate-an-LLM-Part-2-Manual-Evaluation-of-Wandbot-our-LLM-Powered-Docs-Assistant--Vmlldzo1NzU4NTM3)\n",
    "\n",
    "The main take away from these reports are:\n",
    "\n",
    "- we first deployed wandbot for internal usage based on rigorous eyeballing based evalution.\n",
    "- the user query distribution was throughly analyzed and clustered. we sampled a good representative queries from these clusters and created a gold standard set of queries.\n",
    "- we then used in-house MLEs to perform manual evaluation using Argilla. Creating such evaluation platforms are easy.\n",
    "- To summarize, speed is the key here. Use whatever means you have to create a meaningful eval set.\n",
    "\n",
    "The evaluation samples are logged as [`weave.Dataset`](https://wandb.github.io/weave/guides/core-types/datasets/). `weave.Dataset` enables you to collect examples for evaluation and automatically track versions for accurate comparisons.\n",
    "\n",
    "However for this use case we synthetically created the evaluation dataset from the Docugami data in `scripts/generate_context_list.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy eval dataset with 20 samples.\n",
    "# eval_dataset = weave.ref(\n",
    "#     \"eval_data:latest\"\n",
    "# ).get()\n",
    "eval_dataset = weave.ref(\"weave:///a-sh0ts/rag-course-finance/object/eval_data:CoQDvdOENbZqkwg7IlhZm33drBCAf9OUNvf8ar6YHzM\").get()\n",
    "\n",
    "print(\"Number of evaluation samples: \", len(eval_dataset.rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through each sample is easy.\n",
    "\n",
    "We have the question, ground truth answer and ground truth contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(eval_dataset.rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Retriever\n",
    "\n",
    "The fundamental idea of evaluating a retriever is to check how well the retrieved content matches the expected contents. For evaluating a RAG pipeline end to end, we need query and ground truth answer pairs. The ground truth answer must be grounded on some \"ground\" truth chunks. This is a search problem, it's easiest to start with traditional Information retrieval metrics.\n",
    "\n",
    "You might already have access to such evaluation dataset depending on the nature of your application or you can synthetically build one. To build one you can retrieve random documents/chunks and ask an LLM to generate query-answer pairs - the underlying documents/chunks will act as your ground truth chunk.\n",
    "\n",
    "In the sections below, we will look at different metrics that can be used to evaluate the retriever we built in the last chapter.\n",
    "\n",
    "First let us download the chunked data (from chapter 1) and index it using a simple TFIDF based retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = weave.ref(\"chunked_data:latest\").get()\n",
    "print(\"Number of chunked data: \", len(chunked_data.rows))\n",
    "chunked_data.rows[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the `TFIDFRetriever` which is an instance of `weave.Model` and index the chunked data from the last chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retriever import TFIDFRetriever\n",
    "\n",
    "display_source(TFIDFRetriever)\n",
    "\n",
    "retriever = TFIDFRetriever()\n",
    "retriever.index_data(list(map(dict, chunked_data.rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics to evaluate retriever\n",
    "\n",
    "We can evaluate a retriever using traditional ML metrics. We can also evaluate it by using a powerful LLM (next section).\n",
    "\n",
    "Below we are importing both traditional metrics and LLM as a judge metric from the `scripts/retrieval_metrics.py` file.\n",
    "\n",
    "* **Hit Rate**: Measures the proportion of queries where the retriever successfully returns at least one relevant document.\n",
    "* **MRR (Mean Reciprocal Rank)**: Evaluates how quickly the retriever returns the first relevant document, based on the reciprocal of its rank.\n",
    "* **NDCG (Normalized Discounted Cumulative Gain)**: Assesses the quality of the ranked retrieval results, giving more importance to relevant documents appearing earlier.\n",
    "* **MAP (Mean Average Precision)**: Computes the mean precision across all relevant documents retrieved, considering the rank of each relevant document.\n",
    "* **Precision**: Measures the ratio of relevant documents retrieved to the total documents retrieved by the retriever.\n",
    "* **Recall**: Evaluates the ratio of relevant documents retrieved to the total relevant documents available for the query.\n",
    "* **F1 Score**: The harmonic mean of precision and recall, providing a balance between both metrics to gauge retriever performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retrieval_metrics import IR_METRICS\n",
    "from scripts.utils import display_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first understand the basic traditional metrics we will be using. Each metric expects a `model_output` which is a list of retrieved chunks from the retriever and `contexts` which is a list of ground truth contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scorer in IR_METRICS:\n",
    "    display_source(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating retrieval on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=IR_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\": 5},\n",
    ")\n",
    "\n",
    "retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM evaluator for evaluating retriever\n",
    "\n",
    "**ref: https://arxiv.org/pdf/2406.06519**\n",
    "\n",
    "How do we evaluate if we don't have any ground truth?\n",
    "\n",
    "We can use a powerful LLM as a judge to evaluate the retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retrieval_metrics import LLM_METRICS\n",
    "\n",
    "for metric in LLM_METRICS:\n",
    "    display_source(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"LLM_Judge_Retrieval_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=LLM_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\": 5},\n",
    ")\n",
    "retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.rag_pipeline import SimpleRAGPipeline\n",
    "from scripts.response_generator import SimpleResponseGenerator\n",
    "\n",
    "INITIAL_PROMPT = open(\"prompts/initial_system.txt\", \"r\").read()\n",
    "response_generator = SimpleResponseGenerator(model=\"gpt-4o-mini\", prompt=INITIAL_PROMPT)\n",
    "rag_pipeline = SimpleRAGPipeline(\n",
    "    retriever=retriever, response_generator=response_generator, top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.response_metrics import NLP_METRICS\n",
    "\n",
    "for scorer in NLP_METRICS:\n",
    "    display_source(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=NLP_METRICS[:-1],\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")\n",
    "\n",
    "response_scores = asyncio.run(response_evaluations.evaluate(rag_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Response Judge\n",
    "\n",
    "Some metrics cannot be defined objectively and are particularly useful for more subjective or complex criteria.\n",
    "We care about correctness, faithfulness, and relevance.\n",
    "\n",
    "- **Answer Correctness** - Is the generated answer correct compared to the reference and thoroughly answers the user's query?\n",
    "- **Answer Relevancy** - Is the generated answer relevant and comprehensive?\n",
    "- **Answer Factfulness** - Is the generated answer factually consistent with the context document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.response_metrics import LLM_METRICS\n",
    "\n",
    "for metric in LLM_METRICS:\n",
    "    display_source(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_evaluations = weave.Evaluation(\n",
    "    name=\"Correctness_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=LLM_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")\n",
    "\n",
    "response_scores = asyncio.run(correctness_evaluations.evaluate(rag_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Implement the `Relevance` and `Faithfulness` LLM evaluators (LLM as a Judge) and evaluate the pipeline on all the dimensions.\n",
    "2. Generate and share a W&B report with the following sections in the form of tables and charts:\n",
    "    \n",
    "    - Summary of the evaluation\n",
    "    - Retrieval Evaluations\n",
    "        - IR Metrics\n",
    "        - LLM As a Retrieval Judge Metric\n",
    "    - Response Evaluations\n",
    "        - Traditional NLP Metrics\n",
    "        - LLM Judgement Metrics\n",
    "    - Overall Evaluations\n",
    "    - Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
