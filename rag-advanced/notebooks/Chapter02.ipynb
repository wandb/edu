{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP # Chapter 2:\n",
    "\n",
    "**Comprehensive Evaluation Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import difflib\n",
    "import Levenshtein\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import wandb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import json\n",
    "import pathlib\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and improving an evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data for evaluation\n",
    "Get from data from the docs website [FAQs](https://docs.wandb.ai/guides/technical-faq) to test the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # TODO: Remove this once we more to the final project\n",
    "# eval_artifact = wandb.Artifact(\n",
    "#     name=\"eval_dataset\",\n",
    "#     type=\"dataset\",\n",
    "#     description=\"Evaluation dataset for RAG\",\n",
    "#     metadata={\n",
    "#         \"total_samples\": 20,\n",
    "#         \"date_collected\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "#         \"chapter\": \"Chapter 1\",\n",
    "#     },\n",
    "# )\n",
    "# eval_artifact.add_file(\"../data/eval/eval_dataset.jsonl\")\n",
    "# run.log_artifact(eval_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparambharat\u001b[0m (\u001b[33mrag-course\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/mugan/data/wandb/projects/edu/rag-advanced/notebooks/wandb/run-20240708_171639-viwrtu1t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rag-course/dev/runs/viwrtu1t' target=\"_blank\">gallant-dew-34</a></strong> to <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">https://wandb.ai/rag-course/dev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rag-course/dev/runs/viwrtu1t' target=\"_blank\">https://wandb.ai/rag-course/dev/runs/viwrtu1t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/08 17:16:42 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/okxu0mvh/artifact/938190750/wandb_manifest.json?Expires=1720442801&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=E07SayxlnKgIbXzbCzjTmZ3JuxUsL8hvftJQInaxLcSBbGGA44it7V4X%2FPXTq%2Fn2%2BQ0fedoAEKkslrE9oTLffQyKFHw2Pkv6Vzo9YD96ydEdicErLJBaRukp7UORoLd%2FlZK63zAuqReu7T5zI0WpCn2I%2BQ1nYbzPcxQWG5jgjRIUjvR2JINur2xcmy7GamoygVEGxgP6IRB%2F5CE8Cb3xNgfVhBu4GKv%2B6pKV1kY7G%2FavudowA2AVKnbcuCTFvc%2BmJNBAPHxTqPb5DFxNxshMBf7g24HeH3OpRi%2BeRIcYmRQn0Lj15w4TJVgkKKrxTq8vvqNrKZhG7yM4Pr7BzAmA6g%3D%3D\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>The summary is the value that shows in the tab...</td>\n",
       "      <td>guides/technical-faq/general.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>If you have two W&amp;B accounts working from the ...</td>\n",
       "      <td>guides/technical-faq/general.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>We love the TensorBoard folks, and we have a T...</td>\n",
       "      <td>guides/technical-faq/general.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>guides/technical-faq/admin.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>guides/technical-faq/admin.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>**Dataset Examples**\\n\\nBy default, we don't l...</td>\n",
       "      <td>guides/technical-faq/metrics-and-performance.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>Using `wandb.log({'final_accuracy': 0.9}` will...</td>\n",
       "      <td>guides/technical-faq/metrics-and-performance.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>We recommend you have roughly 10k runs per pro...</td>\n",
       "      <td>guides/technical-faq/metrics-and-performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>If you're training on an offline machine and w...</td>\n",
       "      <td>guides/technical-faq/setup.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>If you're seeing SSL or network errors:`wandb:...</td>\n",
       "      <td>guides/technical-faq/troubleshooting.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>If the wandb library is unable to connect to t...</td>\n",
       "      <td>guides/technical-faq/troubleshooting.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, the API...</td>\n",
       "      <td>quickstart.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>Create a W&amp;B Experiment in four steps:\\n\\n1. [...</td>\n",
       "      <td>guides/track/launch.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>Use `wandb.log()` to save your table to the ru...</td>\n",
       "      <td>track/log/log-tables.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>You can log a list of values iteratively, or ...</td>\n",
       "      <td>track/log/logging-faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>guides/sweep/faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>You can use the `${args_no_boolean_flags}` mac...</td>\n",
       "      <td>guides/sweep/faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>It's available as the `.name` attribute of a `...</td>\n",
       "      <td>guides/track/tracking-faq.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>When `wandb.init` is called in your script, we...</td>\n",
       "      <td>guides/track/tracking-faq.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>We treat `/` as a separator for organizing log...</td>\n",
       "      <td>guides/track/log/logging-faqs.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is the difference between `.log()` and `....   \n",
       "1   How do I switch between accounts on the same m...   \n",
       "2              How is W&B different from TensorBoard?   \n",
       "3   What is the difference between team and organi...   \n",
       "4   What is the difference between team and entity...   \n",
       "5   Can I just log metrics, no code or dataset exa...   \n",
       "6   How can I log a metric that doesn't change ove...   \n",
       "7                How many runs to create per project?   \n",
       "8                            Can I run wandb offline?   \n",
       "9                  How do I deal with network issues?   \n",
       "10  What happens if internet connection is lost wh...   \n",
       "11                        Where do I find my API key?   \n",
       "12                  How do I create a W&B Experiment?   \n",
       "13                               Log a table to a run   \n",
       "14                     How do I log a list of values?   \n",
       "15  Is there a way to add extra values to a sweep,...   \n",
       "16  Can we flag boolean variables as hyperparameters?   \n",
       "17  How do I programmatically access the human-rea...   \n",
       "18  How can I save the git commit associated with ...   \n",
       "19  How can I organize my logged charts and media ...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   The summary is the value that shows in the tab...   \n",
       "1   If you have two W&B accounts working from the ...   \n",
       "2   We love the TensorBoard folks, and we have a T...   \n",
       "3   A team is a collaborative workspace for a grou...   \n",
       "4   A team is a collaborative workspace for a grou...   \n",
       "5   **Dataset Examples**\\n\\nBy default, we don't l...   \n",
       "6   Using `wandb.log({'final_accuracy': 0.9}` will...   \n",
       "7   We recommend you have roughly 10k runs per pro...   \n",
       "8   If you're training on an offline machine and w...   \n",
       "9   If you're seeing SSL or network errors:`wandb:...   \n",
       "10  If the wandb library is unable to connect to t...   \n",
       "11  Once you've signed in to www.wandb.ai, the API...   \n",
       "12  Create a W&B Experiment in four steps:\\n\\n1. [...   \n",
       "13  Use `wandb.log()` to save your table to the ru...   \n",
       "14   You can log a list of values iteratively, or ...   \n",
       "15  You cannot change the Sweep configuration once...   \n",
       "16  You can use the `${args_no_boolean_flags}` mac...   \n",
       "17  It's available as the `.name` attribute of a `...   \n",
       "18  When `wandb.init` is called in your script, we...   \n",
       "19  We treat `/` as a separator for organizing log...   \n",
       "\n",
       "                                             source  \n",
       "0                   guides/technical-faq/general.md  \n",
       "1                   guides/technical-faq/general.md  \n",
       "2                   guides/technical-faq/general.md  \n",
       "3                     guides/technical-faq/admin.md  \n",
       "4                     guides/technical-faq/admin.md  \n",
       "5   guides/technical-faq/metrics-and-performance.md  \n",
       "6   guides/technical-faq/metrics-and-performance.md  \n",
       "7      guides/technical-faq/metrics-and-performance  \n",
       "8                     guides/technical-faq/setup.md  \n",
       "9           guides/technical-faq/troubleshooting.md  \n",
       "10          guides/technical-faq/troubleshooting.md  \n",
       "11                                    quickstart.md  \n",
       "12                           guides/track/launch.md  \n",
       "13                          track/log/log-tables.md  \n",
       "14                        track/log/logging-faqs.md  \n",
       "15                             guides/sweep/faqs.md  \n",
       "16                             guides/sweep/faqs.md  \n",
       "17                     guides/track/tracking-faq.md  \n",
       "18                     guides/track/tracking-faq.md  \n",
       "19                 guides/track/log/logging-faqs.md  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Retriever\n",
    "\n",
    "This is a search problem, it's easiest to start with tradiaional Information retrieval metrics.\n",
    "\n",
    "\n",
    "ref: https://weaviate.io/blog/retrieval-evaluation-metrics\n",
    "\n",
    "**TODO** Add weave model and evals in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/08 17:16:44 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/vr8n8v06/artifact/942570916/wandb_manifest.json?Expires=1720442804&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=UCuwOEHnAbof9fQovcnlU2jWpWChIt1bLTmzCqjV7x8wMd6QAraCQgnOoY2DvYjAFewFISORnlLUn0YIiNHl3T4q4zaJx6%2FobVbrnbLPNw8USKy0vd3jVyyl21jfRsEErgKfnV8OMOHDFoiEOZmW6c4AJP4IkxpVQWKPpNlrtFu%2B1BSl%2BMf8%2BpEHJOArnRJEVCF7yb9GLuI2K2S9knvzo%2BO9ZgWkwDMiY8v%2FzWs8RWlsiuY%2Bo6mkwHo2u1njg%2BG6NZXGd0McNOXNQeVf2wEduEiBVBu8gyHDBWKATN7hnXBW%2Ft4ciqrF47PX0OHTlB2aW%2Bsu72SmfP%2B6e8VzEsUwdA%3D%3D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md',\n",
       "   'raw_tokens': 300,\n",
       "   'cleaned_tokens': 300},\n",
       "  'cleaned_content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in'},\n",
       " {'content': 'your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md',\n",
       "   'raw_tokens': 170,\n",
       "   'cleaned_tokens': 170},\n",
       "  'cleaned_content': 'your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the data from Chapter 1\n",
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave version 0.50.7 is available!  To upgrade, please run:\n",
      " $ pip install weave --upgrade\n",
      "Logged in as Weights & Biases user: parambharat.\n",
      "View Weave data at https://wandb.ai/rag-course/dev/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.weave_client.WeaveClient at 0x7d9cb952f3d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "\n",
    "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the Retriever class from Chapter 1\n",
    "retriever = weave.ref(\"weave:///rag-course/dev/object/Retriever:OvLVBKNX0eRiaaGOBCavdnlxzNQiC6SCMtOGRRhi0uM\").get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/rag-course/dev/r/call/a3400435-4985-4da9-aa93-20615b72f15f\n",
      "üç© https://wandb.ai/rag-course/dev/r/call/7670eca1-b9ca-47fc-be16-003197d351df\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'source': 'guides/technical-faq/admin.md',\n",
       "  'text': '--- displayed_sidebar: default --- # Admin ### What is the difference between team and organization? A team is a collaborative workspace for a group of users working on the same projects, while an organization is a higher-level entity that may consist of multiple teams and is often related to billing and account management. ### What is the difference between team and entity? As a user - what does entity mean for me? A team is a collaborative workspace for a group of users working on the same projects, while an entity refers to either a username or a team name. When you log runs in W&B, you can set the entity to your personal account or a team account `wandb.init(entity=\"example-team\")`. ### What is a team and where can I find more information about it? If you want to know more about teams, visit the [teams section](../app/features/teams.md). ### When should I log to my personal entity against my team entity? Personal Entities are no longer available for accounts created after May 21st, 2024. W&B encourages all users, regardless of sign up date, to log new projects to a Team so you have the option to share your results with others. ### Who can create a team? Who can add or delete people from a team? Who can delete projects? You can check the different roles and permissions [here](../app/features/teams.md#team-roles-and-permissions). ### What type of roles are available and what are the differences between them? Go to [this](../app/features/teams.md#team-roles-and-permissions) page to see the different roles and permissions available. ### What are service accounts, and how do we add one to our team? Check [this](./general.md#what-is-a-service-account-and-why-is-it-useful) page from our docs to know more about service accounts. ### How can I see the bytes stored, bytes tracked and tracked hours of my organization? * You can check the',\n",
       "  'score': 0.26835651767405744},\n",
       " {'source': 'guides/technical-faq/general.md',\n",
       "  'text': '--- displayed_sidebar: default --- # General ### What does `wandb.init` do to my training process? When `wandb.init()` is called from your training script an API call is made to create a run object on our servers. A new process is started to stream and collect metrics, thereby keeping all threads and logic out of your primary process. Your script runs normally and writes to local files, while the separate process streams them to our servers along with system metrics. You can always turn off streaming by running `wandb off` from your training directory, or setting the `WANDB_MODE` environment variable to `offline`. ### Does your tool track or store training data? You can pass a SHA or other unique identifier to `wandb.config.update(...)` to associate a dataset with a training run. W&B does not store any data unless `wandb.save` is called with the local file name. ### What formula do you use for your smoothing algorithm? We use the same exponential moving average formula as TensorBoard. You can find an [explanation here](https://stackoverflow.com/questions/42281844/what-is-the-mathematics-behind-the-smoothing-parameter-in-tensorboards-scalar). ### How do I get the random run name in my script? Call `wandb.run.save()` and then get the name with `wandb.run.name` . ### What is the difference between `.log()` and `.summary`? The summary is the value that shows in the table while the log will save all the values for plotting later. For example, you might want to call `wandb.log` every time the accuracy changes. Usually, you can just use .log. `wandb.log()` will also update the summary value by default unless you have set the summary manually for that metric The scatterplot and parallel coordinate plots will also use the summary value while the line plot plots all of the values set by .log The reason we have both is that some people like to set the summary manually because',\n",
       "  'score': 0.2345125305145298},\n",
       " {'source': 'guides/track/tracking-faq.md',\n",
       "  'text': 'or you just want to hold off on the upload, here\\'s how to run `wandb` in offline mode and sync later. You will need to set two [environment variables](./environment-variables.md). 1. `WANDB_API_KEY=$KEY`, where `$KEY` is the API Key from your [settings page](https://app.wandb.ai/settings) 2. `WANDB_MODE=\"offline\"` And here\\'s a sample of what this would look like in your script: ```python import wandb import os os.environ[\"WANDB_API_KEY\"] = YOUR_KEY_HERE os.environ[\"WANDB_MODE\"] = \"offline\" config = { \"dataset\": \"CIFAR10\", \"machine\": \"offline cluster\", \"model\": \"CNN\", \"learning_rate\": 0.01, \"batch_size\": 128, } wandb.init(project=\"offline-demo\") for i in range(100): wandb.log({\"accuracy\": i}) ``` Here\\'s a sample terminal output: ![](/images/experiments/sample_terminal_output.png) And once you\\'re ready, just run a sync command to send that folder to the cloud. ```shell wandb sync wandb/dryrun-folder-name ``` ![](/images/experiments/sample_terminal_output_cloud.png) ### What is the difference between wandb.init modes? Modes can be \"online\", \"offline\" or \"disabled\", and default to online. `online`(default): In this mode, the client sends data to the wandb server. `offline`: In this mode, instead of sending data to the wandb server, the client will store data on your local machine which can be later synced with the [`wandb sync`](../../ref/cli/wandb-sync.md) command. `disabled`: In this mode, the client returns mocked objects and prevents all network communication. The client will essentially act like a no-op. In other words, all logging is entirely disabled. However, stubs out of all the API methods are still callable. This is usually used in tests. ### My run\\'s state is \"crashed\" on the UI but is still running on my machine. What do I do to get my data back? You most likely lost connection to your machine while training. You can recover your data by running [`wandb sync [PATH_TO_RUN]`](../../ref/cli/wandb-sync.md). The path to your run will be a folder in your `wandb` directory corresponding to the Run ID of the run in progress. ### `LaunchError: Permission denied` If',\n",
       "  'score': 0.18652642585894408},\n",
       " {'source': 'ref/python/run.md',\n",
       "  'text': \"# Run <p><button style={{display: 'flex', alignItems: 'center', backgroundColor: 'white', border: '1px solid #ddd', padding: '10px', borderRadius: '6px', cursor: 'pointer', boxShadow: '0 2px 3px rgba(0,0,0,0.1)', transition: 'all 0.3s'}}><a href='https://www.github.com/wandb/wandb/tree/v0.17.3/wandb/sdk/wandb_run.py#L461-L4184' style={{fontSize: '1.2em', display: 'flex', alignItems: 'center'}}><img src='https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png' height='32px' width='32px' style={{marginRight: '10px'}}/>View source on GitHub</a></button></p> A unit of computation logged by wandb. Typically, this is an ML experiment. ```python Run( settings: Settings, config: Optional[Dict[str, Any]] = None, sweep_config: Optional[Dict[str, Any]] = None, launch_config: Optional[Dict[str, Any]] = None ) -> None ``` Create a run with `wandb.init()`: <!--yeadoc-test:run-object-basic--> ```python import wandb run = wandb.init() ``` There is only ever at most one active `wandb.Run` in any process, and it is accessible as `wandb.run`: <!--yeadoc-test:global-run-object--> ```python import wandb assert wandb.run is None wandb.init() assert wandb.run is not None ``` anything you log with `wandb.log` will be sent to that run. If you want to start more runs in the same script or notebook, you'll need to finish the run that is in-flight. Runs can be finished with `wandb.finish` or by using them in a `with` block: <!--yeadoc-test:run-context-manager--> ```python import wandb wandb.init() wandb.finish() assert wandb.run is None with wandb.init() as run: pass # log data here assert wandb.run is None ``` See the documentation for `wandb.init` for more on creating runs, or check out [our guide to `wandb.init`](https://docs.wandb.ai/guides/track/launch). In distributed training, you can either create a single run in the rank 0 process and then log information only from that process, or you can create a run in each process, logging from each separately, and group the results together with the `group` argument to `wandb.init`. For more details on distributed training with W&B, check out [our guide](https://docs.wandb.ai/guides/track/log/distributed-training). Currently, there is a parallel `Run` object in the `wandb.Api`. Eventually these two objects will be merged. | Attributes | | | :--- | :--- | | `summary` |\",\n",
       "  'score': 0.17243056929706313},\n",
       " {'source': 'guides/reports/reports-faq.md',\n",
       "  'text': \"List item | `*` , `-` | | Ordered list item | `1.` | | Checklist | `[]` | | Blockquote | `>` , `\\\\|` | | Heading 1 | `#` | | Heading 2 | `##` | | Heading 3 | `###` | | Code block | ` ``` ` | | Inline code | `` ` `` | | Callout block | `>>>` | | Horizontal rule | `---`, `___` | A comprehensive list of LaTeX can be found [here](https://en.wikibooks.org/wiki/LaTeX/Mathematics). ### Adding multiple authors to a report When multiple users work on a report together, they should be credited as such. In our reports, you can do just that and add all the users that worked together on a report in the Author line. This is done by clicking Edit on the top right of the page. From there, a '+' will be seen on the right of the author line to where you can add all of the team members that have contributed to the report. ![](@site/static/images/reports/reports_faq_add_multiple_reports.gif) ### Embedding Reports You are able to share the report that you have created by embedding it. This is done simply by clicking Share on the top right of your report and copying the embedded code at the bottom of the pop-up window that appears. ![](@site/static/images/reports/emgedding_reports.gif) ### WYSIWYG FAQ **What's changed in the new reports release?** Reports look the same in view mode, and can have all the same content as they did before, but report editing is now WYSIWYG. **What is WYSIWYG?** WYSIWYG is an acronym for What You See Is What You Get. It refers to a type of editor where the content always looks the same, whether you're editing or presenting. In contrast, W&B reports used to have Markdown editors, where you edit in [Markdown](https://www.markdownguide.org) and have\",\n",
       "  'score': 0.1621014162244836}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorizer = TfidfVectorizer()\n",
    "retriever.index_data(chunked_data)\n",
    "retriever.predict(\"what is wandb\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def compute_hit_rate(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the hit rate for a single query.\n",
    "\n",
    "    :param retrieved_docs: List of retrieved documents\n",
    "    :param actual_doc: The single actual relevant document\n",
    "    :return: Hit rate (1 if the relevant document is retrieved, 0 otherwise)\n",
    "    \"\"\"\n",
    "    search_results = [doc['source'] for doc in model_output]\n",
    "    return 1 if source in search_results else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's how we can evaluate the retrieval system normally in python\n",
    "\n",
    "# hit_rates = []\n",
    "# for sample in eval_samples:\n",
    "#     query = sample[\"question\"]\n",
    "#     expected_source = sample[\"source\"]\n",
    "#     search_results = [doc['source'] for doc in retriever.search(query, k=5)]\n",
    "#     hit_rate = compute_hit_rate(search_results, expected_source)\n",
    "#     hit_rates.append({\"query\": query, \"hit_rate\": hit_rate})\n",
    "\n",
    "# hit_rate_df = pd.DataFrame(hit_rates)\n",
    "# display(hit_rate_df)\n",
    "\n",
    "\n",
    "# # we need a single number to rate the retrieval system\n",
    "# # the mean hit rate is a good metric to evaluate the retrieval system as a whole\n",
    "\n",
    "# print(f\"Mean Hit Rate: {hit_rate_df['hit_rate'].mean():.4f}\")\n",
    "# print(f\"Std-dev Hit Rate: {hit_rate_df['hit_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m11\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m12\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m13\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m14\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m15\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m16\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m17\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m18\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m19\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m20\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'compute_hit_rate'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"font-weight: bold\">}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3661641120910645</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\u001b[32m'compute_hit_rate'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.7\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.3661641120910645\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/rag-course/dev/r/call/bfffe4e4-e4a7-4dd1-a36e-a77c57503e66\n"
     ]
    }
   ],
   "source": [
    "# the same evaluatuion can be done in weave\n",
    "\n",
    "hit_rate_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Hit_Score\",\n",
    "    dataset=eval_samples,\n",
    "    scorers=[compute_hit_rate],\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\":5}\n",
    ")\n",
    "hit_rate = asyncio.run(hit_rate_evaluation.evaluate(retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating retrieval on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR (Mean Reciprocal Rank)\n",
    "@weave.op()\n",
    "def compute_mrr(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    mrr_score = 0\n",
    "    for rank, result in enumerate(model_output, 1):\n",
    "        if result['source'] == source:\n",
    "            mrr_score = 1 / rank\n",
    "            break\n",
    "    return mrr_score\n",
    "\n",
    "\n",
    "# NDCG (Normalized Discounted Cumulative Gain)\n",
    "@weave.op()\n",
    "def compute_ndcg(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "\n",
    "    # Sort the results by score to calculate IDCG\n",
    "    sorted_model_output = sorted(model_output, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result['source'] == source:\n",
    "            # Calculate DCG\n",
    "            dcg += (2 ** result['score'] - 1) / np.log2(i + 2)  # i+2 because log2 starts at 1 for i=0\n",
    "\n",
    "    for i, result in enumerate(sorted_model_output):\n",
    "        if result['source'] == source:\n",
    "            # Calculate IDCG\n",
    "            idcg += (2 ** result['score'] - 1) / np.log2(i + 2)\n",
    "\n",
    "    # To avoid division by zero\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate nDCG\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "\n",
    "# MAP (Mean Average Precision)\n",
    "@weave.op()\n",
    "def compute_map(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    num_relevant = 0\n",
    "    sum_precision = 0.0\n",
    "\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result['source'] == source:\n",
    "            num_relevant += 1\n",
    "            sum_precision += num_relevant / (i + 1)\n",
    "\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    average_precision = sum_precision / num_relevant\n",
    "    return average_precision\n",
    "\n",
    "\n",
    "# Precision <- this is more discounted precision because we only have 1 reference \n",
    "@weave.op()\n",
    "def compute_rank_precision(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    total_score = 0.0\n",
    "    relevant_count = 0\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result['source'] == source:\n",
    "            total_score += max(1 - 0.2 * i, 0)\n",
    "            relevant_count += 1\n",
    "    return total_score / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "# Recall\n",
    "@weave.op()\n",
    "def compute_recall(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    total_relevant_items = sum(1 for result in model_output if result['source'] == source)\n",
    "    retrieved_relevant_items = total_relevant_items\n",
    "    return retrieved_relevant_items / total_relevant_items if total_relevant_items > 0 else 0.0\n",
    "\n",
    "# F1 Score\n",
    "@weave.op()\n",
    "def compute_f1(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    precision = compute_rank_precision(model_output, source)\n",
    "    recall = compute_recall(model_output, source)\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m11\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m12\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m13\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m14\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m15\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m16\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m17\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m18\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m19\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m20\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_mrr'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6166666666666666</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_ndcg'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_map'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6041666666666666</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_hit_rate'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_rank_precision'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.64</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_recall'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_f1'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6649337805297558</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5406972885131835</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'compute_mrr'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.6166666666666666\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_ndcg'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.7\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_map'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.6041666666666666\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_hit_rate'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.7\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_rank_precision'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.64\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_recall'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.7\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_f1'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.6649337805297558\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.5406972885131835\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/rag-course/dev/r/call/a5a091ab-7bca-4701-a7a2-04036a3bb84c\n"
     ]
    }
   ],
   "source": [
    "retrieval_scorers = [compute_mrr, compute_ndcg, compute_map, compute_hit_rate, compute_rank_precision, compute_recall, compute_f1]\n",
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_samples,\n",
    "    scorers=retrieval_scorers,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\":5}\n",
    ")\n",
    "retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(retriever))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Retrieval Judge\n",
    "\n",
    "**ref: https://arxiv.org/pdf/2406.06519**\n",
    "\n",
    "How do we evaluate if we don't have any ground truth? \n",
    "\n",
    "We can use a powerful LLM as a judge to evaluate the retriever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RETRIEVAL_EVAL_PROMPT =\"\"\"\n",
    "Given a query and a document excerpt, you must provide a score on an integer scale of 0 to 2 with the following meanings:\n",
    "    0 = represents that the excerpt is irrelevant to the query,\n",
    "    1 = represents that the excerpt is somewhat relevant to the query,\n",
    "    2 = represents that the excerpt is is highly relevant to the query.\n",
    "    \n",
    "\n",
    "Important Instruction: Assign category 1 if the excerpt is somewhat related to the query but not completely, category 2 if the excerpt only and entirely refers to the query. If neither of these criteria satisfies the query, give it category 0.\n",
    "\n",
    "\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the query. Measure how well the content matches a likely intent of the query(M).\n",
    "Measure how trustworthy the excerpt is (T).\n",
    "Consider the aspects above and the relative importance of each, and decide on a final score (O). \n",
    "Final score must be an integer value only.\n",
    "Do not provide any code in result. Provide each score in the following JSON format: \n",
    "{{\"final_score\": <integer score without providing any reasoning.>}}\n",
    "\n",
    "## Examples\n",
    "\n",
    "Example 1: \n",
    "<Query>\n",
    "How do I programmatically access the human-readable run name?\n",
    "</Query>\n",
    "<Document>\n",
    "If you do not explicitly name your run, a random run name will be assigned to the run to help identify the run in the UI. For instance, random run names will look like \"pleasant-flower-4\" or \"misunderstood-glade-2\".\n",
    "\n",
    "If you'd like to overwrite the run name (like snowy-owl-10) with the run ID (like qvlp96vk) you can use this snippet:\n",
    "\n",
    "import wandbRetrieval_Evaluation\n",
    "\n",
    "wandb.init()\n",
    "wandb.run.name = wandb.run.id\n",
    "wandb.run.save()\n",
    "\n",
    "</Document>\n",
    "{{\"final_score\": 0}}\n",
    "\n",
    "Example 2:\n",
    "<Query>\n",
    "What are Runs?\n",
    "</Query>\n",
    "<Document>\n",
    "A single unit of computation logged by W&B is called a run. You can think of a W&B run as an atomic element of your whole project. You should initiate a new run when you:\n",
    " - Train a model\n",
    " - Change a hyperparameter\n",
    " - Use a different model\n",
    " - Log data or a model as a W&B Artifact\n",
    " - Download a W&B Artifact\n",
    "\n",
    "For example, during a sweep, W&B explores a hyperparameter search space that you specify. Each new hyperparameter combination created by the sweep is implemented and recorded as a unique run. \n",
    "</Document>\n",
    "{{\"final_score\": 2}}\n",
    "\n",
    "Example 3:\n",
    "<Query>\n",
    "How do I use W&B with Keras ?\n",
    "</Query>\n",
    "<Document>\n",
    "We have added three new callbacks for Keras and TensorFlow users, available from wandb v0.13.4. For the legacy WandbCallback scroll down.\n",
    "These new callbacks,\n",
    " - Adhere to Keras design philosophy\n",
    " - Reduce the cognitive load of using a single callback (WandbCallback) for everything\n",
    " - Make it easy for Keras users to modify the callback by subclassing it to support their niche use case\n",
    "</Document>\n",
    "{{\"final_score\": 1}}\n",
    "\n",
    "<Query>\n",
    "{query}\n",
    "</Query>\n",
    "\n",
    "<Document>\n",
    "{document}\n",
    "</Document>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "@weave.op()\n",
    "async def evaluate_retriever_using_llm_judge(query: str, passage: str) -> str:\n",
    "    response = await client.chat(\n",
    "        message=RETRIEVAL_EVAL_PROMPT.format(query=query, document=passage),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def run_retriever_evaluation_using_llm(eval_samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    scores = []\n",
    "    for sample in eval_samples:\n",
    "        query = sample[\"question\"]\n",
    "        search_results = retriever.search(query, k=5)\n",
    "        tasks = []\n",
    "        for result in search_results:\n",
    "            tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"]))\n",
    "        sample_scores = await asyncio.gather(*tasks)\n",
    "        sample_scores = map(json.loads, sample_scores)\n",
    "        sample_scores = list(map(lambda x: x[\"final_score\"], sample_scores))\n",
    "        scores.append({\"query\": query, \"scores\": sample_scores})\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/rag-course/dev/r/call/2528896c-8b3e-4acf-8b97-dcc900ccc571\n"
     ]
    }
   ],
   "source": [
    "llm_judge_retrieval_results = asyncio.run(run_retriever_evaluation_using_llm(eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>scores</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>[2, 0, 2, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>[2, 1, 1, 0, 0]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>[2, 0, 0, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>[2, 2, 1, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>[2, 1, 0, 2, 1]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>[2, 0, 1, 1, 0]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>[2, 1, 1, 2, 0]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>[2, 2, 1, 0, 0]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>[2, 0, 2, 0, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>[2, 2, 0, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>[2, 0, 2, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>[1, 0, 2, 2, 1]</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>[0, 2, 0, 0, 0]</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>[0, 2, 2, 2, 2]</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query           scores  \\\n",
       "0   What is the difference between `.log()` and `....  [2, 0, 2, 2, 2]   \n",
       "1   How do I switch between accounts on the same m...  [2, 1, 1, 0, 0]   \n",
       "2              How is W&B different from TensorBoard?  [2, 2, 2, 2, 2]   \n",
       "3   What is the difference between team and organi...  [2, 2, 2, 2, 2]   \n",
       "4   What is the difference between team and entity...  [2, 0, 0, 2, 2]   \n",
       "5   Can I just log metrics, no code or dataset exa...  [2, 2, 1, 2, 2]   \n",
       "6   How can I log a metric that doesn't change ove...  [2, 1, 0, 2, 1]   \n",
       "7                How many runs to create per project?  [2, 0, 1, 1, 0]   \n",
       "8                            Can I run wandb offline?  [2, 2, 2, 2, 2]   \n",
       "9                  How do I deal with network issues?  [2, 1, 1, 2, 0]   \n",
       "10  What happens if internet connection is lost wh...  [2, 2, 1, 0, 0]   \n",
       "11                        Where do I find my API key?  [2, 0, 2, 0, 2]   \n",
       "12                  How do I create a W&B Experiment?  [2, 2, 0, 2, 2]   \n",
       "13                               Log a table to a run  [2, 2, 2, 2, 2]   \n",
       "14                     How do I log a list of values?  [2, 0, 2, 2, 2]   \n",
       "15  Is there a way to add extra values to a sweep,...  [1, 0, 2, 2, 1]   \n",
       "16  Can we flag boolean variables as hyperparameters?  [0, 2, 0, 0, 0]   \n",
       "17  How do I programmatically access the human-rea...  [2, 2, 2, 2, 2]   \n",
       "18  How can I save the git commit associated with ...  [2, 2, 2, 2, 2]   \n",
       "19  How can I organize my logged charts and media ...  [0, 2, 2, 2, 2]   \n",
       "\n",
       "    rank_score  \n",
       "0     1.000000  \n",
       "1     1.000000  \n",
       "2     1.000000  \n",
       "3     1.000000  \n",
       "4     1.000000  \n",
       "5     1.000000  \n",
       "6     1.000000  \n",
       "7     1.000000  \n",
       "8     1.000000  \n",
       "9     1.000000  \n",
       "10    1.000000  \n",
       "11    1.000000  \n",
       "12    1.000000  \n",
       "13    1.000000  \n",
       "14    1.000000  \n",
       "15    0.333333  \n",
       "16    0.500000  \n",
       "17    1.000000  \n",
       "18    1.000000  \n",
       "19    0.500000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank Score: 0.9167\n",
      "Std-dev Rank Score: 0.2059\n"
     ]
    }
   ],
   "source": [
    "# we have the scores for each document\n",
    "llm_judge_retrieval_results_df = pd.DataFrame(llm_judge_retrieval_results)\n",
    "\n",
    "# we can compute the reciprocal rank of the first document that is relevant to the query i.e. rated as 2 by our llm judge.\n",
    "def compute_rank_score(scores: List[int]) -> float:\n",
    "    rank_score = 0\n",
    "    for rank, result in enumerate(scores, 1):\n",
    "        if result == 2:\n",
    "            rank_score = 1 / rank\n",
    "            return rank_score\n",
    "    return rank_score\n",
    "\n",
    "llm_judge_retrieval_results_df[\"rank_score\"] = llm_judge_retrieval_results_df[\"scores\"].map(compute_rank_score)\n",
    "\n",
    "\n",
    "display(llm_judge_retrieval_results_df)\n",
    "\n",
    "\n",
    "print(f\"Mean Rank Score: {llm_judge_retrieval_results_df['rank_score'].mean():.4f}\")\n",
    "print(f\"Std-dev Rank Score: {llm_judge_retrieval_results_df['rank_score'].std():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/rag-course/dev/r/call/4075015d-9e16-4f1c-bef0-c2e1268ae5f7\n",
      "üç© https://wandb.ai/rag-course/dev/r/call/86bb800e-faf1-461f-9612-e82120ca8aa6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'W&B seems to refer to Weight & Bias, which is a platform for developers and researchers to train and track machine learning models. Some of its features include logging and visualizing experiment data, sharing reports, and collaborating with team members. It appears that the W&B API can be integrated into training scripts to send data to the W&B server and manage experiment configurations.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_generator = weave.ref(\"weave:///rag-course/dev/object/ResponseGenerator:YQiRpgHDhvJcrZEVpXJbvy2PuJyh5W3gXuhX06zZiYQ\").get()\n",
    "query = \"What is w&b?\"\n",
    "context = retriever.search(query, 5)\n",
    "\n",
    "response_generator.client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "response_generator.predict(query, context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/rag-course/dev/r/call/dd2acb3c-cadb-4a77-a4cf-fa465c911f23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'W&B seems to refer to Weight & Bias, which is a platform for developers and researchers to train and track machine learning models. Some of its features include logging and visualizing experiment data, sharing reports, and collaborating with team members. It appears that the W&B API can be integrated into training scripts to send data to the W&B server and manage experiment configurations.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline = weave.ref(\"weave:///rag-course/dev/object/RAGPipeline:njpUKmvYezO3X9cJJ5BXAujh0XSTHcpRyDsYswHbPRs\").get()\n",
    "rag_pipeline.retriever = retriever\n",
    "rag_pipeline.response_generator = response_generator\n",
    "rag_pipeline.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.ensure_loaded()\n",
    "# We can measure the similarity of the response to the expected answer using difflib and Levenshtein distance\n",
    "# These are simple metrics.\n",
    "\n",
    "@weave.op()\n",
    "def compute_diff(model_output: str, answer: str) -> float:\n",
    "    return difflib.SequenceMatcher(None, model_output, answer).ratio()\n",
    "\n",
    "@weave.op()\n",
    "def compute_levenshtein(model_output: str, answer: str) -> float:\n",
    "    return Levenshtein.ratio(model_output, answer)\n",
    "\n",
    "\n",
    "\n",
    "# semantic answer similarity. (SAS) - https://arxiv.org/abs/2108.06130\n",
    "# Originally, one should use a transformer based cross-encoder to measure and classify this. \n",
    "# For example, use something from https://sbert.net/docs/cross_encoder/usage/usage.html\n",
    "# we can also calculate the cosine similarity between the candidate and the reference using our retriever's vectorizer\n",
    "@weave.op()\n",
    "def compute_similarity(model_output: str, answer: str) -> float:\n",
    "    vectors = retriever.vectorizer.transform([model_output, answer])\n",
    "    similarity = cosine_similarity(vectors)[0][1]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# or we can use traditional metrics used to measure generation systems.\n",
    "# ref: https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/\n",
    "\n",
    "@weave.op()\n",
    "def compute_rouge(model_output: str, answer: str) -> float:\n",
    "    rouge = Rouge(metrics=[\"rouge-l\"], stats=\"f\")\n",
    "    scores = rouge.get_scores(model_output, answer)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_bleu(model_output: str, answer: str) -> float:\n",
    "    chencherry = SmoothingFunction()\n",
    "    smoothing_function = chencherry.method2\n",
    "\n",
    "    reference = word_tokenize(answer)\n",
    "    candidate = word_tokenize(model_output)\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m11\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m12\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m13\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m14\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m15\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m16\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m17\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m18\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m19\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m20\u001b[0m of \u001b[1;36m20\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_diff'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.28007283754932016</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_levenshtein'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5449797391703644</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_similarity'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.580044730960847</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_rouge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.46875247976176027</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'compute_bleu'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.26813438725093003</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.79070258140564</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'compute_diff'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.28007283754932016\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_levenshtein'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.5449797391703644\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_similarity'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.580044730960847\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_rouge'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.46875247976176027\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'compute_bleu'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.26813438725093003\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m5.79070258140564\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/rag-course/dev/r/call/c1cdbd37-83fb-43b9-8a0a-9776d3c71c96\n"
     ]
    }
   ],
   "source": [
    "response_scorers = [compute_diff, compute_levenshtein, compute_similarity, compute_rouge, compute_bleu]\n",
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_samples, \n",
    "    scorers=response_scorers, \n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(response_evaluations.evaluate(rag_pipeline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Response Judge\n",
    "\n",
    "Some metrics cannot be defined objectively and are particularly useful for more subjective or complex criteria.\n",
    "We care about correctness, faithfulness, and relevance.\n",
    "\n",
    "- **Answer Correctness** - Is the generated answer correct compared to the reference and thoroughly answers the user's query?\n",
    "- **Answer Relevancy** - Is the generated answer relevant and comprehensive?\n",
    "- **Answer Factfulness** - Is the generated answer factually consistent with the context document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CORRECTNESS_EVAL_PROMPT =\"\"\"\n",
    "You are a Weight & Biases support expert tasked with evaluating the correctness of answers to questions asked by users to a technical support chatbot. \n",
    "You are tasked with judging the correctness of a generated answer based on the user's query, and a reference answer.\n",
    "\n",
    "You will be given the following information:\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\n",
    "<reference_answer>\n",
    "{reference_answer}\n",
    "</reference_answer>\n",
    "\n",
    "<generated_answer>\n",
    "{generated_answer}\n",
    "</generated_answer>\n",
    "\n",
    "Important Instruction: To evaluate the generated answer, follow these steps:\n",
    "\n",
    "1. Intent Analysis: Consider the underlying intent of the query.\n",
    "2. Relevance: Check if the generated answer addresses all aspects of the question.\n",
    "3. Accuracy: Compare the generated answer to the reference answer for completeness and correctness.\n",
    "4. Trustworthiness: Measure how trustworthy the generated answer is when compared to the reference.\n",
    "\n",
    "Assign a score on an integer scale of 0 to 2 with the following meanings:\n",
    "- 0 = The generated answer is incorrect and does not satisfy any of the criteria.\n",
    "- 1 = The generated answer is partially correct, contains mistakes or is not factually correct.\n",
    "- 2 = The generated answer is correct, completely answers the query, does not contain any mistakes, and is factually consistent with the reference answer.\n",
    "\n",
    "After your analysis, provide your verdict in the following JSON format:\n",
    "\n",
    "{{\n",
    "    \"reason\": \"<<Provide a brief explanation for your decision here>>\",\n",
    "    \"final_score\": <<Provide a score as per the above guidelines>>,\n",
    "    \"decision\": \"<<Provide your final decision here, either 'correct' or 'incorrect'>>\"\n",
    "}}\n",
    "\n",
    "Here are some examples of correct output:\n",
    "\n",
    "Example 1:\n",
    "{{\n",
    "    \"reason\": \"The generated answer has the exact details as the reference answer and completely answers the user's query.\",\n",
    "    \"final_score\": 2,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "\n",
    "Example 2:\n",
    "{{\n",
    "    \"reason\": \"The generated answer doesn't match the reference answer and deviates from the user's query.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 3:\n",
    "{{\n",
    "    \"reason\": \"The generated answer follows the same steps as the reference answer. However, it significantly misses the user's intent,\n",
    "    \"final_score\": 1,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 4:\n",
    "{{\n",
    "    \"reason\": \"The generated is not factually correct and includes assumptions about code methods completely different from the reference answer\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Please provide your evaluation based on the given information and format your response according to the specified JSON structure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "@weave.op()\n",
    "async def evaluate_correctness_using_llm_judge(question: str, answer: str, model_output: str) -> Dict[str, Any]:\n",
    "    response = await client.chat(\n",
    "        message=CORRECTNESS_EVAL_PROMPT.format(query=question, reference_answer=answer, generated_answer=model_output),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    return json.loads(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scorers = [evaluate_correctness_using_llm_judge]\n",
    "correctness_evaluations = weave.Evaluation(\n",
    "    name=\"Correctness_Evaluation\",\n",
    "    dataset=eval_samples, \n",
    "    scorers=response_scorers, \n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(correctness_evaluations.evaluate(rag_pipeline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Implement the `Relevance` and `Faithfulness` evaluators and evaluate the pipeline on all the dimensions.\n",
    "2. Generate and share a W&B report with the following sections in the form of tables and charts:\n",
    "    \n",
    "    - Summary of the evaluation\n",
    "    - Retreival Evaluations\n",
    "        - IR Metrics\n",
    "        - LLM As a Retrieval Judge Metric\n",
    "    - Response Evalations\n",
    "        - Traditional NLP Metrics\n",
    "        - LLM Judgement Metrics\n",
    "    - Overall Evalations\n",
    "    - Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
